{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#------------------------------------------------------------------\n",
    "# TRAINING\n",
    "#------------------------------------------------------------------\n",
    "from unet_4block_conv import *\n",
    "from unet_3block_conv import *\n",
    "from outils_prepro import * \n",
    "from data_loader_seg import *\n",
    "from model_prob_unet_init import *\n",
    "from create_images_3_classes import *\n",
    "from training import *\n",
    "from loss import *\n",
    "#---------------------\n",
    "# Data\n",
    "error = 15\n",
    "Area = 100\n",
    "image_shape = (128, 128) \n",
    "batch_size = 2\n",
    "#----------------------\n",
    "path_ori = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\originals_180919\\originals'\n",
    "path = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\ora_180919\\Layers'\n",
    "image_Paths, mask_Paths = create_dir_paths(path)\n",
    "image_list , mask_list =  create_box_images(image_Paths, mask_Paths, error, Area)\n",
    "print('Images and masks are created')\n",
    "print(len(image_list), len(mask_list))     \n",
    "data = SegmentationDataset(image_list, mask_list, image_shape)\n",
    "dataloaders = torch.utils.data.DataLoader(data, batch_size)\n",
    "\n",
    "#----------------------\n",
    "# Model \n",
    "input_channels = 3\n",
    "num_classes = 3\n",
    "learning_rate = 1e-3  \n",
    "epochs = 301\n",
    "loss_fn  =  DiceLoss() \n",
    "folder_name = 'model_unet_3'\n",
    "\n",
    "\n",
    "filters = 8\n",
    "beta = 101\n",
    "z_dim = 10\n",
    " # different learning rate and the same error as Porba Unet\n",
    "#----------------------\n",
    "if filters == 8:\n",
    "    featureDim = 16384\n",
    "if filters ==4:\n",
    "    featureDim = 8192\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelPath = os.path.join(r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg', folder_name)\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)\n",
    "\n",
    "#net = Probabilistic_UNET(input_channels, num_classes, filters, z_dim, image_shape, featureDim)\n",
    "net = UNET(input_channels, num_classes) \n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0)\n",
    "#summary(net)\n",
    "#sum(p.numel() for p in net.parameters() if p.requires_grad) \n",
    "tloss, tloss_list = training_Unet(dataloaders, epochs, device, loss_fn, net, optimizer, modelPath)\n",
    "plt.plot(tloss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "76 76 76\n",
      "76 76 76 76\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e3431a2668aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\ora_180919\\Layers'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mimage_Paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_Paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dir_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mimage_list\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmask_list\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mcreate_box_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_Paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_Paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArea\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Images and masks are created'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#------------------------------------------------------------------\n",
    "# TRAINING\n",
    "#------------------------------------------------------------------\n",
    "from unet_4block_conv import *\n",
    "from unet_3block_conv import *\n",
    "from outils_prepro import * \n",
    "from data_loader_seg import *\n",
    "from model_prob_unet_init import *\n",
    "from create_images_3_classes import *\n",
    "from training import *\n",
    "from loss import *\n",
    "#---------------------\n",
    "# Data\n",
    "error = 15\n",
    "Area = 100\n",
    "image_shape = (128, 128) \n",
    "batch_size = 2\n",
    "#----------------------\n",
    "path_ori = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\originals_180919\\originals'\n",
    "path = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\ora_180919\\Layers'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "76 76 76\n",
      "76 76 76 76\n",
      "Images and masks are created\n",
      "135 135\n"
     ]
    }
   ],
   "source": [
    "image_Paths, mask_Paths = create_dir_paths(path)\n",
    "image_list , mask_list, _ =  create_box_images(image_Paths, mask_Paths, error, Area)\n",
    "print('Images and masks are created')\n",
    "print(len(image_list), len(mask_list))     \n",
    "data = SegmentationDataset(image_list, mask_list, image_shape)\n",
    "dataloaders = torch.utils.data.DataLoader(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 105, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataloaders:\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "76 76 76\n",
      "76 76 76 76\n",
      "Images and masks are created\n",
      "135 135\n",
      "Current step: 0  Loss: 5589.08447265625  Recons: 0.7418190240859985  dkl : 279.4171447753906\n",
      "Current step: 30  Loss: 759.8366088867188  Recons: 0.7441853284835815  dkl : 37.954620361328125\n",
      "Current step: 60  Loss: 335.79864501953125  Recons: 0.6570471525192261  dkl : 16.757080078125\n",
      "Saving Model0.torch\n",
      "Current step: 0  Loss: 223.6140899658203  Recons: 0.6919004917144775  dkl : 11.146109580993652\n",
      "Current step: 30  Loss: 139.64939880371094  Recons: 0.7263984680175781  dkl : 6.946149826049805\n",
      "Current step: 60  Loss: 91.46733093261719  Recons: 0.6629618406295776  dkl : 4.540218353271484\n",
      "Current step: 0  Loss: 59.744991302490234  Recons: 0.6771818399429321  dkl : 2.953390598297119\n",
      "Current step: 30  Loss: 54.06808853149414  Recons: 0.7442220449447632  dkl : 2.6661934852600098\n",
      "Current step: 60  Loss: 40.523277282714844  Recons: 0.6460108757019043  dkl : 1.9938633441925049\n",
      "Current step: 0  Loss: 24.217920303344727  Recons: 0.6161706447601318  dkl : 1.1800874471664429\n",
      "Current step: 30  Loss: 27.40199089050293  Recons: 0.6318298578262329  dkl : 1.3385080099105835\n",
      "Current step: 60  Loss: 22.468276977539062  Recons: 0.5814913511276245  dkl : 1.0943392515182495\n",
      "Current step: 0  Loss: 12.027388572692871  Recons: 0.6110246181488037  dkl : 0.5708181858062744\n",
      "Current step: 30  Loss: 15.19682502746582  Recons: 0.5538468360900879  dkl : 0.7321488857269287\n",
      "Current step: 60  Loss: 14.141706466674805  Recons: 0.5344433784484863  dkl : 0.6803631782531738\n",
      "Current step: 0  Loss: 6.605057239532471  Recons: 0.5585798025131226  dkl : 0.3023238778114319\n",
      "Current step: 30  Loss: 9.171892166137695  Recons: 0.519959568977356  dkl : 0.4325966536998749\n",
      "Current step: 60  Loss: 9.664937019348145  Recons: 0.47198212146759033  dkl : 0.4596477746963501\n",
      "Saving Model5.torch\n",
      "Current step: 0  Loss: 4.111292362213135  Recons: 0.5540419816970825  dkl : 0.1778625249862671\n",
      "Current step: 30  Loss: 6.189240455627441  Recons: 0.49211156368255615  dkl : 0.2848564386367798\n",
      "Current step: 60  Loss: 7.176759719848633  Recons: 0.5764977931976318  dkl : 0.33001309633255005\n",
      "Current step: 0  Loss: 2.9705002307891846  Recons: 0.5021359920501709  dkl : 0.12341821193695068\n",
      "Current step: 30  Loss: 4.687995910644531  Recons: 0.4753109812736511  dkl : 0.2106342613697052\n",
      "Current step: 60  Loss: 5.494603157043457  Recons: 0.40930771827697754  dkl : 0.254264771938324\n",
      "Current step: 0  Loss: 2.6152379512786865  Recons: 0.5288412570953369  dkl : 0.10431984066963196\n",
      "Current step: 30  Loss: 3.9550061225891113  Recons: 0.5405594110488892  dkl : 0.1707223355770111\n",
      "Current step: 60  Loss: 4.579607963562012  Recons: 0.45697253942489624  dkl : 0.20613177120685577\n",
      "Current step: 0  Loss: 2.5073556900024414  Recons: 0.5009311437606812  dkl : 0.1003212183713913\n",
      "Current step: 30  Loss: 3.4101409912109375  Recons: 0.4589152932167053  dkl : 0.14756128191947937\n",
      "Current step: 60  Loss: 3.9731807708740234  Recons: 0.4821876883506775  dkl : 0.1745496541261673\n",
      "Current step: 0  Loss: 2.54887318611145  Recons: 0.5064206123352051  dkl : 0.10212262719869614\n",
      "Current step: 30  Loss: 3.1381959915161133  Recons: 0.4594106078147888  dkl : 0.13393926620483398\n",
      "Current step: 60  Loss: 3.511557102203369  Recons: 0.46235162019729614  dkl : 0.1524602770805359\n",
      "Saving Model10.torch\n",
      "Current step: 0  Loss: 2.598496913909912  Recons: 0.4907975196838379  dkl : 0.10538497567176819\n",
      "Current step: 30  Loss: 3.0475080013275146  Recons: 0.5565850734710693  dkl : 0.12454614043235779\n",
      "Current step: 60  Loss: 3.1482317447662354  Recons: 0.41348904371261597  dkl : 0.1367371380329132\n",
      "Current step: 0  Loss: 2.6132349967956543  Recons: 0.4395502805709839  dkl : 0.1086842343211174\n",
      "Current step: 30  Loss: 2.771070718765259  Recons: 0.4305155277252197  dkl : 0.11702775955200195\n",
      "Current step: 60  Loss: 2.9206349849700928  Recons: 0.4197816252708435  dkl : 0.12504266202449799\n",
      "Current step: 0  Loss: 2.716243267059326  Recons: 0.49873924255371094  dkl : 0.110875204205513\n",
      "Current step: 30  Loss: 2.649257183074951  Recons: 0.4419037103652954  dkl : 0.11036768555641174\n",
      "Current step: 60  Loss: 2.777580738067627  Recons: 0.46678656339645386  dkl : 0.11553970724344254\n",
      "Current step: 0  Loss: 2.7386891841888428  Recons: 0.49593472480773926  dkl : 0.11213771998882294\n",
      "Current step: 30  Loss: 2.564164161682129  Recons: 0.4719087481498718  dkl : 0.10461276769638062\n",
      "Current step: 60  Loss: 2.5974321365356445  Recons: 0.45251357555389404  dkl : 0.10724593698978424\n",
      "Current step: 0  Loss: 2.7575595378875732  Recons: 0.5052967071533203  dkl : 0.11261314153671265\n",
      "Current step: 30  Loss: 2.44950008392334  Recons: 0.47070324420928955  dkl : 0.09893984347581863\n",
      "Current step: 60  Loss: 2.434062957763672  Recons: 0.43195003271102905  dkl : 0.1001056432723999\n",
      "Saving Model15.torch\n",
      "Current step: 0  Loss: 2.752762794494629  Recons: 0.5042446851730347  dkl : 0.11242590099573135\n",
      "Current step: 30  Loss: 2.372443675994873  Recons: 0.5066324472427368  dkl : 0.09329056739807129\n",
      "Current step: 60  Loss: 2.2848093509674072  Recons: 0.4134223461151123  dkl : 0.09356935322284698\n",
      "Current step: 0  Loss: 2.702362060546875  Recons: 0.47586387395858765  dkl : 0.11132490634918213\n",
      "Current step: 30  Loss: 2.215341567993164  Recons: 0.4613914489746094  dkl : 0.08769751340150833\n",
      "Current step: 60  Loss: 2.2416300773620605  Recons: 0.4967194199562073  dkl : 0.08724553883075714\n",
      "Current step: 0  Loss: 2.6651573181152344  Recons: 0.4720917344093323  dkl : 0.1096532791852951\n",
      "Current step: 30  Loss: 2.131922721862793  Recons: 0.4856436848640442  dkl : 0.08231395483016968\n",
      "Current step: 60  Loss: 2.0553882122039795  Recons: 0.4258037209510803  dkl : 0.08147922903299332\n",
      "Current step: 0  Loss: 2.616772413253784  Recons: 0.4650254249572754  dkl : 0.10758735239505768\n",
      "Current step: 30  Loss: 2.0183472633361816  Recons: 0.5055471658706665  dkl : 0.0756400004029274\n",
      "Current step: 60  Loss: 1.9222009181976318  Recons: 0.39581912755966187  dkl : 0.07631908357143402\n",
      "Current step: 0  Loss: 2.561508893966675  Recons: 0.4679937958717346  dkl : 0.10467575490474701\n",
      "Current step: 30  Loss: 1.8727219104766846  Recons: 0.48063379526138306  dkl : 0.06960440427064896\n",
      "Current step: 60  Loss: 1.8628010749816895  Recons: 0.42867255210876465  dkl : 0.07170642912387848\n",
      "Saving Model20.torch\n",
      "Current step: 0  Loss: 2.52236008644104  Recons: 0.48952656984329224  dkl : 0.1016416847705841\n",
      "Current step: 30  Loss: 1.7707009315490723  Recons: 0.4949913024902344  dkl : 0.06378547847270966\n",
      "Current step: 60  Loss: 1.7658838033676147  Recons: 0.4141058921813965  dkl : 0.06758889555931091\n",
      "Current step: 0  Loss: 2.434497594833374  Recons: 0.46706271171569824  dkl : 0.09837174415588379\n",
      "Current step: 30  Loss: 1.6789406538009644  Recons: 0.5150913000106812  dkl : 0.05819246917963028\n",
      "Current step: 60  Loss: 1.676051139831543  Recons: 0.4017704129219055  dkl : 0.06371404230594635\n",
      "Current step: 0  Loss: 2.3126256465911865  Recons: 0.41483157873153687  dkl : 0.09488970041275024\n",
      "Current step: 30  Loss: 1.5605902671813965  Recons: 0.49468129873275757  dkl : 0.053295448422431946\n",
      "Current step: 60  Loss: 1.6272449493408203  Recons: 0.4225419759750366  dkl : 0.060235146433115005\n",
      "Current step: 0  Loss: 2.275909662246704  Recons: 0.4515898823738098  dkl : 0.09121599048376083\n",
      "Current step: 30  Loss: 1.4212182760238647  Recons: 0.44235777854919434  dkl : 0.04894302412867546\n",
      "Current step: 60  Loss: 1.5816439390182495  Recons: 0.43928396701812744  dkl : 0.057117998600006104\n",
      "Current step: 0  Loss: 2.2175517082214355  Recons: 0.4626268744468689  dkl : 0.08774624019861221\n",
      "Current step: 30  Loss: 1.4377288818359375  Recons: 0.532921552658081  dkl : 0.0452403649687767\n",
      "Current step: 60  Loss: 1.4599359035491943  Recons: 0.3708450198173523  dkl : 0.054454538971185684\n",
      "Saving Model25.torch\n",
      "Current step: 0  Loss: 2.147045135498047  Recons: 0.44984954595565796  dkl : 0.08485977351665497\n",
      "Current step: 30  Loss: 1.2930151224136353  Recons: 0.45132172107696533  dkl : 0.042084671556949615\n",
      "Current step: 60  Loss: 1.4498969316482544  Recons: 0.41330528259277344  dkl : 0.05182958021759987\n",
      "Current step: 0  Loss: 2.102354049682617  Recons: 0.45181721448898315  dkl : 0.08252684026956558\n",
      "Current step: 30  Loss: 1.248971939086914  Recons: 0.45829397439956665  dkl : 0.03953389450907707\n",
      "Current step: 60  Loss: 1.4380837678909302  Recons: 0.44739532470703125  dkl : 0.04953442141413689\n",
      "Current step: 0  Loss: 2.068289041519165  Recons: 0.4706045985221863  dkl : 0.07988422363996506\n",
      "Current step: 30  Loss: 1.291008472442627  Recons: 0.5489867925643921  dkl : 0.03710108622908592\n",
      "Current step: 60  Loss: 1.326167106628418  Recons: 0.37252992391586304  dkl : 0.047681860625743866\n",
      "Current step: 0  Loss: 2.009357452392578  Recons: 0.45580989122390747  dkl : 0.07767738401889801\n",
      "Current step: 30  Loss: 1.1795110702514648  Recons: 0.4777185320854187  dkl : 0.03508962690830231\n",
      "Current step: 60  Loss: 1.307450294494629  Recons: 0.38869553804397583  dkl : 0.045937735587358475\n",
      "Current step: 0  Loss: 1.935306191444397  Recons: 0.42238473892211914  dkl : 0.07564607262611389\n",
      "Current step: 30  Loss: 1.1606208086013794  Recons: 0.49419647455215454  dkl : 0.03332121670246124\n",
      "Current step: 60  Loss: 1.268899917602539  Recons: 0.38330715894699097  dkl : 0.044279634952545166\n",
      "Saving Model30.torch\n",
      "Current step: 0  Loss: 1.8629698753356934  Recons: 0.3960529565811157  dkl : 0.073345847427845\n",
      "Current step: 30  Loss: 1.111136794090271  Recons: 0.4805700182914734  dkl : 0.03152833878993988\n",
      "Current step: 60  Loss: 1.2325462102890015  Recons: 0.37501388788223267  dkl : 0.04287661612033844\n",
      "Current step: 0  Loss: 1.8564395904541016  Recons: 0.4317639470100403  dkl : 0.07123377919197083\n",
      "Current step: 30  Loss: 1.0901275873184204  Recons: 0.49258309602737427  dkl : 0.029877224937081337\n",
      "Current step: 60  Loss: 1.266310214996338  Recons: 0.43498694896698  dkl : 0.041566163301467896\n",
      "Current step: 0  Loss: 1.8315508365631104  Recons: 0.4524877071380615  dkl : 0.06895315647125244\n",
      "Current step: 30  Loss: 1.0398640632629395  Recons: 0.4748637080192566  dkl : 0.028250014409422874\n",
      "Current step: 60  Loss: 1.2005770206451416  Recons: 0.39349979162216187  dkl : 0.04035385698080063\n",
      "Current step: 0  Loss: 1.7644805908203125  Recons: 0.4243801236152649  dkl : 0.06700502336025238\n",
      "Current step: 30  Loss: 1.0162653923034668  Recons: 0.4786751866340637  dkl : 0.026879508048295975\n",
      "Current step: 60  Loss: 1.186056137084961  Recons: 0.39939844608306885  dkl : 0.039332885295152664\n",
      "Current step: 0  Loss: 1.7191059589385986  Recons: 0.42171984910964966  dkl : 0.06486930698156357\n",
      "Current step: 30  Loss: 0.964148223400116  Recons: 0.4550085663795471  dkl : 0.025456983596086502\n",
      "Current step: 60  Loss: 1.1683900356292725  Recons: 0.4053891897201538  dkl : 0.03815004229545593\n",
      "Saving Model35.torch\n",
      "Current step: 0  Loss: 1.6188743114471436  Recons: 0.3573368191719055  dkl : 0.06307687610387802\n",
      "Current step: 30  Loss: 0.9717354774475098  Recons: 0.4874424338340759  dkl : 0.024214651435613632\n",
      "Current step: 60  Loss: 1.0913732051849365  Recons: 0.34762173891067505  dkl : 0.03718757629394531\n",
      "Current step: 0  Loss: 1.5867587327957153  Recons: 0.365329384803772  dkl : 0.06107147037982941\n",
      "Current step: 30  Loss: 0.929198145866394  Recons: 0.46858102083206177  dkl : 0.023030856624245644\n",
      "Current step: 60  Loss: 1.1161940097808838  Recons: 0.3889632225036621  dkl : 0.03636154159903526\n",
      "Current step: 0  Loss: 1.5430543422698975  Recons: 0.3633306622505188  dkl : 0.058986179530620575\n",
      "Current step: 30  Loss: 0.8959414958953857  Recons: 0.4553549289703369  dkl : 0.022029327228665352\n",
      "Current step: 60  Loss: 1.1361658573150635  Recons: 0.4245487451553345  dkl : 0.03558085858821869\n",
      "Current step: 0  Loss: 1.5039199590682983  Recons: 0.36470699310302734  dkl : 0.05696064606308937\n",
      "Current step: 30  Loss: 0.852716326713562  Recons: 0.42624783515930176  dkl : 0.021323423832654953\n",
      "Current step: 60  Loss: 1.0634350776672363  Recons: 0.3715054392814636  dkl : 0.034596480429172516\n",
      "Current step: 0  Loss: 1.5168757438659668  Recons: 0.4154694676399231  dkl : 0.05507031828165054\n",
      "Current step: 30  Loss: 0.8482582569122314  Recons: 0.4345299005508423  dkl : 0.020686419680714607\n",
      "Current step: 60  Loss: 1.053031086921692  Recons: 0.38098275661468506  dkl : 0.03360241651535034\n",
      "Saving Model40.torch\n",
      "Current step: 0  Loss: 1.420975923538208  Recons: 0.3556951880455017  dkl : 0.053264036774635315\n",
      "Current step: 30  Loss: 0.8887835741043091  Recons: 0.4852467179298401  dkl : 0.0201768409460783\n",
      "Current step: 60  Loss: 1.0119848251342773  Recons: 0.3589377999305725  dkl : 0.0326523520052433\n",
      "Current step: 0  Loss: 1.4341638088226318  Recons: 0.40526026487350464  dkl : 0.05144517868757248\n",
      "Current step: 30  Loss: 0.8664230108261108  Recons: 0.4723019003868103  dkl : 0.019706055521965027\n",
      "Current step: 60  Loss: 0.9769335389137268  Recons: 0.34561336040496826  dkl : 0.03156600892543793\n",
      "Current step: 0  Loss: 1.4155454635620117  Recons: 0.4110441207885742  dkl : 0.050225067883729935\n",
      "Current step: 30  Loss: 0.8740688562393188  Recons: 0.48722195625305176  dkl : 0.019342346116900444\n",
      "Current step: 60  Loss: 1.012411117553711  Recons: 0.3896721601486206  dkl : 0.031136950477957726\n",
      "Current step: 0  Loss: 1.3112733364105225  Recons: 0.3546752333641052  dkl : 0.04782990738749504\n",
      "Current step: 30  Loss: 0.8243108987808228  Recons: 0.4498453140258789  dkl : 0.018723279237747192\n",
      "Current step: 60  Loss: 0.9889124035835266  Recons: 0.36703550815582275  dkl : 0.031093845143914223\n",
      "Current step: 0  Loss: 1.289379596710205  Recons: 0.3797034025192261  dkl : 0.04548380896449089\n",
      "Current step: 30  Loss: 0.8504320383071899  Recons: 0.4772425889968872  dkl : 0.018659472465515137\n",
      "Current step: 60  Loss: 0.9535505771636963  Recons: 0.3786364197731018  dkl : 0.028745708987116814\n",
      "Saving Model45.torch\n",
      "Current step: 0  Loss: 1.2969865798950195  Recons: 0.38627350330352783  dkl : 0.04553565755486488\n",
      "Current step: 30  Loss: 0.7947266101837158  Recons: 0.4365946054458618  dkl : 0.01790659874677658\n",
      "Current step: 60  Loss: 0.8890143632888794  Recons: 0.3403578996658325  dkl : 0.027432821691036224\n",
      "Current step: 0  Loss: 1.2522454261779785  Recons: 0.3788958787918091  dkl : 0.04366748034954071\n",
      "Current step: 30  Loss: 0.7968583106994629  Recons: 0.4351065754890442  dkl : 0.018087588250637054\n",
      "Current step: 60  Loss: 0.9041765332221985  Recons: 0.3503301739692688  dkl : 0.027692317962646484\n",
      "Current step: 0  Loss: 1.165696620941162  Recons: 0.3337695002555847  dkl : 0.04159635305404663\n",
      "Current step: 30  Loss: 0.8068612813949585  Recons: 0.4670432209968567  dkl : 0.016990901902318\n",
      "Current step: 60  Loss: 0.8811385035514832  Recons: 0.3356468081474304  dkl : 0.027274584397673607\n",
      "Current step: 0  Loss: 1.178649663925171  Recons: 0.3553653359413147  dkl : 0.04116421192884445\n",
      "Current step: 30  Loss: 0.8093061447143555  Recons: 0.4477996230125427  dkl : 0.018075324594974518\n",
      "Current step: 60  Loss: 0.8206550478935242  Recons: 0.3198506236076355  dkl : 0.025040220469236374\n",
      "Current step: 0  Loss: 1.174192190170288  Recons: 0.3734467029571533  dkl : 0.04003727063536644\n",
      "Current step: 30  Loss: 0.8259029984474182  Recons: 0.4677976965904236  dkl : 0.01790526509284973\n",
      "Current step: 60  Loss: 0.8169446587562561  Recons: 0.32781606912612915  dkl : 0.024456430226564407\n",
      "Saving Model50.torch\n",
      "Current step: 0  Loss: 1.1537110805511475  Recons: 0.35206079483032227  dkl : 0.04008251056075096\n",
      "Current step: 30  Loss: 0.7758085131645203  Recons: 0.4448944926261902  dkl : 0.016545701771974564\n",
      "Current step: 60  Loss: 0.7764659523963928  Recons: 0.31812554597854614  dkl : 0.022917021065950394\n",
      "Current step: 0  Loss: 1.0957558155059814  Recons: 0.34088653326034546  dkl : 0.0377434641122818\n",
      "Current step: 30  Loss: 0.7421517372131348  Recons: 0.44480425119400024  dkl : 0.014867372810840607\n",
      "Current step: 60  Loss: 0.7838432192802429  Recons: 0.3229835629463196  dkl : 0.023042982444167137\n",
      "Current step: 0  Loss: 1.0866422653198242  Recons: 0.3529178500175476  dkl : 0.03668622300028801\n",
      "Current step: 30  Loss: 0.744210958480835  Recons: 0.4503614902496338  dkl : 0.014692471362650394\n",
      "Current step: 60  Loss: 0.7488061785697937  Recons: 0.3086640238761902  dkl : 0.022007107734680176\n",
      "Current step: 0  Loss: 1.0855326652526855  Recons: 0.37276405096054077  dkl : 0.03563842922449112\n",
      "Current step: 30  Loss: 0.7245004177093506  Recons: 0.45544856786727905  dkl : 0.013452592305839062\n",
      "Current step: 60  Loss: 0.7442827820777893  Recons: 0.3155737519264221  dkl : 0.0214354507625103\n",
      "Current step: 0  Loss: 1.0427604913711548  Recons: 0.3556917905807495  dkl : 0.034353435039520264\n",
      "Current step: 30  Loss: 0.7335426807403564  Recons: 0.4685811400413513  dkl : 0.013248075731098652\n",
      "Current step: 60  Loss: 0.7461019158363342  Recons: 0.3316510319709778  dkl : 0.020722543820738792\n",
      "Saving Model55.torch\n",
      "Current step: 0  Loss: 1.0034370422363281  Recons: 0.32976245880126953  dkl : 0.03368372842669487\n",
      "Current step: 30  Loss: 0.7271769046783447  Recons: 0.47006452083587646  dkl : 0.012855621054768562\n",
      "Current step: 60  Loss: 0.6773911714553833  Recons: 0.27774935960769653  dkl : 0.019982092082500458\n",
      "Current step: 0  Loss: 0.9808334112167358  Recons: 0.3301026225090027  dkl : 0.03253654018044472\n",
      "Current step: 30  Loss: 0.6975661516189575  Recons: 0.44357818365097046  dkl : 0.012699398212134838\n",
      "Current step: 60  Loss: 0.7044273018836975  Recons: 0.3276141285896301  dkl : 0.01884065940976143\n",
      "Current step: 0  Loss: 0.9538859128952026  Recons: 0.33358556032180786  dkl : 0.03101501800119877\n",
      "Current step: 30  Loss: 0.6982336044311523  Recons: 0.4408571124076843  dkl : 0.01286882534623146\n",
      "Current step: 60  Loss: 0.6844068765640259  Recons: 0.32070159912109375  dkl : 0.018185265362262726\n",
      "Current step: 0  Loss: 0.9452564120292664  Recons: 0.31725555658340454  dkl : 0.03140004351735115\n",
      "Current step: 30  Loss: 0.7113422751426697  Recons: 0.47728967666625977  dkl : 0.011702630668878555\n",
      "Current step: 60  Loss: 0.6708142757415771  Recons: 0.30551719665527344  dkl : 0.018264854326844215\n",
      "Current step: 0  Loss: 0.9515799880027771  Recons: 0.34967076778411865  dkl : 0.030095461755990982\n",
      "Current step: 30  Loss: 0.678431510925293  Recons: 0.4483135938644409  dkl : 0.011505896225571632\n",
      "Current step: 60  Loss: 0.6432313919067383  Recons: 0.2913181185722351  dkl : 0.017595665529370308\n",
      "Saving Model60.torch\n",
      "Current step: 0  Loss: 0.9187560677528381  Recons: 0.33882004022598267  dkl : 0.028996802866458893\n",
      "Current step: 30  Loss: 0.6394015550613403  Recons: 0.43245750665664673  dkl : 0.01034720428287983\n",
      "Current step: 60  Loss: 0.6396980881690979  Recons: 0.29115432500839233  dkl : 0.01742718741297722\n",
      "Current step: 0  Loss: 0.860379159450531  Recons: 0.3090784549713135  dkl : 0.027565035969018936\n",
      "Current step: 30  Loss: 0.6915910840034485  Recons: 0.47772103548049927  dkl : 0.010693501681089401\n",
      "Current step: 60  Loss: 0.625607430934906  Recons: 0.2894133925437927  dkl : 0.016809701919555664\n",
      "Current step: 0  Loss: 0.8449766635894775  Recons: 0.3249983787536621  dkl : 0.025998912751674652\n",
      "Current step: 30  Loss: 0.6559203267097473  Recons: 0.46094805002212524  dkl : 0.009748613461852074\n",
      "Current step: 60  Loss: 0.5856064558029175  Recons: 0.25963759422302246  dkl : 0.016298441216349602\n",
      "Current step: 0  Loss: 0.8237101435661316  Recons: 0.32848888635635376  dkl : 0.024761062115430832\n",
      "Current step: 30  Loss: 0.6319481134414673  Recons: 0.4371510148048401  dkl : 0.00973985344171524\n",
      "Current step: 60  Loss: 0.5637465715408325  Recons: 0.24477261304855347  dkl : 0.015948696061968803\n",
      "Current step: 0  Loss: 0.8070390224456787  Recons: 0.3277125954627991  dkl : 0.023966319859027863\n",
      "Current step: 30  Loss: 0.631731390953064  Recons: 0.4362184405326843  dkl : 0.009775646962225437\n",
      "Current step: 60  Loss: 0.5584484338760376  Recons: 0.2659227252006531  dkl : 0.014626285061240196\n",
      "Saving Model65.torch\n",
      "Current step: 0  Loss: 0.7808640003204346  Recons: 0.31979697942733765  dkl : 0.023053349927067757\n",
      "Current step: 30  Loss: 0.6873626708984375  Recons: 0.4249640107154846  dkl : 0.01311993133276701\n",
      "Current step: 60  Loss: 0.5482670068740845  Recons: 0.2789619565010071  dkl : 0.013465254567563534\n",
      "Current step: 0  Loss: 0.8579727411270142  Recons: 0.31524181365966797  dkl : 0.02713654562830925\n",
      "Current step: 30  Loss: 0.6305835247039795  Recons: 0.4237803816795349  dkl : 0.010340157896280289\n",
      "Current step: 60  Loss: 0.5250234603881836  Recons: 0.25332170724868774  dkl : 0.013585085980594158\n",
      "Current step: 0  Loss: 0.7769089341163635  Recons: 0.30036377906799316  dkl : 0.023827258497476578\n",
      "Current step: 30  Loss: 0.608670711517334  Recons: 0.43215471506118774  dkl : 0.008825799450278282\n",
      "Current step: 60  Loss: 0.5105121731758118  Recons: 0.2472395896911621  dkl : 0.013163628987967968\n",
      "Current step: 0  Loss: 0.7507323026657104  Recons: 0.3085632920265198  dkl : 0.022108452394604683\n",
      "Current step: 30  Loss: 0.5841233730316162  Recons: 0.4337472915649414  dkl : 0.00751880556344986\n",
      "Current step: 60  Loss: 0.48380550742149353  Recons: 0.2293042540550232  dkl : 0.012725062668323517\n",
      "Current step: 0  Loss: 0.7412929534912109  Recons: 0.3143361210823059  dkl : 0.02134784311056137\n",
      "Current step: 30  Loss: 0.5681300163269043  Recons: 0.4271804690361023  dkl : 0.0070474762469530106\n",
      "Current step: 60  Loss: 0.4860559403896332  Recons: 0.2314314842224121  dkl : 0.012731222435832024\n",
      "Saving Model70.torch\n",
      "Current step: 0  Loss: 0.7185776233673096  Recons: 0.3109455108642578  dkl : 0.020381605252623558\n",
      "Current step: 30  Loss: 0.6021645069122314  Recons: 0.44746071100234985  dkl : 0.00773518905043602\n",
      "Current step: 60  Loss: 0.49275630712509155  Recons: 0.2504262924194336  dkl : 0.012116500176489353\n",
      "Current step: 0  Loss: 0.6779382228851318  Recons: 0.29608315229415894  dkl : 0.019092755392193794\n",
      "Current step: 30  Loss: 0.6120846271514893  Recons: 0.4364192485809326  dkl : 0.008783268742263317\n",
      "Current step: 60  Loss: 0.5031358003616333  Recons: 0.2579646110534668  dkl : 0.012258557602763176\n",
      "Current step: 0  Loss: 0.6372158527374268  Recons: 0.3045024871826172  dkl : 0.01663566753268242\n",
      "Current step: 30  Loss: 0.5958619117736816  Recons: 0.4185449481010437  dkl : 0.008865848183631897\n",
      "Current step: 60  Loss: 0.4775213599205017  Recons: 0.25124692916870117  dkl : 0.011313721537590027\n",
      "Current step: 0  Loss: 0.6347329616546631  Recons: 0.32481616735458374  dkl : 0.015495837666094303\n",
      "Current step: 30  Loss: 0.6177000999450684  Recons: 0.43555670976638794  dkl : 0.00910717062652111\n",
      "Current step: 60  Loss: 0.44851595163345337  Recons: 0.2309543490409851  dkl : 0.010878080502152443\n",
      "Current step: 0  Loss: 0.6070736050605774  Recons: 0.30384522676467896  dkl : 0.015161419287323952\n",
      "Current step: 30  Loss: 0.5801601409912109  Recons: 0.4169178009033203  dkl : 0.00816211849451065\n",
      "Current step: 60  Loss: 0.4758685827255249  Recons: 0.26974231004714966  dkl : 0.010306314565241337\n",
      "Saving Model75.torch\n",
      "Current step: 0  Loss: 0.5941754579544067  Recons: 0.3005075454711914  dkl : 0.014683393761515617\n",
      "Current step: 30  Loss: 0.5552810430526733  Recons: 0.40849846601486206  dkl : 0.007339129224419594\n",
      "Current step: 60  Loss: 0.41719087958335876  Recons: 0.22116166353225708  dkl : 0.009801460430026054\n",
      "Current step: 0  Loss: 0.6047688126564026  Recons: 0.3002898097038269  dkl : 0.015223950147628784\n",
      "Current step: 30  Loss: 0.5559055209159851  Recons: 0.41528576612472534  dkl : 0.0070309871807694435\n",
      "Current step: 60  Loss: 0.4079725742340088  Recons: 0.22627466917037964  dkl : 0.009084896184504032\n",
      "Current step: 0  Loss: 0.5964367985725403  Recons: 0.2735617160797119  dkl : 0.01614375412464142\n",
      "Current step: 30  Loss: 0.567254900932312  Recons: 0.4211726784706116  dkl : 0.007304111495614052\n",
      "Current step: 60  Loss: 0.4078803062438965  Recons: 0.2363581657409668  dkl : 0.008576106280088425\n",
      "Current step: 0  Loss: 0.599801242351532  Recons: 0.2931625247001648  dkl : 0.0153319351375103\n",
      "Current step: 30  Loss: 0.5630284547805786  Recons: 0.40121692419052124  dkl : 0.008090575225651264\n",
      "Current step: 60  Loss: 0.41311272978782654  Recons: 0.24295490980148315  dkl : 0.00850789062678814\n",
      "Current step: 0  Loss: 0.5652527809143066  Recons: 0.28454089164733887  dkl : 0.014035593718290329\n",
      "Current step: 30  Loss: 0.5619044303894043  Recons: 0.4113357663154602  dkl : 0.0075284321792423725\n",
      "Current step: 60  Loss: 0.4028307795524597  Recons: 0.22254091501235962  dkl : 0.009014492854475975\n",
      "Saving Model80.torch\n",
      "Current step: 0  Loss: 0.5604581832885742  Recons: 0.2872301936149597  dkl : 0.013661397621035576\n",
      "Current step: 30  Loss: 0.658723771572113  Recons: 0.4657139182090759  dkl : 0.009650493040680885\n",
      "Current step: 60  Loss: 0.42126670479774475  Recons: 0.2681202292442322  dkl : 0.007657323963940144\n",
      "Current step: 0  Loss: 0.5300583839416504  Recons: 0.29965949058532715  dkl : 0.011519944295287132\n",
      "Current step: 30  Loss: 0.5623970031738281  Recons: 0.40882962942123413  dkl : 0.007678368128836155\n",
      "Current step: 60  Loss: 0.38406407833099365  Recons: 0.22909873723983765  dkl : 0.0077482666820287704\n",
      "Current step: 0  Loss: 0.5008511543273926  Recons: 0.2846689820289612  dkl : 0.01080910861492157\n",
      "Current step: 30  Loss: 0.5405170321464539  Recons: 0.4063790440559387  dkl : 0.006706899963319302\n",
      "Current step: 60  Loss: 0.3573530912399292  Recons: 0.2142215371131897  dkl : 0.007156576961278915\n",
      "Current step: 0  Loss: 0.49581244587898254  Recons: 0.2780911326408386  dkl : 0.010886065661907196\n",
      "Current step: 30  Loss: 0.5322684049606323  Recons: 0.4080941081047058  dkl : 0.006208715494722128\n",
      "Current step: 60  Loss: 0.35568660497665405  Recons: 0.2175140380859375  dkl : 0.006908628158271313\n",
      "Current step: 0  Loss: 0.5167936682701111  Recons: 0.2924935221672058  dkl : 0.011215007863938808\n",
      "Current step: 30  Loss: 0.5318766236305237  Recons: 0.40557265281677246  dkl : 0.006315198261290789\n",
      "Current step: 60  Loss: 0.33231717348098755  Recons: 0.20187503099441528  dkl : 0.006522106938064098\n",
      "Saving Model85.torch\n",
      "Current step: 0  Loss: 0.5229265689849854  Recons: 0.28615784645080566  dkl : 0.011838437989354134\n",
      "Current step: 30  Loss: 0.5292568206787109  Recons: 0.3934834599494934  dkl : 0.006788667291402817\n",
      "Current step: 60  Loss: 0.3491567373275757  Recons: 0.21788352727890015  dkl : 0.006563659757375717\n",
      "Current step: 0  Loss: 0.5112095475196838  Recons: 0.2773878574371338  dkl : 0.011691084131598473\n",
      "Current step: 30  Loss: 0.5247815251350403  Recons: 0.3907245993614197  dkl : 0.006702845916152\n",
      "Current step: 60  Loss: 0.3374485373497009  Recons: 0.21282362937927246  dkl : 0.006231245584785938\n",
      "Current step: 0  Loss: 0.4851927161216736  Recons: 0.27718955278396606  dkl : 0.010400157421827316\n",
      "Current step: 30  Loss: 0.5275930762290955  Recons: 0.41518324613571167  dkl : 0.005620491690933704\n",
      "Current step: 60  Loss: 0.3411857485771179  Recons: 0.21577239036560059  dkl : 0.006270667538046837\n",
      "Current step: 0  Loss: 0.4766223728656769  Recons: 0.2852932810783386  dkl : 0.009566454216837883\n",
      "Current step: 30  Loss: 0.5216232538223267  Recons: 0.4139387607574463  dkl : 0.005384225398302078\n",
      "Current step: 60  Loss: 0.35221636295318604  Recons: 0.23056912422180176  dkl : 0.006082361098378897\n",
      "Current step: 0  Loss: 0.4751470983028412  Recons: 0.27942752838134766  dkl : 0.009785978123545647\n",
      "Current step: 30  Loss: 0.5518105030059814  Recons: 0.4368053078651428  dkl : 0.005750259850174189\n",
      "Current step: 60  Loss: 0.4093220829963684  Recons: 0.2856617569923401  dkl : 0.006183016579598188\n",
      "Saving Model90.torch\n",
      "Current step: 0  Loss: 0.49912774562835693  Recons: 0.30165010690689087  dkl : 0.009873881004750729\n",
      "Current step: 30  Loss: 0.5180480480194092  Recons: 0.39859867095947266  dkl : 0.005972467362880707\n",
      "Current step: 60  Loss: 0.3461046814918518  Recons: 0.2171638011932373  dkl : 0.00644704420119524\n",
      "Current step: 0  Loss: 0.4900159239768982  Recons: 0.2768546938896179  dkl : 0.010658061131834984\n",
      "Current step: 30  Loss: 0.5216956734657288  Recons: 0.4015428423881531  dkl : 0.006007642485201359\n",
      "Current step: 60  Loss: 0.3369365334510803  Recons: 0.20798951387405396  dkl : 0.006447351071983576\n",
      "Current step: 0  Loss: 0.4918069541454315  Recons: 0.28903913497924805  dkl : 0.010138390585780144\n",
      "Current step: 30  Loss: 0.5057837963104248  Recons: 0.3923458456993103  dkl : 0.00567189697176218\n",
      "Current step: 60  Loss: 0.3216877579689026  Recons: 0.20741909742355347  dkl : 0.0057134320959448814\n",
      "Current step: 0  Loss: 0.45198142528533936  Recons: 0.28215914964675903  dkl : 0.00849111471325159\n",
      "Current step: 30  Loss: 0.5152082443237305  Recons: 0.4018647074699402  dkl : 0.00566717516630888\n",
      "Current step: 60  Loss: 0.32293474674224854  Recons: 0.20586133003234863  dkl : 0.00585367064923048\n",
      "Current step: 0  Loss: 0.4542050361633301  Recons: 0.28559714555740356  dkl : 0.00843039434403181\n",
      "Current step: 30  Loss: 0.5450255870819092  Recons: 0.3866574168205261  dkl : 0.007918407209217548\n",
      "Current step: 60  Loss: 0.30393660068511963  Recons: 0.20165401697158813  dkl : 0.005114128813147545\n",
      "Saving Model95.torch\n",
      "Current step: 0  Loss: 0.47395893931388855  Recons: 0.27415281534194946  dkl : 0.009990306571125984\n",
      "Current step: 30  Loss: 0.5012492537498474  Recons: 0.38270002603530884  dkl : 0.005927460268139839\n",
      "Current step: 60  Loss: 0.3034778833389282  Recons: 0.2085166573524475  dkl : 0.00474806223064661\n",
      "Current step: 0  Loss: 0.47397565841674805  Recons: 0.28136056661605835  dkl : 0.009630754590034485\n",
      "Current step: 30  Loss: 0.5803536772727966  Recons: 0.39452892541885376  dkl : 0.009291238151490688\n",
      "Current step: 60  Loss: 0.31563401222229004  Recons: 0.21351152658462524  dkl : 0.0051061250269412994\n",
      "Current step: 0  Loss: 0.6401435136795044  Recons: 0.27338534593582153  dkl : 0.018337907269597054\n",
      "Current step: 30  Loss: 0.5917916297912598  Recons: 0.38897448778152466  dkl : 0.010140855796635151\n",
      "Current step: 60  Loss: 0.3061676621437073  Recons: 0.20924711227416992  dkl : 0.004846027120947838\n",
      "Current step: 0  Loss: 0.6446853876113892  Recons: 0.30249929428100586  dkl : 0.017109306529164314\n",
      "Current step: 30  Loss: 0.675803005695343  Recons: 0.38513559103012085  dkl : 0.014533370733261108\n",
      "Current step: 60  Loss: 0.32650136947631836  Recons: 0.22001981735229492  dkl : 0.005324077792465687\n",
      "Current step: 0  Loss: 0.6105588674545288  Recons: 0.28235721588134766  dkl : 0.016410082578659058\n",
      "Current step: 30  Loss: 0.6294915676116943  Recons: 0.37807756662368774  dkl : 0.0125706996768713\n",
      "Current step: 60  Loss: 0.31778484582901  Recons: 0.21250391006469727  dkl : 0.005264046136289835\n",
      "Saving Model100.torch\n",
      "Current step: 0  Loss: 0.44486308097839355  Recons: 0.2685360908508301  dkl : 0.008816349320113659\n",
      "Current step: 30  Loss: 0.6289610862731934  Recons: 0.4121106266975403  dkl : 0.010842523537576199\n",
      "Current step: 60  Loss: 0.29943615198135376  Recons: 0.20578408241271973  dkl : 0.004682604223489761\n",
      "Current step: 0  Loss: 0.4573274850845337  Recons: 0.27204465866088867  dkl : 0.009264140389859676\n",
      "Current step: 30  Loss: 0.6217130422592163  Recons: 0.4035457968711853  dkl : 0.01090836152434349\n",
      "Current step: 60  Loss: 0.28113675117492676  Recons: 0.1969429850578308  dkl : 0.004209688398987055\n",
      "Current step: 0  Loss: 0.4428968131542206  Recons: 0.2719307541847229  dkl : 0.008548302575945854\n",
      "Current step: 30  Loss: 0.5591667890548706  Recons: 0.3851432800292969  dkl : 0.008701176382601261\n",
      "Current step: 60  Loss: 0.28311464190483093  Recons: 0.20440715551376343  dkl : 0.0039353747852146626\n",
      "Current step: 0  Loss: 0.4512037932872772  Recons: 0.2676352858543396  dkl : 0.009178425185382366\n",
      "Current step: 30  Loss: 0.5170515775680542  Recons: 0.3795351982116699  dkl : 0.006875819060951471\n",
      "Current step: 60  Loss: 0.2761242985725403  Recons: 0.19927018880844116  dkl : 0.003842706326395273\n",
      "Current step: 0  Loss: 0.43859273195266724  Recons: 0.2699277997016907  dkl : 0.008433246985077858\n",
      "Current step: 30  Loss: 0.5141277313232422  Recons: 0.383716881275177  dkl : 0.006520543247461319\n",
      "Current step: 60  Loss: 0.2663424015045166  Recons: 0.19410759210586548  dkl : 0.0036117411218583584\n",
      "Saving Model105.torch\n",
      "Current step: 0  Loss: 0.5022871494293213  Recons: 0.27475160360336304  dkl : 0.011376777663826942\n",
      "Current step: 30  Loss: 0.6319389343261719  Recons: 0.37434810400009155  dkl : 0.012879541143774986\n",
      "Current step: 60  Loss: 0.412881463766098  Recons: 0.20871597528457642  dkl : 0.010208274237811565\n",
      "Current step: 0  Loss: 0.5420546531677246  Recons: 0.2805333137512207  dkl : 0.013076066970825195\n",
      "Current step: 30  Loss: 0.8505700826644897  Recons: 0.3900511860847473  dkl : 0.02302594482898712\n",
      "Current step: 60  Loss: 0.27859121561050415  Recons: 0.19716304540634155  dkl : 0.004071409348398447\n",
      "Current step: 0  Loss: 0.6967288255691528  Recons: 0.2803525924682617  dkl : 0.020818810909986496\n",
      "Current step: 30  Loss: 0.6412795782089233  Recons: 0.38274747133255005  dkl : 0.012926604598760605\n",
      "Current step: 60  Loss: 0.3143393099308014  Recons: 0.19586163759231567  dkl : 0.005923883989453316\n",
      "Current step: 0  Loss: 0.47932010889053345  Recons: 0.2762783169746399  dkl : 0.010152090340852737\n",
      "Current step: 30  Loss: 0.572923481464386  Recons: 0.3799894452095032  dkl : 0.0096467025578022\n",
      "Current step: 60  Loss: 0.2893465757369995  Recons: 0.2253119945526123  dkl : 0.003201729618012905\n",
      "Current step: 0  Loss: 0.5139408707618713  Recons: 0.359672486782074  dkl : 0.007713419385254383\n",
      "Current step: 30  Loss: 0.5918983817100525  Recons: 0.38224005699157715  dkl : 0.010482915677130222\n",
      "Current step: 60  Loss: 0.2827264666557312  Recons: 0.20325583219528198  dkl : 0.003973531536757946\n",
      "Saving Model110.torch\n",
      "Current step: 0  Loss: 0.5741205215454102  Recons: 0.2864842414855957  dkl : 0.014381815679371357\n",
      "Current step: 30  Loss: 0.6300239562988281  Recons: 0.4002600312232971  dkl : 0.01148819737136364\n",
      "Current step: 60  Loss: 0.2810485363006592  Recons: 0.20347923040390015  dkl : 0.003878465387970209\n",
      "Current step: 0  Loss: 0.48041626811027527  Recons: 0.28588104248046875  dkl : 0.009726760908961296\n",
      "Current step: 30  Loss: 0.6147388815879822  Recons: 0.379599392414093  dkl : 0.011756975203752518\n",
      "Current step: 60  Loss: 0.3051805794239044  Recons: 0.1982704997062683  dkl : 0.005345504265278578\n",
      "Current step: 0  Loss: 0.45021504163742065  Recons: 0.2790539860725403  dkl : 0.008558053523302078\n",
      "Current step: 30  Loss: 0.5355125069618225  Recons: 0.377005398273468  dkl : 0.007925355806946754\n",
      "Current step: 60  Loss: 0.3314843773841858  Recons: 0.20057600736618042  dkl : 0.006545417942106724\n",
      "Current step: 0  Loss: 0.4638146460056305  Recons: 0.2762663960456848  dkl : 0.009377412497997284\n",
      "Current step: 30  Loss: 0.5263874530792236  Recons: 0.3834614157676697  dkl : 0.007146300282329321\n",
      "Current step: 60  Loss: 0.3300408720970154  Recons: 0.19835251569747925  dkl : 0.006584418471902609\n",
      "Current step: 0  Loss: 0.4467330873012543  Recons: 0.2768046259880066  dkl : 0.008496423251926899\n",
      "Current step: 30  Loss: 0.5554735660552979  Recons: 0.37810665369033813  dkl : 0.008868344128131866\n",
      "Current step: 60  Loss: 0.3407886028289795  Recons: 0.19710683822631836  dkl : 0.007184088230133057\n",
      "Saving Model115.torch\n",
      "Current step: 0  Loss: 0.44581133127212524  Recons: 0.2766602039337158  dkl : 0.008457556366920471\n",
      "Current step: 30  Loss: 0.5539277791976929  Recons: 0.37525004148483276  dkl : 0.008933886885643005\n",
      "Current step: 60  Loss: 0.33045536279678345  Recons: 0.1966286301612854  dkl : 0.006691335700452328\n",
      "Current step: 0  Loss: 0.4640445411205292  Recons: 0.293769896030426  dkl : 0.008513731881976128\n",
      "Current step: 30  Loss: 0.5967574119567871  Recons: 0.3775833249092102  dkl : 0.010958703234791756\n",
      "Current step: 60  Loss: 0.30610501766204834  Recons: 0.19415885210037231  dkl : 0.005597308743745089\n",
      "Current step: 0  Loss: 0.4511736035346985  Recons: 0.27158039808273315  dkl : 0.008979659527540207\n",
      "Current step: 30  Loss: 0.5292248725891113  Recons: 0.3741213083267212  dkl : 0.0077551789581775665\n",
      "Current step: 60  Loss: 0.2777218222618103  Recons: 0.19835996627807617  dkl : 0.003968092612922192\n",
      "Current step: 0  Loss: 0.3938710689544678  Recons: 0.2724481225013733  dkl : 0.006071147974580526\n",
      "Current step: 30  Loss: 0.47460436820983887  Recons: 0.37823718786239624  dkl : 0.004818358924239874\n",
      "Current step: 60  Loss: 0.29004138708114624  Recons: 0.19834870100021362  dkl : 0.004584634676575661\n",
      "Current step: 0  Loss: 0.385436475276947  Recons: 0.2698444724082947  dkl : 0.0057795993052423\n",
      "Current step: 30  Loss: 0.47630470991134644  Recons: 0.376520574092865  dkl : 0.004989205859601498\n",
      "Current step: 60  Loss: 0.2893804907798767  Recons: 0.19480133056640625  dkl : 0.004728957079350948\n",
      "Saving Model120.torch\n",
      "Current step: 0  Loss: 0.391807496547699  Recons: 0.2647077441215515  dkl : 0.006354988552629948\n",
      "Current step: 30  Loss: 0.5506659746170044  Recons: 0.37432563304901123  dkl : 0.008817017078399658\n",
      "Current step: 60  Loss: 0.2618793249130249  Recons: 0.1941928267478943  dkl : 0.003384325420483947\n",
      "Current step: 0  Loss: 0.38948118686676025  Recons: 0.26780813932418823  dkl : 0.006083653308451176\n",
      "Current step: 30  Loss: 0.5436934232711792  Recons: 0.3744507431983948  dkl : 0.008462135680019855\n",
      "Current step: 60  Loss: 0.2864665985107422  Recons: 0.20070785284042358  dkl : 0.004287936724722385\n",
      "Current step: 0  Loss: 0.38120245933532715  Recons: 0.26314109563827515  dkl : 0.005903067998588085\n",
      "Current step: 30  Loss: 0.475685179233551  Recons: 0.3747859597206116  dkl : 0.0050449613481760025\n",
      "Current step: 60  Loss: 0.2925122380256653  Recons: 0.2051195502281189  dkl : 0.004369633737951517\n",
      "Current step: 0  Loss: 0.35129010677337646  Recons: 0.2705686688423157  dkl : 0.004036071710288525\n",
      "Current step: 30  Loss: 0.5641365051269531  Recons: 0.38045960664749146  dkl : 0.009183844551444054\n",
      "Current step: 60  Loss: 0.26868951320648193  Recons: 0.19130903482437134  dkl : 0.0038690241053700447\n",
      "Current step: 0  Loss: 0.3789615035057068  Recons: 0.2734997272491455  dkl : 0.005273088347166777\n",
      "Current step: 30  Loss: 0.539174497127533  Recons: 0.3747939467430115  dkl : 0.008219027891755104\n",
      "Current step: 60  Loss: 0.2786151170730591  Recons: 0.2014567255973816  dkl : 0.0038579199463129044\n",
      "Saving Model125.torch\n",
      "Current step: 0  Loss: 0.3542419672012329  Recons: 0.26229792833328247  dkl : 0.004597201012074947\n",
      "Current step: 30  Loss: 0.6363083124160767  Recons: 0.3842701315879822  dkl : 0.012601910158991814\n",
      "Current step: 60  Loss: 0.2681971788406372  Recons: 0.19429856538772583  dkl : 0.0036949310451745987\n",
      "Current step: 0  Loss: 0.4061359167098999  Recons: 0.28933173418045044  dkl : 0.0058402083814144135\n",
      "Current step: 30  Loss: 0.5759664177894592  Recons: 0.3787333369255066  dkl : 0.009861653670668602\n",
      "Current step: 60  Loss: 0.30884888768196106  Recons: 0.2127097249031067  dkl : 0.0048069581389427185\n",
      "Current step: 0  Loss: 0.40799397230148315  Recons: 0.27925699949264526  dkl : 0.0064368476159870625\n",
      "Current step: 30  Loss: 0.5879637598991394  Recons: 0.37652045488357544  dkl : 0.010572164319455624\n",
      "Current step: 60  Loss: 0.266990065574646  Recons: 0.20230746269226074  dkl : 0.0032341298647224903\n",
      "Current step: 0  Loss: 0.5662587285041809  Recons: 0.26580673456192017  dkl : 0.015022600069642067\n",
      "Current step: 30  Loss: 0.5536485910415649  Recons: 0.37488240003585815  dkl : 0.00893830880522728\n",
      "Current step: 60  Loss: 0.284495085477829  Recons: 0.1946675181388855  dkl : 0.004491378087550402\n",
      "Current step: 0  Loss: 0.45307499170303345  Recons: 0.26335835456848145  dkl : 0.009485832415521145\n",
      "Current step: 30  Loss: 0.5339748859405518  Recons: 0.3741380572319031  dkl : 0.007991841994225979\n",
      "Current step: 60  Loss: 0.25560736656188965  Recons: 0.19453424215316772  dkl : 0.003053656779229641\n",
      "Saving Model130.torch\n",
      "Current step: 0  Loss: 0.4172823429107666  Recons: 0.26380079984664917  dkl : 0.007674076594412327\n",
      "Current step: 30  Loss: 0.5276147723197937  Recons: 0.3731640577316284  dkl : 0.007722534704953432\n",
      "Current step: 60  Loss: 0.25468865036964417  Recons: 0.19392138719558716  dkl : 0.003038363764062524\n",
      "Current step: 0  Loss: 0.4684411287307739  Recons: 0.26566362380981445  dkl : 0.010138874873518944\n",
      "Current step: 30  Loss: 0.8589416742324829  Recons: 0.3746926784515381  dkl : 0.02421244978904724\n",
      "Current step: 60  Loss: 0.2518646717071533  Recons: 0.19494563341140747  dkl : 0.0028459527529776096\n",
      "Current step: 0  Loss: 0.47114861011505127  Recons: 0.27868062257766724  dkl : 0.009623399004340172\n",
      "Current step: 30  Loss: 0.6639496088027954  Recons: 0.3751153349876404  dkl : 0.014441714622080326\n",
      "Current step: 60  Loss: 0.27468931674957275  Recons: 0.19310975074768066  dkl : 0.004078978206962347\n",
      "Current step: 0  Loss: 0.43656125664711  Recons: 0.2629156708717346  dkl : 0.008682278916239738\n",
      "Current step: 30  Loss: 0.5248119831085205  Recons: 0.37512487173080444  dkl : 0.007484354078769684\n",
      "Current step: 60  Loss: 0.27519869804382324  Recons: 0.1922898292541504  dkl : 0.004145443439483643\n",
      "Current step: 0  Loss: 0.38596251606941223  Recons: 0.27156704664230347  dkl : 0.005719773471355438\n",
      "Current step: 30  Loss: 0.5932292342185974  Recons: 0.38155972957611084  dkl : 0.010583475232124329\n",
      "Current step: 60  Loss: 0.27025750279426575  Recons: 0.1905260682106018  dkl : 0.003986571915447712\n",
      "Saving Model135.torch\n",
      "Current step: 0  Loss: 0.435041218996048  Recons: 0.2864472270011902  dkl : 0.0074296994134783745\n",
      "Current step: 30  Loss: 0.49245843291282654  Recons: 0.3775429129600525  dkl : 0.005745775997638702\n",
      "Current step: 60  Loss: 0.2831301689147949  Recons: 0.1931409239768982  dkl : 0.004499461967498064\n",
      "Current step: 0  Loss: 0.46356719732284546  Recons: 0.27287644147872925  dkl : 0.009534538723528385\n",
      "Current step: 30  Loss: 0.46410879492759705  Recons: 0.3747212290763855  dkl : 0.0044693779200315475\n",
      "Current step: 60  Loss: 0.25832226872444153  Recons: 0.18940860033035278  dkl : 0.003445683280006051\n",
      "Current step: 0  Loss: 0.4480753540992737  Recons: 0.2628862261772156  dkl : 0.009259456768631935\n",
      "Current step: 30  Loss: 0.4658246636390686  Recons: 0.37516820430755615  dkl : 0.00453282380476594\n",
      "Current step: 60  Loss: 0.27111899852752686  Recons: 0.1921432614326477  dkl : 0.003948787227272987\n",
      "Current step: 0  Loss: 0.4096131920814514  Recons: 0.26253318786621094  dkl : 0.0073540001176297665\n",
      "Current step: 30  Loss: 0.4769010841846466  Recons: 0.3762168288230896  dkl : 0.005034212954342365\n",
      "Current step: 60  Loss: 0.33167654275894165  Recons: 0.24159425497055054  dkl : 0.004504113458096981\n",
      "Current step: 0  Loss: 0.3882122337818146  Recons: 0.29817962646484375  dkl : 0.004501630552113056\n",
      "Current step: 30  Loss: 0.5319749116897583  Recons: 0.39835894107818604  dkl : 0.006680797785520554\n",
      "Current step: 60  Loss: 0.3196926712989807  Recons: 0.23789721727371216  dkl : 0.004089771769940853\n",
      "Saving Model140.torch\n",
      "Current step: 0  Loss: 0.332255095243454  Recons: 0.2753596305847168  dkl : 0.0028447736985981464\n",
      "Current step: 30  Loss: 0.6236778497695923  Recons: 0.38605278730392456  dkl : 0.011881251819431782\n",
      "Current step: 60  Loss: 0.27230656147003174  Recons: 0.2116946578025818  dkl : 0.003030594438314438\n",
      "Current step: 0  Loss: 0.370892196893692  Recons: 0.2944936752319336  dkl : 0.0038199261762201786\n",
      "Current step: 30  Loss: 0.5752037763595581  Recons: 0.3900013566017151  dkl : 0.009260122664272785\n",
      "Current step: 60  Loss: 0.2584257125854492  Recons: 0.1912996768951416  dkl : 0.003356301924213767\n",
      "Current step: 0  Loss: 0.3670205771923065  Recons: 0.2579421401023865  dkl : 0.005453921388834715\n",
      "Current step: 30  Loss: 0.6250325441360474  Recons: 0.3758205771446228  dkl : 0.012460596859455109\n",
      "Current step: 60  Loss: 0.2692439556121826  Recons: 0.21819812059402466  dkl : 0.0025522916112095118\n",
      "Current step: 0  Loss: 0.42168059945106506  Recons: 0.26544398069381714  dkl : 0.007811831310391426\n",
      "Current step: 30  Loss: 0.7333716154098511  Recons: 0.3848617672920227  dkl : 0.01742549240589142\n",
      "Current step: 60  Loss: 0.3210703730583191  Recons: 0.1944987177848816  dkl : 0.00632858369499445\n",
      "Current step: 0  Loss: 0.3837078809738159  Recons: 0.26361316442489624  dkl : 0.006004735827445984\n",
      "Current step: 30  Loss: 0.596958577632904  Recons: 0.3752196431159973  dkl : 0.011086946353316307\n",
      "Current step: 60  Loss: 0.24586190283298492  Recons: 0.188107430934906  dkl : 0.0028877235017716885\n",
      "Saving Model145.torch\n",
      "Current step: 0  Loss: 0.55005943775177  Recons: 0.2746100425720215  dkl : 0.013772471807897091\n",
      "Current step: 30  Loss: 0.6662849187850952  Recons: 0.3738221526145935  dkl : 0.014623139053583145\n",
      "Current step: 60  Loss: 0.27173662185668945  Recons: 0.18864154815673828  dkl : 0.0041547538712620735\n",
      "Current step: 0  Loss: 0.5233364701271057  Recons: 0.26057857275009155  dkl : 0.013137895613908768\n",
      "Current step: 30  Loss: 0.7133957147598267  Recons: 0.3748054504394531  dkl : 0.016929512843489647\n",
      "Current step: 60  Loss: 0.306077778339386  Recons: 0.1848425269126892  dkl : 0.0060617635026574135\n",
      "Current step: 0  Loss: 0.4612628221511841  Recons: 0.27070826292037964  dkl : 0.009527727030217648\n",
      "Current step: 30  Loss: 0.677082359790802  Recons: 0.37325793504714966  dkl : 0.015191221609711647\n",
      "Current step: 60  Loss: 0.24990621209144592  Recons: 0.18796610832214355  dkl : 0.0030970049556344748\n",
      "Current step: 0  Loss: 0.44121408462524414  Recons: 0.2607307434082031  dkl : 0.00902416743338108\n",
      "Current step: 30  Loss: 0.6220602989196777  Recons: 0.3739810585975647  dkl : 0.012403963133692741\n",
      "Current step: 60  Loss: 0.32097822427749634  Recons: 0.18707555532455444  dkl : 0.006695132702589035\n",
      "Current step: 0  Loss: 0.42421841621398926  Recons: 0.27481985092163086  dkl : 0.007469927426427603\n",
      "Current step: 30  Loss: 0.5204142928123474  Recons: 0.37639689445495605  dkl : 0.00720087019726634\n",
      "Current step: 60  Loss: 0.2939046323299408  Recons: 0.1829959750175476  dkl : 0.0055454326793551445\n",
      "Saving Model150.torch\n",
      "Current step: 0  Loss: 0.42085763812065125  Recons: 0.270529568195343  dkl : 0.007516403216868639\n",
      "Current step: 30  Loss: 0.49678927659988403  Recons: 0.3768005967140198  dkl : 0.005999433342367411\n",
      "Current step: 60  Loss: 0.326068252325058  Recons: 0.19455736875534058  dkl : 0.006575544364750385\n",
      "Current step: 0  Loss: 0.40696483850479126  Recons: 0.2668004631996155  dkl : 0.007008219137787819\n",
      "Current step: 30  Loss: 0.5823073387145996  Recons: 0.3729429841041565  dkl : 0.01046821940690279\n",
      "Current step: 60  Loss: 0.31256264448165894  Recons: 0.18525105714797974  dkl : 0.006365580018609762\n",
      "Current step: 0  Loss: 0.3608575463294983  Recons: 0.2629857063293457  dkl : 0.004893591161817312\n",
      "Current step: 30  Loss: 0.6311321258544922  Recons: 0.3727582097053528  dkl : 0.01291869767010212\n",
      "Current step: 60  Loss: 0.33585673570632935  Recons: 0.18132925033569336  dkl : 0.007726374547928572\n",
      "Current step: 0  Loss: 0.3850269913673401  Recons: 0.26270246505737305  dkl : 0.006116226315498352\n",
      "Current step: 30  Loss: 0.57594895362854  Recons: 0.3731697201728821  dkl : 0.010138960555195808\n",
      "Current step: 60  Loss: 0.28829291462898254  Recons: 0.18190115690231323  dkl : 0.005319588351994753\n",
      "Current step: 0  Loss: 0.3391053080558777  Recons: 0.2618715167045593  dkl : 0.003861689008772373\n",
      "Current step: 30  Loss: 0.5122984051704407  Recons: 0.37418532371520996  dkl : 0.006905653048306704\n",
      "Current step: 60  Loss: 0.2822780907154083  Recons: 0.19006848335266113  dkl : 0.0046104807406663895\n",
      "Saving Model155.torch\n",
      "Current step: 0  Loss: 0.33212482929229736  Recons: 0.25638335943222046  dkl : 0.0037870730739086866\n",
      "Current step: 30  Loss: 0.49759238958358765  Recons: 0.3747125267982483  dkl : 0.0061439936980605125\n",
      "Current step: 60  Loss: 0.2623720169067383  Recons: 0.1861565113067627  dkl : 0.0038107752334326506\n",
      "Current step: 0  Loss: 0.3490268588066101  Recons: 0.2673811912536621  dkl : 0.004082283936440945\n",
      "Current step: 30  Loss: 0.48749345541000366  Recons: 0.3736845850944519  dkl : 0.005690443329513073\n",
      "Current step: 60  Loss: 0.2542750835418701  Recons: 0.18420737981796265  dkl : 0.003503384767100215\n",
      "Current step: 0  Loss: 0.4204072058200836  Recons: 0.2764577865600586  dkl : 0.007197471335530281\n",
      "Current step: 30  Loss: 0.568637490272522  Recons: 0.3768860697746277  dkl : 0.009587569162249565\n",
      "Current step: 60  Loss: 0.23930573463439941  Recons: 0.18404608964920044  dkl : 0.0027629826217889786\n",
      "Current step: 0  Loss: 0.45295363664627075  Recons: 0.280179500579834  dkl : 0.008638706058263779\n",
      "Current step: 30  Loss: 0.54622483253479  Recons: 0.37432146072387695  dkl : 0.008595166727900505\n",
      "Current step: 60  Loss: 0.23251333832740784  Recons: 0.18928760290145874  dkl : 0.002161286771297455\n",
      "Current step: 0  Loss: 0.3704769015312195  Recons: 0.2690083980560303  dkl : 0.005073425360023975\n",
      "Current step: 30  Loss: 0.506453812122345  Recons: 0.3818659782409668  dkl : 0.006229391787201166\n",
      "Current step: 60  Loss: 0.2273600846529007  Recons: 0.18699461221694946  dkl : 0.0020182738080620766\n",
      "Saving Model160.torch\n",
      "Current step: 0  Loss: 0.35885143280029297  Recons: 0.2776307463645935  dkl : 0.004061033949255943\n",
      "Current step: 30  Loss: 0.5027966499328613  Recons: 0.374331533908844  dkl : 0.006423255894333124\n",
      "Current step: 60  Loss: 0.23407715559005737  Recons: 0.18468815088272095  dkl : 0.0024694500025361776\n",
      "Current step: 0  Loss: 0.3272297978401184  Recons: 0.26346540451049805  dkl : 0.0031882193870842457\n",
      "Current step: 30  Loss: 0.5381901264190674  Recons: 0.37667566537857056  dkl : 0.008075723424553871\n",
      "Current step: 60  Loss: 0.24042116105556488  Recons: 0.18705105781555176  dkl : 0.0026685050688683987\n",
      "Current step: 0  Loss: 0.3007550537586212  Recons: 0.25886744260787964  dkl : 0.0020943807903677225\n",
      "Current step: 30  Loss: 0.45953691005706787  Recons: 0.3733317255973816  dkl : 0.004310258664190769\n",
      "Current step: 60  Loss: 0.23116345703601837  Recons: 0.18528366088867188  dkl : 0.002293989760801196\n",
      "Current step: 0  Loss: 0.3096945583820343  Recons: 0.27094340324401855  dkl : 0.001937558059580624\n",
      "Current step: 30  Loss: 0.4336058795452118  Recons: 0.3722530007362366  dkl : 0.0030676433816552162\n",
      "Current step: 60  Loss: 0.23348864912986755  Recons: 0.18247824907302856  dkl : 0.0025505204685032368\n",
      "Current step: 0  Loss: 0.30816206336021423  Recons: 0.2665230631828308  dkl : 0.002081950195133686\n",
      "Current step: 30  Loss: 0.4880593419075012  Recons: 0.37191134691238403  dkl : 0.005807400681078434\n",
      "Current step: 60  Loss: 0.22423769533634186  Recons: 0.18253403902053833  dkl : 0.0020851828157901764\n",
      "Saving Model165.torch\n",
      "Current step: 0  Loss: 0.315898597240448  Recons: 0.2582242488861084  dkl : 0.002883718116208911\n",
      "Current step: 30  Loss: 0.4452635645866394  Recons: 0.3743789792060852  dkl : 0.0035442295484244823\n",
      "Current step: 60  Loss: 0.22832630574703217  Recons: 0.18090981245040894  dkl : 0.002370824571698904\n",
      "Current step: 0  Loss: 0.3222566843032837  Recons: 0.25822240114212036  dkl : 0.0032017137855291367\n",
      "Current step: 30  Loss: 0.4473026394844055  Recons: 0.3721432685852051  dkl : 0.0037579676136374474\n",
      "Current step: 60  Loss: 0.2333376407623291  Recons: 0.18538516759872437  dkl : 0.0023976233787834644\n",
      "Current step: 0  Loss: 0.337959349155426  Recons: 0.2850787043571472  dkl : 0.0026440322399139404\n",
      "Current step: 30  Loss: 0.4783305525779724  Recons: 0.37516409158706665  dkl : 0.005158322863280773\n",
      "Current step: 60  Loss: 0.23874515295028687  Recons: 0.19059449434280396  dkl : 0.002407533349469304\n",
      "Current step: 0  Loss: 0.3186362385749817  Recons: 0.2644878029823303  dkl : 0.0027074222452938557\n",
      "Current step: 30  Loss: 0.4508787989616394  Recons: 0.37385112047195435  dkl : 0.0038513843901455402\n",
      "Current step: 60  Loss: 0.2301459014415741  Recons: 0.18362551927566528  dkl : 0.0023260186426341534\n",
      "Current step: 0  Loss: 0.41028496623039246  Recons: 0.2689034342765808  dkl : 0.007069076411426067\n",
      "Current step: 30  Loss: 0.4611453115940094  Recons: 0.37462908029556274  dkl : 0.004325811751186848\n",
      "Current step: 60  Loss: 0.23765408992767334  Recons: 0.18002325296401978  dkl : 0.0028815416153520346\n",
      "Saving Model170.torch\n",
      "Current step: 0  Loss: 0.32347649335861206  Recons: 0.261549174785614  dkl : 0.0030963667668402195\n",
      "Current step: 30  Loss: 0.4751355051994324  Recons: 0.37195461988449097  dkl : 0.0051590446382761\n",
      "Current step: 60  Loss: 0.255575567483902  Recons: 0.18235301971435547  dkl : 0.0036611275281757116\n",
      "Current step: 0  Loss: 0.36080050468444824  Recons: 0.26817840337753296  dkl : 0.004631104879081249\n",
      "Current step: 30  Loss: 0.5217598676681519  Recons: 0.3726641535758972  dkl : 0.007454787380993366\n",
      "Current step: 60  Loss: 0.24387246370315552  Recons: 0.19072437286376953  dkl : 0.0026574041694402695\n",
      "Current step: 0  Loss: 0.37387534976005554  Recons: 0.2797520160675049  dkl : 0.004706166684627533\n",
      "Current step: 30  Loss: 0.5221338272094727  Recons: 0.3732529282569885  dkl : 0.007444044575095177\n",
      "Current step: 60  Loss: 0.2648107707500458  Recons: 0.19274991750717163  dkl : 0.00360304256901145\n",
      "Current step: 0  Loss: 0.3448413014411926  Recons: 0.2696266174316406  dkl : 0.0037607343401759863\n",
      "Current step: 30  Loss: 0.5573605298995972  Recons: 0.3843058943748474  dkl : 0.008652729913592339\n",
      "Current step: 60  Loss: 0.29859012365341187  Recons: 0.2358573079109192  dkl : 0.0031366401817649603\n",
      "Current step: 0  Loss: 0.38450515270233154  Recons: 0.29616332054138184  dkl : 0.00441709253937006\n",
      "Current step: 30  Loss: 0.6104978919029236  Recons: 0.378074586391449  dkl : 0.011621165089309216\n",
      "Current step: 60  Loss: 0.2687224745750427  Recons: 0.1900297999382019  dkl : 0.003934633918106556\n",
      "Saving Model175.torch\n",
      "Current step: 0  Loss: 0.45007601380348206  Recons: 0.2750132083892822  dkl : 0.008753140456974506\n",
      "Current step: 30  Loss: 0.6829444169998169  Recons: 0.37372249364852905  dkl : 0.015461097471415997\n",
      "Current step: 60  Loss: 0.3203147053718567  Recons: 0.18496304750442505  dkl : 0.00676758261397481\n",
      "Current step: 0  Loss: 0.45141586661338806  Recons: 0.26125651597976685  dkl : 0.009507967159152031\n",
      "Current step: 30  Loss: 0.6785717010498047  Recons: 0.374178946018219  dkl : 0.015219636261463165\n",
      "Current step: 60  Loss: 0.25051140785217285  Recons: 0.18304824829101562  dkl : 0.00337315839715302\n",
      "Current step: 0  Loss: 0.4367161989212036  Recons: 0.25823044776916504  dkl : 0.008924286812543869\n",
      "Current step: 30  Loss: 0.5555031299591064  Recons: 0.37307506799697876  dkl : 0.00912140216678381\n",
      "Current step: 60  Loss: 0.2640012502670288  Recons: 0.1831679344177246  dkl : 0.004041665233671665\n",
      "Current step: 0  Loss: 0.4796833395957947  Recons: 0.2654358744621277  dkl : 0.010712373070418835\n",
      "Current step: 30  Loss: 0.6619241237640381  Recons: 0.3727656602859497  dkl : 0.01445792242884636\n",
      "Current step: 60  Loss: 0.2535853683948517  Recons: 0.18058204650878906  dkl : 0.003650166094303131\n",
      "Current step: 0  Loss: 0.4260087013244629  Recons: 0.2671878933906555  dkl : 0.007941040210425854\n",
      "Current step: 30  Loss: 0.7333467602729797  Recons: 0.3727981448173523  dkl : 0.018027430400252342\n",
      "Current step: 60  Loss: 0.2541162073612213  Recons: 0.1834101676940918  dkl : 0.0035353023558855057\n",
      "Saving Model180.torch\n",
      "Current step: 0  Loss: 0.35558003187179565  Recons: 0.25582724809646606  dkl : 0.00498763844370842\n",
      "Current step: 30  Loss: 0.543379008769989  Recons: 0.37308210134506226  dkl : 0.008514845743775368\n",
      "Current step: 60  Loss: 0.2661692500114441  Recons: 0.18043631315231323  dkl : 0.004286646377295256\n",
      "Current step: 0  Loss: 0.34046509861946106  Recons: 0.25790804624557495  dkl : 0.004127853084355593\n",
      "Current step: 30  Loss: 0.5637425780296326  Recons: 0.376542866230011  dkl : 0.009359984658658504\n",
      "Current step: 60  Loss: 0.23413456976413727  Recons: 0.18593114614486694  dkl : 0.0024101713206619024\n",
      "Current step: 0  Loss: 0.3144393265247345  Recons: 0.25857609510421753  dkl : 0.002793160965666175\n",
      "Current step: 30  Loss: 0.6131635904312134  Recons: 0.37495601177215576  dkl : 0.01191037893295288\n",
      "Current step: 60  Loss: 0.24367710947990417  Recons: 0.18194526433944702  dkl : 0.0030865923035889864\n",
      "Current step: 0  Loss: 0.3191690742969513  Recons: 0.26179295778274536  dkl : 0.0028688053134828806\n",
      "Current step: 30  Loss: 0.5834665894508362  Recons: 0.3705061078071594  dkl : 0.010648023337125778\n",
      "Current step: 60  Loss: 0.24493248760700226  Recons: 0.1859397292137146  dkl : 0.0029496378265321255\n",
      "Current step: 0  Loss: 0.3280109167098999  Recons: 0.27001291513442993  dkl : 0.002899900544434786\n",
      "Current step: 30  Loss: 0.5388200283050537  Recons: 0.372142493724823  dkl : 0.00833387766033411\n",
      "Current step: 60  Loss: 0.24767544865608215  Recons: 0.19097846746444702  dkl : 0.002834849525243044\n",
      "Saving Model185.torch\n",
      "Current step: 0  Loss: 0.3365488648414612  Recons: 0.26410579681396484  dkl : 0.0036221533082425594\n",
      "Current step: 30  Loss: 0.5124927163124084  Recons: 0.3734666705131531  dkl : 0.0069513022899627686\n",
      "Current step: 60  Loss: 0.2546924650669098  Recons: 0.1885935664176941  dkl : 0.003304944606497884\n",
      "Current step: 0  Loss: 0.3459775447845459  Recons: 0.2614450454711914  dkl : 0.004226625431329012\n",
      "Current step: 30  Loss: 0.4866897463798523  Recons: 0.3712713122367859  dkl : 0.005770922638475895\n",
      "Current step: 60  Loss: 0.26342272758483887  Recons: 0.1819143295288086  dkl : 0.004075419157743454\n",
      "Current step: 0  Loss: 0.3388127088546753  Recons: 0.25485295057296753  dkl : 0.004197987727820873\n",
      "Current step: 30  Loss: 0.47462183237075806  Recons: 0.3723403215408325  dkl : 0.0051140752620995045\n",
      "Current step: 60  Loss: 0.27307212352752686  Recons: 0.1811574101448059  dkl : 0.004595735110342503\n",
      "Current step: 0  Loss: 0.33418142795562744  Recons: 0.2544136643409729  dkl : 0.003988388925790787\n",
      "Current step: 30  Loss: 0.4814814329147339  Recons: 0.3717188239097595  dkl : 0.005488130263984203\n",
      "Current step: 60  Loss: 0.25992920994758606  Recons: 0.18006008863449097  dkl : 0.0039934562519192696\n",
      "Current step: 0  Loss: 0.31750205159187317  Recons: 0.2593894600868225  dkl : 0.002905629575252533\n",
      "Current step: 30  Loss: 0.5009516477584839  Recons: 0.371521532535553  dkl : 0.006471504457294941\n",
      "Current step: 60  Loss: 0.25955381989479065  Recons: 0.18129795789718628  dkl : 0.003912793006747961\n",
      "Saving Model190.torch\n",
      "Current step: 0  Loss: 0.30793651938438416  Recons: 0.257610559463501  dkl : 0.0025162980891764164\n",
      "Current step: 30  Loss: 0.5458208918571472  Recons: 0.3716903328895569  dkl : 0.008706528693437576\n",
      "Current step: 60  Loss: 0.2683645784854889  Recons: 0.18050450086593628  dkl : 0.004393003415316343\n",
      "Current step: 0  Loss: 0.31500330567359924  Recons: 0.255465030670166  dkl : 0.0029769139364361763\n",
      "Current step: 30  Loss: 0.5695710182189941  Recons: 0.37479090690612793  dkl : 0.00973900593817234\n",
      "Current step: 60  Loss: 0.26478737592697144  Recons: 0.18185287714004517  dkl : 0.004146725870668888\n",
      "Current step: 0  Loss: 0.32213494181632996  Recons: 0.2545784115791321  dkl : 0.00337782665155828\n",
      "Current step: 30  Loss: 0.57093745470047  Recons: 0.370891273021698  dkl : 0.010002308525145054\n",
      "Current step: 60  Loss: 0.2584001123905182  Recons: 0.1849374771118164  dkl : 0.0036731320433318615\n",
      "Current step: 0  Loss: 0.3239514231681824  Recons: 0.25469493865966797  dkl : 0.003462824271991849\n",
      "Current step: 30  Loss: 0.5467933416366577  Recons: 0.37211865186691284  dkl : 0.008733736351132393\n",
      "Current step: 60  Loss: 0.2737078070640564  Recons: 0.1828588843345642  dkl : 0.0045424457639455795\n",
      "Current step: 0  Loss: 0.35267314314842224  Recons: 0.2531735301017761  dkl : 0.004974980838596821\n",
      "Current step: 30  Loss: 0.5832567811012268  Recons: 0.3715091943740845  dkl : 0.010587379336357117\n",
      "Current step: 60  Loss: 0.2911328077316284  Recons: 0.18096524477005005  dkl : 0.005508378613740206\n",
      "Saving Model195.torch\n",
      "Current step: 0  Loss: 0.3766053318977356  Recons: 0.25653475522994995  dkl : 0.006003528367727995\n",
      "Current step: 30  Loss: 0.5326695442199707  Recons: 0.3728504776954651  dkl : 0.007990953512489796\n",
      "Current step: 60  Loss: 0.31873297691345215  Recons: 0.17865276336669922  dkl : 0.0070040104910731316\n",
      "Current step: 0  Loss: 0.38121476769447327  Recons: 0.25902724266052246  dkl : 0.006109375972300768\n",
      "Current step: 30  Loss: 0.5332918763160706  Recons: 0.37303656339645386  dkl : 0.008012766018509865\n",
      "Current step: 60  Loss: 0.3090500831604004  Recons: 0.1840704083442688  dkl : 0.006248983088880777\n",
      "Current step: 0  Loss: 0.36453977227211  Recons: 0.2571203112602234  dkl : 0.0053709726780653\n",
      "Current step: 30  Loss: 0.5901577472686768  Recons: 0.37143760919570923  dkl : 0.010936005972325802\n",
      "Current step: 60  Loss: 0.3310704231262207  Recons: 0.18384981155395508  dkl : 0.007361029740422964\n",
      "Current step: 0  Loss: 0.4429267644882202  Recons: 0.25540465116500854  dkl : 0.009376104921102524\n",
      "Current step: 30  Loss: 0.6419825553894043  Recons: 0.3731709122657776  dkl : 0.013440581038594246\n",
      "Current step: 60  Loss: 0.31332167983055115  Recons: 0.1894703507423401  dkl : 0.006192566826939583\n",
      "Current step: 0  Loss: 0.4861501455307007  Recons: 0.25667834281921387  dkl : 0.01147359050810337\n",
      "Current step: 30  Loss: 0.5854740142822266  Recons: 0.3722991347312927  dkl : 0.010658744722604752\n",
      "Current step: 60  Loss: 0.28889912366867065  Recons: 0.18177366256713867  dkl : 0.005356273613870144\n",
      "Saving Model200.torch\n",
      "Current step: 0  Loss: 0.519008994102478  Recons: 0.2761581540107727  dkl : 0.012142542749643326\n",
      "Current step: 30  Loss: 0.7646329998970032  Recons: 0.38383716344833374  dkl : 0.019039791077375412\n",
      "Current step: 60  Loss: 0.31085747480392456  Recons: 0.211575448513031  dkl : 0.00496410159394145\n",
      "Current step: 0  Loss: 0.391693115234375  Recons: 0.2805289626121521  dkl : 0.005558207631111145\n",
      "Current step: 30  Loss: 0.7131896018981934  Recons: 0.3834734559059143  dkl : 0.016485806554555893\n",
      "Current step: 60  Loss: 0.27887392044067383  Recons: 0.1887703537940979  dkl : 0.004505178891122341\n",
      "Current step: 0  Loss: 0.4127635359764099  Recons: 0.2683921456336975  dkl : 0.00721856951713562\n",
      "Current step: 30  Loss: 0.6547342538833618  Recons: 0.37353843450546265  dkl : 0.014059789478778839\n",
      "Current step: 60  Loss: 0.3282754421234131  Recons: 0.1861298680305481  dkl : 0.00710727833211422\n",
      "Current step: 0  Loss: 0.47982627153396606  Recons: 0.26790672540664673  dkl : 0.010595977306365967\n",
      "Current step: 30  Loss: 0.6143018007278442  Recons: 0.3713341951370239  dkl : 0.012148380279541016\n",
      "Current step: 60  Loss: 0.27603840827941895  Recons: 0.18107908964157104  dkl : 0.00474796537309885\n",
      "Current step: 0  Loss: 0.46932902932167053  Recons: 0.25728386640548706  dkl : 0.010602258145809174\n",
      "Current step: 30  Loss: 0.6071114540100098  Recons: 0.37283772230148315  dkl : 0.011713686399161816\n",
      "Current step: 60  Loss: 0.32002824544906616  Recons: 0.18065816164016724  dkl : 0.006968504749238491\n",
      "Saving Model205.torch\n",
      "Current step: 0  Loss: 0.41861969232559204  Recons: 0.2540593147277832  dkl : 0.008228018879890442\n",
      "Current step: 30  Loss: 0.7512221336364746  Recons: 0.37169867753982544  dkl : 0.01897617243230343\n",
      "Current step: 60  Loss: 0.33013761043548584  Recons: 0.17995041608810425  dkl : 0.007509359624236822\n",
      "Current step: 0  Loss: 0.47892946004867554  Recons: 0.2643546462059021  dkl : 0.010728741064667702\n",
      "Current step: 30  Loss: 0.6736419200897217  Recons: 0.3718889355659485  dkl : 0.01508764922618866\n",
      "Current step: 60  Loss: 0.338512122631073  Recons: 0.2065901756286621  dkl : 0.006596097722649574\n",
      "Current step: 0  Loss: 0.4185810685157776  Recons: 0.26423150300979614  dkl : 0.007717478554695845\n",
      "Current step: 30  Loss: 0.7248752117156982  Recons: 0.3732230067253113  dkl : 0.017582610249519348\n",
      "Current step: 60  Loss: 0.29927241802215576  Recons: 0.17943793535232544  dkl : 0.005991724785417318\n",
      "Current step: 0  Loss: 0.399284303188324  Recons: 0.25972747802734375  dkl : 0.006977842189371586\n",
      "Current step: 30  Loss: 0.6210317015647888  Recons: 0.37356168031692505  dkl : 0.012373500503599644\n",
      "Current step: 60  Loss: 0.2956685721874237  Recons: 0.18057793378829956  dkl : 0.005754532292485237\n",
      "Current step: 0  Loss: 0.4448721408843994  Recons: 0.26197171211242676  dkl : 0.009145020507276058\n",
      "Current step: 30  Loss: 0.5601550936698914  Recons: 0.37331509590148926  dkl : 0.009341999888420105\n",
      "Current step: 60  Loss: 0.32156985998153687  Recons: 0.18659251928329468  dkl : 0.006748867221176624\n",
      "Saving Model210.torch\n",
      "Current step: 0  Loss: 0.4698535203933716  Recons: 0.26567918062210083  dkl : 0.010208716616034508\n",
      "Current step: 30  Loss: 0.5428861975669861  Recons: 0.37312012910842896  dkl : 0.008488303050398827\n",
      "Current step: 60  Loss: 0.310386061668396  Recons: 0.17812520265579224  dkl : 0.006613042205572128\n",
      "Current step: 0  Loss: 0.43015021085739136  Recons: 0.2687597870826721  dkl : 0.008069521747529507\n",
      "Current step: 30  Loss: 0.5054060220718384  Recons: 0.37069594860076904  dkl : 0.006735504604876041\n",
      "Current step: 60  Loss: 0.29723989963531494  Recons: 0.17852520942687988  dkl : 0.005935734137892723\n",
      "Current step: 0  Loss: 0.3643520772457123  Recons: 0.2573277950286865  dkl : 0.005351213738322258\n",
      "Current step: 30  Loss: 0.5216489434242249  Recons: 0.37278681993484497  dkl : 0.007443105801939964\n",
      "Current step: 60  Loss: 0.2717544734477997  Recons: 0.17753171920776367  dkl : 0.00471113808453083\n",
      "Current step: 0  Loss: 0.33115050196647644  Recons: 0.2566344141960144  dkl : 0.0037258046213537455\n",
      "Current step: 30  Loss: 0.5769132971763611  Recons: 0.3716351389884949  dkl : 0.010263907723128796\n",
      "Current step: 60  Loss: 0.23374921083450317  Recons: 0.1765652298927307  dkl : 0.002859199419617653\n",
      "Current step: 0  Loss: 0.30257371068000793  Recons: 0.25145387649536133  dkl : 0.0025559912901371717\n",
      "Current step: 30  Loss: 0.5293979644775391  Recons: 0.3720160722732544  dkl : 0.007869092747569084\n",
      "Current step: 60  Loss: 0.22876405715942383  Recons: 0.18215280771255493  dkl : 0.0023305625654757023\n",
      "Saving Model215.torch\n",
      "Current step: 0  Loss: 0.31840550899505615  Recons: 0.2588488459587097  dkl : 0.0029778331518173218\n",
      "Current step: 30  Loss: 0.5225685834884644  Recons: 0.37488776445388794  dkl : 0.007384040858596563\n",
      "Current step: 60  Loss: 0.27456778287887573  Recons: 0.22439956665039062  dkl : 0.002508410718291998\n",
      "Current step: 0  Loss: 0.37580692768096924  Recons: 0.29931992292404175  dkl : 0.0038243511226028204\n",
      "Current step: 30  Loss: 0.5679031610488892  Recons: 0.3761792778968811  dkl : 0.009586194530129433\n",
      "Current step: 60  Loss: 0.24899044632911682  Recons: 0.18471360206604004  dkl : 0.003213842399418354\n",
      "Current step: 0  Loss: 0.3440895974636078  Recons: 0.2655251622200012  dkl : 0.003928221762180328\n",
      "Current step: 30  Loss: 0.5683548450469971  Recons: 0.37045878171920776  dkl : 0.009894801303744316\n",
      "Current step: 60  Loss: 0.2400347739458084  Recons: 0.18032598495483398  dkl : 0.00298543949611485\n",
      "Current step: 0  Loss: 0.3259289860725403  Recons: 0.26450294256210327  dkl : 0.0030713018495589495\n",
      "Current step: 30  Loss: 0.5614462494850159  Recons: 0.3721579909324646  dkl : 0.009464412927627563\n",
      "Current step: 60  Loss: 0.237878680229187  Recons: 0.179243803024292  dkl : 0.0029317433945834637\n",
      "Current step: 0  Loss: 0.3230222165584564  Recons: 0.2714979648590088  dkl : 0.002576212165877223\n",
      "Current step: 30  Loss: 0.4976527690887451  Recons: 0.3723570704460144  dkl : 0.006264785770326853\n",
      "Current step: 60  Loss: 0.2613546848297119  Recons: 0.18439382314682007  dkl : 0.003848043270409107\n",
      "Saving Model220.torch\n",
      "Current step: 0  Loss: 0.33052533864974976  Recons: 0.2654404640197754  dkl : 0.003254243638366461\n",
      "Current step: 30  Loss: 0.5057950019836426  Recons: 0.37045127153396606  dkl : 0.006767185404896736\n",
      "Current step: 60  Loss: 0.27009978890419006  Recons: 0.17593592405319214  dkl : 0.004708193242549896\n",
      "Current step: 0  Loss: 0.3429975211620331  Recons: 0.2679840922355652  dkl : 0.0037506716325879097\n",
      "Current step: 30  Loss: 0.463652104139328  Recons: 0.3712298274040222  dkl : 0.004621114116162062\n",
      "Current step: 60  Loss: 0.2778627574443817  Recons: 0.1800439953804016  dkl : 0.00489093828946352\n",
      "Current step: 0  Loss: 0.34655189514160156  Recons: 0.25902003049850464  dkl : 0.004376593511551619\n",
      "Current step: 30  Loss: 0.4629228711128235  Recons: 0.3712645173072815  dkl : 0.004582918249070644\n",
      "Current step: 60  Loss: 0.26909881830215454  Recons: 0.17700129747390747  dkl : 0.004604875575751066\n",
      "Current step: 0  Loss: 0.31849297881126404  Recons: 0.25580739974975586  dkl : 0.0031342790462076664\n",
      "Current step: 30  Loss: 0.4929516911506653  Recons: 0.3710506558418274  dkl : 0.006095051299780607\n",
      "Current step: 60  Loss: 0.25962281227111816  Recons: 0.17754888534545898  dkl : 0.004103696905076504\n",
      "Current step: 0  Loss: 0.3179345726966858  Recons: 0.2542213797569275  dkl : 0.0031856601126492023\n",
      "Current step: 30  Loss: 0.5367320775985718  Recons: 0.3701067566871643  dkl : 0.008331266231834888\n",
      "Current step: 60  Loss: 0.25944942235946655  Recons: 0.18226861953735352  dkl : 0.0038590398617088795\n",
      "Saving Model225.torch\n",
      "Current step: 0  Loss: 0.3481818437576294  Recons: 0.25322723388671875  dkl : 0.0047477297484874725\n",
      "Current step: 30  Loss: 0.5647015571594238  Recons: 0.3710029721260071  dkl : 0.009684927761554718\n",
      "Current step: 60  Loss: 0.2446455955505371  Recons: 0.17733031511306763  dkl : 0.0033657639287412167\n",
      "Current step: 0  Loss: 0.35605838894844055  Recons: 0.2576562762260437  dkl : 0.004920105449855328\n",
      "Current step: 30  Loss: 0.5736123323440552  Recons: 0.37110739946365356  dkl : 0.010125245898962021\n",
      "Current step: 60  Loss: 0.22313851118087769  Recons: 0.17483925819396973  dkl : 0.002414962276816368\n",
      "Current step: 0  Loss: 0.427837073802948  Recons: 0.2641252875328064  dkl : 0.008185588754713535\n",
      "Current step: 30  Loss: 0.656489372253418  Recons: 0.3698691725730896  dkl : 0.014331008307635784\n",
      "Current step: 60  Loss: 0.24790632724761963  Recons: 0.17679190635681152  dkl : 0.0035557206720113754\n",
      "Current step: 0  Loss: 0.4227907061576843  Recons: 0.270333468914032  dkl : 0.00762286176905036\n",
      "Current step: 30  Loss: 0.636427640914917  Recons: 0.3694567084312439  dkl : 0.01334854494780302\n",
      "Current step: 60  Loss: 0.2769376337528229  Recons: 0.17571449279785156  dkl : 0.005061157047748566\n",
      "Current step: 0  Loss: 0.40726763010025024  Recons: 0.2631966471672058  dkl : 0.007203550077974796\n",
      "Current step: 30  Loss: 0.5953075289726257  Recons: 0.37097257375717163  dkl : 0.01121674757450819\n",
      "Current step: 60  Loss: 0.274316668510437  Recons: 0.1770372986793518  dkl : 0.004863967653363943\n",
      "Saving Model230.torch\n",
      "Current step: 0  Loss: 0.3508884906768799  Recons: 0.25448018312454224  dkl : 0.0048204162158071995\n",
      "Current step: 30  Loss: 0.6243126392364502  Recons: 0.3695608973503113  dkl : 0.012737587094306946\n",
      "Current step: 60  Loss: 0.364653080701828  Recons: 0.17974454164505005  dkl : 0.009245427325367928\n",
      "Current step: 0  Loss: 0.3498198688030243  Recons: 0.2535693645477295  dkl : 0.00481252558529377\n",
      "Current step: 30  Loss: 0.6039755940437317  Recons: 0.36956650018692017  dkl : 0.011720454320311546\n",
      "Current step: 60  Loss: 0.2939882278442383  Recons: 0.18038606643676758  dkl : 0.005680108442902565\n",
      "Current step: 0  Loss: 0.30372750759124756  Recons: 0.2524232864379883  dkl : 0.002565210685133934\n",
      "Current step: 30  Loss: 0.6078407764434814  Recons: 0.3698995113372803  dkl : 0.011897061951458454\n",
      "Current step: 60  Loss: 0.23951663076877594  Recons: 0.17715173959732056  dkl : 0.0031182444654405117\n",
      "Current step: 0  Loss: 0.37452393770217896  Recons: 0.2523302435874939  dkl : 0.006109684705734253\n",
      "Current step: 30  Loss: 0.49931639432907104  Recons: 0.37068015336990356  dkl : 0.0064318119548261166\n",
      "Current step: 60  Loss: 0.2765287160873413  Recons: 0.1782362461090088  dkl : 0.004914623685181141\n",
      "Current step: 0  Loss: 0.3694969713687897  Recons: 0.2731723189353943  dkl : 0.0048162322491407394\n",
      "Current step: 30  Loss: 0.7871966361999512  Recons: 0.37208741903305054  dkl : 0.02075546234846115\n",
      "Current step: 60  Loss: 0.2456856071949005  Recons: 0.1791892647743225  dkl : 0.0033248174004256725\n",
      "Saving Model235.torch\n",
      "Current step: 0  Loss: 0.3915554881095886  Recons: 0.26292991638183594  dkl : 0.006431277841329575\n",
      "Current step: 30  Loss: 0.5968736410140991  Recons: 0.3696078658103943  dkl : 0.011363287456333637\n",
      "Current step: 60  Loss: 0.2609419524669647  Recons: 0.17854267358779907  dkl : 0.004119964316487312\n",
      "Current step: 0  Loss: 0.35412752628326416  Recons: 0.2584384083747864  dkl : 0.004784455522894859\n",
      "Current step: 30  Loss: 0.5220104455947876  Recons: 0.3687672019004822  dkl : 0.007662160787731409\n",
      "Current step: 60  Loss: 0.2854854464530945  Recons: 0.17769503593444824  dkl : 0.005389519967138767\n",
      "Current step: 0  Loss: 0.3691183924674988  Recons: 0.25586897134780884  dkl : 0.005662471055984497\n",
      "Current step: 30  Loss: 0.5157256126403809  Recons: 0.369545042514801  dkl : 0.0073090288788080215\n",
      "Current step: 60  Loss: 0.3033198416233063  Recons: 0.19185584783554077  dkl : 0.005573200061917305\n",
      "Current step: 0  Loss: 0.40411874651908875  Recons: 0.29958218336105347  dkl : 0.005226828157901764\n",
      "Current step: 30  Loss: 0.5361873507499695  Recons: 0.3723919987678528  dkl : 0.00818976853042841\n",
      "Current step: 60  Loss: 0.28107374906539917  Recons: 0.18292349576950073  dkl : 0.004907512106001377\n",
      "Current step: 0  Loss: 0.3777182400226593  Recons: 0.27870863676071167  dkl : 0.0049504805356264114\n",
      "Current step: 30  Loss: 0.5649553537368774  Recons: 0.3732909560203552  dkl : 0.009583218023180962\n",
      "Current step: 60  Loss: 0.24044586718082428  Recons: 0.1804688572883606  dkl : 0.0029988503083586693\n",
      "Saving Model240.torch\n",
      "Current step: 0  Loss: 0.3469797670841217  Recons: 0.2606009840965271  dkl : 0.00431893952190876\n",
      "Current step: 30  Loss: 0.573996901512146  Recons: 0.36926132440567017  dkl : 0.010236780159175396\n",
      "Current step: 60  Loss: 0.22388064861297607  Recons: 0.18234843015670776  dkl : 0.002076611388474703\n",
      "Current step: 0  Loss: 0.3471229672431946  Recons: 0.2531663775444031  dkl : 0.004697829484939575\n",
      "Current step: 30  Loss: 0.6149280071258545  Recons: 0.3731148838996887  dkl : 0.012090657837688923\n",
      "Current step: 60  Loss: 0.2258966863155365  Recons: 0.17853373289108276  dkl : 0.002368147950619459\n",
      "Current step: 0  Loss: 0.4073294401168823  Recons: 0.25292593240737915  dkl : 0.007720175199210644\n",
      "Current step: 30  Loss: 0.5289076566696167  Recons: 0.3708772659301758  dkl : 0.00790152046829462\n",
      "Current step: 60  Loss: 0.23389150202274323  Recons: 0.17650079727172852  dkl : 0.002869535004720092\n",
      "Current step: 0  Loss: 0.3974968194961548  Recons: 0.2587965130805969  dkl : 0.00693501578643918\n",
      "Current step: 30  Loss: 0.5314692854881287  Recons: 0.3689683675765991  dkl : 0.008125045336782932\n",
      "Current step: 60  Loss: 0.25145119428634644  Recons: 0.18163031339645386  dkl : 0.0034910442773252726\n",
      "Current step: 0  Loss: 0.5093369483947754  Recons: 0.2687951922416687  dkl : 0.012027088552713394\n",
      "Current step: 30  Loss: 0.6380458474159241  Recons: 0.3681332468986511  dkl : 0.013495630584657192\n",
      "Current step: 60  Loss: 0.27483776211738586  Recons: 0.1854836344718933  dkl : 0.004467706196010113\n",
      "Saving Model245.torch\n",
      "Current step: 0  Loss: 0.40070945024490356  Recons: 0.2656562328338623  dkl : 0.006752660498023033\n",
      "Current step: 30  Loss: 0.5521769523620605  Recons: 0.3696959614753723  dkl : 0.009124048054218292\n",
      "Current step: 60  Loss: 0.22220279276371002  Recons: 0.1804402470588684  dkl : 0.0020881271921098232\n",
      "Current step: 0  Loss: 0.3776898980140686  Recons: 0.2606813907623291  dkl : 0.005850424524396658\n",
      "Current step: 30  Loss: 0.48860758543014526  Recons: 0.37011510133743286  dkl : 0.005924623925238848\n",
      "Current step: 60  Loss: 0.25379979610443115  Recons: 0.17808938026428223  dkl : 0.0037855214904993773\n",
      "Current step: 0  Loss: 0.3488197326660156  Recons: 0.26405614614486694  dkl : 0.004238178487867117\n",
      "Current step: 30  Loss: 0.5649871826171875  Recons: 0.36883097887039185  dkl : 0.009807808324694633\n",
      "Current step: 60  Loss: 0.2913179099559784  Recons: 0.17869669198989868  dkl : 0.005631060805171728\n",
      "Current step: 0  Loss: 0.36364883184432983  Recons: 0.2602255344390869  dkl : 0.005171165801584721\n",
      "Current step: 30  Loss: 0.5948984026908875  Recons: 0.3695563077926636  dkl : 0.011267105117440224\n",
      "Current step: 60  Loss: 0.280094712972641  Recons: 0.1813299059867859  dkl : 0.00493824016302824\n",
      "Current step: 0  Loss: 0.3864462077617645  Recons: 0.27108150720596313  dkl : 0.005768234841525555\n",
      "Current step: 30  Loss: 0.5320776700973511  Recons: 0.36933475732803345  dkl : 0.008137144148349762\n",
      "Current step: 60  Loss: 0.24793317914009094  Recons: 0.17975395917892456  dkl : 0.003408960998058319\n",
      "Saving Model250.torch\n",
      "Current step: 0  Loss: 0.36626505851745605  Recons: 0.2612323760986328  dkl : 0.0052516344003379345\n",
      "Current step: 30  Loss: 0.515947163105011  Recons: 0.37019866704940796  dkl : 0.007287424989044666\n",
      "Current step: 60  Loss: 0.25581905245780945  Recons: 0.1784515380859375  dkl : 0.003868376137688756\n",
      "Current step: 0  Loss: 0.3705902099609375  Recons: 0.2673315405845642  dkl : 0.005162934307008982\n",
      "Current step: 30  Loss: 0.4954372048377991  Recons: 0.36712056398391724  dkl : 0.006415831856429577\n",
      "Current step: 60  Loss: 0.23660767078399658  Recons: 0.17420655488967896  dkl : 0.0031200561206787825\n",
      "Current step: 0  Loss: 0.3947654962539673  Recons: 0.2649100422859192  dkl : 0.00649277213960886\n",
      "Current step: 30  Loss: 0.527991771697998  Recons: 0.3689776062965393  dkl : 0.007950709201395512\n",
      "Current step: 60  Loss: 0.25155532360076904  Recons: 0.17480522394180298  dkl : 0.0038375058211386204\n",
      "Current step: 0  Loss: 0.39993584156036377  Recons: 0.2570280432701111  dkl : 0.007145389448851347\n",
      "Current step: 30  Loss: 0.4693934917449951  Recons: 0.368765652179718  dkl : 0.00503139290958643\n",
      "Current step: 60  Loss: 0.24972492456436157  Recons: 0.17408400774002075  dkl : 0.003782046027481556\n",
      "Current step: 0  Loss: 0.41450172662734985  Recons: 0.257018506526947  dkl : 0.007874160073697567\n",
      "Current step: 30  Loss: 0.5038910508155823  Recons: 0.36741572618484497  dkl : 0.006823766510933638\n",
      "Current step: 60  Loss: 0.27425071597099304  Recons: 0.17610591650009155  dkl : 0.004907239694148302\n",
      "Saving Model255.torch\n",
      "Current step: 0  Loss: 0.4307277798652649  Recons: 0.2541906237602234  dkl : 0.008826857432723045\n",
      "Current step: 30  Loss: 0.5723435878753662  Recons: 0.3686700463294983  dkl : 0.010183678939938545\n",
      "Current step: 60  Loss: 0.2878524363040924  Recons: 0.17411285638809204  dkl : 0.005686978809535503\n",
      "Current step: 0  Loss: 0.4588223397731781  Recons: 0.2551608681678772  dkl : 0.010183073580265045\n",
      "Current step: 30  Loss: 0.5621757507324219  Recons: 0.368773877620697  dkl : 0.009670092724263668\n",
      "Current step: 60  Loss: 0.30372464656829834  Recons: 0.17237251996994019  dkl : 0.006567605771124363\n",
      "Current step: 0  Loss: 0.5143865942955017  Recons: 0.2503102421760559  dkl : 0.01320381835103035\n",
      "Current step: 30  Loss: 0.5425797700881958  Recons: 0.3722166419029236  dkl : 0.0085181575268507\n",
      "Current step: 60  Loss: 0.34109368920326233  Recons: 0.17376333475112915  dkl : 0.008366517722606659\n",
      "Current step: 0  Loss: 0.44677141308784485  Recons: 0.25387221574783325  dkl : 0.009644960053265095\n",
      "Current step: 30  Loss: 0.8488459587097168  Recons: 0.37109535932540894  dkl : 0.023887529969215393\n",
      "Current step: 60  Loss: 0.4028530716896057  Recons: 0.1747557520866394  dkl : 0.01140486542135477\n",
      "Current step: 0  Loss: 0.3948836922645569  Recons: 0.2528095245361328  dkl : 0.007103709038347006\n",
      "Current step: 30  Loss: 0.704627513885498  Recons: 0.37582117319107056  dkl : 0.016440318897366524\n",
      "Current step: 60  Loss: 0.45588910579681396  Recons: 0.20807337760925293  dkl : 0.012390786781907082\n",
      "Saving Model260.torch\n",
      "Current step: 0  Loss: 0.4727458655834198  Recons: 0.2790674567222595  dkl : 0.009683920070528984\n",
      "Current step: 30  Loss: 0.7083672881126404  Recons: 0.3747912645339966  dkl : 0.01667880080640316\n",
      "Current step: 60  Loss: 0.4511881172657013  Recons: 0.18553942441940308  dkl : 0.013282434083521366\n",
      "Current step: 0  Loss: 0.4265483617782593  Recons: 0.26811546087265015  dkl : 0.007921645417809486\n",
      "Current step: 30  Loss: 0.6704763174057007  Recons: 0.38887596130371094  dkl : 0.014080015942454338\n",
      "Current step: 60  Loss: 0.3628576993942261  Recons: 0.21466857194900513  dkl : 0.007409457117319107\n",
      "Current step: 0  Loss: 0.5380150079727173  Recons: 0.2778148055076599  dkl : 0.013010010123252869\n",
      "Current step: 30  Loss: 0.7562502026557922  Recons: 0.3824402093887329  dkl : 0.018690500408411026\n",
      "Current step: 60  Loss: 0.37003976106643677  Recons: 0.18467837572097778  dkl : 0.009268069639801979\n",
      "Current step: 0  Loss: 0.48028939962387085  Recons: 0.26902133226394653  dkl : 0.010563402436673641\n",
      "Current step: 30  Loss: 0.7461535930633545  Recons: 0.3678298592567444  dkl : 0.018916185945272446\n",
      "Current step: 60  Loss: 0.35826727747917175  Recons: 0.1825270652770996  dkl : 0.008787010796368122\n",
      "Current step: 0  Loss: 0.5902352929115295  Recons: 0.2599179148674011  dkl : 0.01651586964726448\n",
      "Current step: 30  Loss: 0.8206565380096436  Recons: 0.3688378930091858  dkl : 0.02259093150496483\n",
      "Current step: 60  Loss: 0.3339877724647522  Recons: 0.17654317617416382  dkl : 0.007872229442000389\n",
      "Saving Model265.torch\n",
      "Current step: 0  Loss: 0.5391498804092407  Recons: 0.25228071212768555  dkl : 0.014343459159135818\n",
      "Current step: 30  Loss: 0.8695633411407471  Recons: 0.36848533153533936  dkl : 0.025053901597857475\n",
      "Current step: 60  Loss: 0.3347952365875244  Recons: 0.17636698484420776  dkl : 0.007921412587165833\n",
      "Current step: 0  Loss: 0.611808180809021  Recons: 0.25041764974594116  dkl : 0.01806952804327011\n",
      "Current step: 30  Loss: 0.8631351590156555  Recons: 0.3701944947242737  dkl : 0.024647032842040062\n",
      "Current step: 60  Loss: 0.39197662472724915  Recons: 0.176602303981781  dkl : 0.010768716223537922\n",
      "Current step: 0  Loss: 0.6599311232566833  Recons: 0.25679564476013184  dkl : 0.020156774669885635\n",
      "Current step: 30  Loss: 0.7747634649276733  Recons: 0.3675840497016907  dkl : 0.020358970388770103\n",
      "Current step: 60  Loss: 0.35740309953689575  Recons: 0.17518383264541626  dkl : 0.009110962972044945\n",
      "Current step: 0  Loss: 0.616005539894104  Recons: 0.2586672306060791  dkl : 0.017866916954517365\n",
      "Current step: 30  Loss: 0.8004059195518494  Recons: 0.3675572872161865  dkl : 0.021642431616783142\n",
      "Current step: 60  Loss: 0.37248387932777405  Recons: 0.17304450273513794  dkl : 0.009971968829631805\n",
      "Current step: 0  Loss: 0.48692598938941956  Recons: 0.2552124857902527  dkl : 0.011585675179958344\n",
      "Current step: 30  Loss: 0.6082053780555725  Recons: 0.36799532175064087  dkl : 0.012010502628982067\n",
      "Current step: 60  Loss: 0.3074336647987366  Recons: 0.1757655143737793  dkl : 0.006583408452570438\n",
      "Saving Model270.torch\n",
      "Current step: 0  Loss: 0.3597899079322815  Recons: 0.2556760311126709  dkl : 0.005205694120377302\n",
      "Current step: 30  Loss: 0.5645592212677002  Recons: 0.3671477437019348  dkl : 0.009870575740933418\n",
      "Current step: 60  Loss: 0.3016352951526642  Recons: 0.17391204833984375  dkl : 0.006386162713170052\n",
      "Current step: 0  Loss: 0.36015987396240234  Recons: 0.2535960078239441  dkl : 0.005328193306922913\n",
      "Current step: 30  Loss: 0.5542480945587158  Recons: 0.3666207194328308  dkl : 0.009381366893649101\n",
      "Current step: 60  Loss: 0.2918272018432617  Recons: 0.17296582460403442  dkl : 0.00594306830316782\n",
      "Current step: 0  Loss: 0.37345176935195923  Recons: 0.2567320466041565  dkl : 0.0058359866961836815\n",
      "Current step: 30  Loss: 0.5652117729187012  Recons: 0.36849039793014526  dkl : 0.009836070239543915\n",
      "Current step: 60  Loss: 0.2880760431289673  Recons: 0.17374467849731445  dkl : 0.005716567859053612\n",
      "Current step: 0  Loss: 0.37764662504196167  Recons: 0.25365978479385376  dkl : 0.006199341267347336\n",
      "Current step: 30  Loss: 0.5496907234191895  Recons: 0.36668848991394043  dkl : 0.009150112047791481\n",
      "Current step: 60  Loss: 0.2768889367580414  Recons: 0.17244714498519897  dkl : 0.00522208958864212\n",
      "Current step: 0  Loss: 0.37243080139160156  Recons: 0.252865731716156  dkl : 0.005978252738714218\n",
      "Current step: 30  Loss: 0.5514209270477295  Recons: 0.3675861954689026  dkl : 0.009191736578941345\n",
      "Current step: 60  Loss: 0.2822868227958679  Recons: 0.173633873462677  dkl : 0.005432648118585348\n",
      "Saving Model275.torch\n",
      "Current step: 0  Loss: 0.37991589307785034  Recons: 0.2550000548362732  dkl : 0.006245792843401432\n",
      "Current step: 30  Loss: 0.561265766620636  Recons: 0.36866694688796997  dkl : 0.009629940614104271\n",
      "Current step: 60  Loss: 0.2862570285797119  Recons: 0.17653590440750122  dkl : 0.005486056208610535\n",
      "Current step: 0  Loss: 0.36934971809387207  Recons: 0.26148897409439087  dkl : 0.005393036641180515\n",
      "Current step: 30  Loss: 0.5635687112808228  Recons: 0.3663651943206787  dkl : 0.009860176593065262\n",
      "Current step: 60  Loss: 0.2926177978515625  Recons: 0.18140214681625366  dkl : 0.005560783203691244\n",
      "Current step: 0  Loss: 0.35429835319519043  Recons: 0.2562563419342041  dkl : 0.004902099724858999\n",
      "Current step: 30  Loss: 0.569875955581665  Recons: 0.3685588240623474  dkl : 0.010065856389701366\n",
      "Current step: 60  Loss: 0.2788484990596771  Recons: 0.1767452359199524  dkl : 0.005105162970721722\n",
      "Current step: 0  Loss: 0.33613425493240356  Recons: 0.24879568815231323  dkl : 0.004366927780210972\n",
      "Current step: 30  Loss: 0.5523593425750732  Recons: 0.36737847328186035  dkl : 0.00924904178828001\n",
      "Current step: 60  Loss: 0.2936270236968994  Recons: 0.17198723554611206  dkl : 0.0060819899663329124\n",
      "Current step: 0  Loss: 0.3583018183708191  Recons: 0.2549667954444885  dkl : 0.005166750401258469\n",
      "Current step: 30  Loss: 0.5457870364189148  Recons: 0.36768537759780884  dkl : 0.008905082941055298\n",
      "Current step: 60  Loss: 0.28409916162490845  Recons: 0.17386960983276367  dkl : 0.005511478055268526\n",
      "Saving Model280.torch\n",
      "Current step: 0  Loss: 0.3507978618144989  Recons: 0.2545180320739746  dkl : 0.004813991487026215\n",
      "Current step: 30  Loss: 0.560053825378418  Recons: 0.36719727516174316  dkl : 0.0096428282558918\n",
      "Current step: 60  Loss: 0.2786344885826111  Recons: 0.17081862688064575  dkl : 0.005390793085098267\n",
      "Current step: 0  Loss: 0.33845797181129456  Recons: 0.25090742111206055  dkl : 0.004377527162432671\n",
      "Current step: 30  Loss: 0.5970985889434814  Recons: 0.36677128076553345  dkl : 0.011516367085278034\n",
      "Current step: 60  Loss: 0.27384841442108154  Recons: 0.17229223251342773  dkl : 0.00507780909538269\n",
      "Current step: 0  Loss: 0.32065677642822266  Recons: 0.24841386079788208  dkl : 0.003612146247178316\n",
      "Current step: 30  Loss: 0.5388988256454468  Recons: 0.36772871017456055  dkl : 0.008558507077395916\n",
      "Current step: 60  Loss: 0.2834860682487488  Recons: 0.1737992763519287  dkl : 0.005484339315444231\n",
      "Current step: 0  Loss: 0.3339370787143707  Recons: 0.24947625398635864  dkl : 0.004223041236400604\n",
      "Current step: 30  Loss: 0.5191221237182617  Recons: 0.367462694644928  dkl : 0.007582972291857004\n",
      "Current step: 60  Loss: 0.3028596043586731  Recons: 0.17257744073867798  dkl : 0.006514107808470726\n",
      "Current step: 0  Loss: 0.34350505471229553  Recons: 0.25083208084106445  dkl : 0.004633648786693811\n",
      "Current step: 30  Loss: 0.5026904344558716  Recons: 0.3663182854652405  dkl : 0.0068186065182089806\n",
      "Current step: 60  Loss: 0.3024783134460449  Recons: 0.17250531911849976  dkl : 0.006498650182038546\n",
      "Saving Model285.torch\n",
      "Current step: 0  Loss: 0.3550873100757599  Recons: 0.252479612827301  dkl : 0.005130385048687458\n",
      "Current step: 30  Loss: 0.47263503074645996  Recons: 0.3666524291038513  dkl : 0.005299130920320749\n",
      "Current step: 60  Loss: 0.30705463886260986  Recons: 0.17192411422729492  dkl : 0.006756525486707687\n",
      "Current step: 0  Loss: 0.3534037470817566  Recons: 0.24750089645385742  dkl : 0.005295141600072384\n",
      "Current step: 30  Loss: 0.4688471555709839  Recons: 0.36665433645248413  dkl : 0.0051096403039991856\n",
      "Current step: 60  Loss: 0.2984735369682312  Recons: 0.17348021268844604  dkl : 0.006249666213989258\n",
      "Current step: 0  Loss: 0.35241231322288513  Recons: 0.25896310806274414  dkl : 0.004672460723668337\n",
      "Current step: 30  Loss: 0.4621374309062958  Recons: 0.37150245904922485  dkl : 0.0045317490585148335\n",
      "Current step: 60  Loss: 0.2988145351409912  Recons: 0.1748926043510437  dkl : 0.0061960965394973755\n",
      "Current step: 0  Loss: 0.3520728349685669  Recons: 0.25650638341903687  dkl : 0.004778321832418442\n",
      "Current step: 30  Loss: 0.4554947018623352  Recons: 0.36611366271972656  dkl : 0.004469052888453007\n",
      "Current step: 60  Loss: 0.3064371943473816  Recons: 0.17714381217956543  dkl : 0.006464669015258551\n",
      "Current step: 0  Loss: 0.35732975602149963  Recons: 0.25059443712234497  dkl : 0.005336766131222248\n",
      "Current step: 30  Loss: 0.46124616265296936  Recons: 0.36650556325912476  dkl : 0.004737029783427715\n",
      "Current step: 60  Loss: 0.29962635040283203  Recons: 0.17855000495910645  dkl : 0.00605381652712822\n",
      "Saving Model290.torch\n",
      "Current step: 0  Loss: 0.3690423369407654  Recons: 0.2546095848083496  dkl : 0.005721637979149818\n",
      "Current step: 30  Loss: 0.46196621656417847  Recons: 0.3665834069252014  dkl : 0.00476914132013917\n",
      "Current step: 60  Loss: 0.2889796793460846  Recons: 0.17660188674926758  dkl : 0.005618889816105366\n",
      "Current step: 0  Loss: 0.3840915262699127  Recons: 0.257093608379364  dkl : 0.00634989608079195\n",
      "Current step: 30  Loss: 0.4650301933288574  Recons: 0.3673313856124878  dkl : 0.004884941037744284\n",
      "Current step: 60  Loss: 0.2759574353694916  Recons: 0.17314094305038452  dkl : 0.005140824243426323\n",
      "Current step: 0  Loss: 0.38539159297943115  Recons: 0.2547793984413147  dkl : 0.006530610378831625\n",
      "Current step: 30  Loss: 0.4620088040828705  Recons: 0.3701484799385071  dkl : 0.0045930165797472\n",
      "Current step: 60  Loss: 0.2801036834716797  Recons: 0.17145591974258423  dkl : 0.005432387813925743\n",
      "Current step: 0  Loss: 0.4078627824783325  Recons: 0.24925965070724487  dkl : 0.007930155843496323\n",
      "Current step: 30  Loss: 0.4467659294605255  Recons: 0.36565130949020386  dkl : 0.004055730998516083\n",
      "Current step: 60  Loss: 0.3170773386955261  Recons: 0.17367053031921387  dkl : 0.00717034051194787\n",
      "Current step: 0  Loss: 0.3933318257331848  Recons: 0.2540855407714844  dkl : 0.006962313782423735\n",
      "Current step: 30  Loss: 0.4200444221496582  Recons: 0.3671305775642395  dkl : 0.0026456916239112616\n",
      "Current step: 60  Loss: 0.33328551054000854  Recons: 0.1716461181640625  dkl : 0.008081968873739243\n",
      "Saving Model295.torch\n",
      "Current step: 0  Loss: 0.4256245493888855  Recons: 0.25543636083602905  dkl : 0.008509409613907337\n",
      "Current step: 30  Loss: 0.4375256597995758  Recons: 0.36735910177230835  dkl : 0.0035083279944956303\n",
      "Current step: 60  Loss: 0.37184256315231323  Recons: 0.17514580488204956  dkl : 0.009834838099777699\n",
      "Current step: 0  Loss: 0.3809909224510193  Recons: 0.24867916107177734  dkl : 0.006615588441491127\n",
      "Current step: 30  Loss: 0.4558814764022827  Recons: 0.3657083511352539  dkl : 0.004508655518293381\n",
      "Current step: 60  Loss: 0.3632693290710449  Recons: 0.17265194654464722  dkl : 0.009530868381261826\n",
      "Current step: 0  Loss: 0.40595337748527527  Recons: 0.25495392084121704  dkl : 0.007549972739070654\n",
      "Current step: 30  Loss: 0.49438798427581787  Recons: 0.36615878343582153  dkl : 0.006411460228264332\n",
      "Current step: 60  Loss: 0.361418217420578  Recons: 0.17369890213012695  dkl : 0.009385965764522552\n",
      "Current step: 0  Loss: 0.33076080679893494  Recons: 0.2579723000526428  dkl : 0.003639425151050091\n",
      "Current step: 30  Loss: 0.5262562036514282  Recons: 0.365608274936676  dkl : 0.00803239643573761\n",
      "Current step: 60  Loss: 0.3382631838321686  Recons: 0.1730850338935852  dkl : 0.008258907124400139\n",
      "Current step: 0  Loss: 0.33760741353034973  Recons: 0.26404017210006714  dkl : 0.0036783618852496147\n",
      "Current step: 30  Loss: 0.5736761093139648  Recons: 0.36768752336502075  dkl : 0.010299431160092354\n",
      "Current step: 60  Loss: 0.3475192189216614  Recons: 0.20128220319747925  dkl : 0.007311851717531681\n",
      "Saving Model300.torch\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ba9ac450d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#------------------------------------------------------------------\n",
    "# TRAINING\n",
    "#------------------------------------------------------------------\n",
    "from unet_4block_conv import *\n",
    "from unet_3block_conv import *\n",
    "from outils_prepro import * \n",
    "from data_loader_seg import *\n",
    "from model_prob_unet_init import *\n",
    "from create_images_3_classes import *\n",
    "from training import *\n",
    "from loss import *\n",
    "#---------------------\n",
    "# Data\n",
    "error = 15\n",
    "Area = 100\n",
    "image_shape = (128, 128) \n",
    "batch_size = 2\n",
    "#----------------------\n",
    "path_ori = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\originals_180919\\originals'\n",
    "path = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\ora_180919\\Layers'\n",
    "image_Paths, mask_Paths = create_dir_paths(path)\n",
    "image_list , mask_list =  create_box_images(image_Paths, mask_Paths, error, Area)\n",
    "print('Images and masks are created')\n",
    "print(len(image_list), len(mask_list))     \n",
    "data = SegmentationDataset(image_list, mask_list, image_shape)\n",
    "dataloaders = torch.utils.data.DataLoader(data, batch_size)\n",
    "\n",
    "#----------------------\n",
    "# Model \n",
    "input_channels = 3\n",
    "num_classes = 3\n",
    "learning_rate = 1e-3  \n",
    "epochs = 301\n",
    "loss_fn  =  DiceLoss() \n",
    "folder_name = 'model1'\n",
    "\n",
    "beta = 20\n",
    "filters = 8\n",
    "z_dim = 10\n",
    " # different learning rate and the same error as Porba Unet\n",
    "#----------------------\n",
    "if filters == 8:\n",
    "    featureDim = 16384\n",
    "if filters ==4:\n",
    "    featureDim = 8192\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelPath = os.path.join(r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg', folder_name)\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)\n",
    "\n",
    "net = Probabilistic_UNET(input_channels, num_classes, filters, z_dim, image_shape, featureDim)\n",
    "#net = UNET(input_channels, num_classes) \n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0)\n",
    "#summary(net)\n",
    "#sum(p.numel() for p in net.parameters() if p.requires_grad) \n",
    "tloss, tloss_list = training_probaUnet(dataloaders, epochs, device, beta, net, optimizer, modelPath)\n",
    "#training_Unet(dataloaders, epochs, device, loss_fn, net, optimizer, modelPath)\n",
    "plt.plot(tloss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(dataloaders): \n",
    "    X, y = batch\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "76 76 76\n",
      "76 76 76 76\n",
      "Images and masks are created\n",
      "143 143\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#------------------------------------------------------------------\n",
    "# TRAINING\n",
    "#------------------------------------------------------------------\n",
    "from unet_4block_conv import *\n",
    "from unet_3block_conv import *\n",
    "from outils_prepro import * \n",
    "from data_loader_seg import *\n",
    "from model_prob_unet_init import *\n",
    "from create_images_3_classes import *\n",
    "from training import *\n",
    "from loss import *\n",
    "#---------------------\n",
    "# Data\n",
    "error = 25\n",
    "Area = 80\n",
    "image_shape = (128, 128) \n",
    "batch_size = 2\n",
    "#----------------------\n",
    "path_ori = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\originals_180919\\originals'\n",
    "path = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\ora_180919\\Layers'\n",
    "image_Paths, mask_Paths = create_dir_paths(path)\n",
    "image_list , mask_list =  create_box_images(image_Paths, mask_Paths, error, Area)\n",
    "print('Images and masks are created')\n",
    "print(len(image_list), len(mask_list))   \n",
    "\n",
    "# train_transform = A.Compose([A.Resize(image_shape[0],image_shape[1], always_apply=True),\n",
    "#                             A.Rotate(limit=15,p=0.1),\n",
    "#                             A.HorizontalFlip(p=0.5),\n",
    "#                             ToTensorV2()])\n",
    "                             \n",
    "train_transform = A.Compose([\n",
    "    A.Resize(image_shape[0],image_shape[1], always_apply=True),\n",
    "    #A.RandomCrop(height=200, width = 200, p=0.2),\n",
    "    A.PadIfNeeded(min_height=image_shape[0], min_width=image_shape[1], border_mode=cv2.BORDER_CONSTANT, \n",
    "                  always_apply=True),\n",
    "    A.VerticalFlip(p=0.2),              \n",
    "    A.Blur(p=0.2),\n",
    "    A.RandomRotate90(p=0.2),\n",
    "    A.ShiftScaleRotate(p=0.2, border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RandomSunFlare(p=0.2, src_radius=200),\n",
    "    A.RandomShadow(p=0.2),\n",
    "    A.RandomFog(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "]\n",
    ")\n",
    "\n",
    "data = SegmentationDataset_DA(image_list, mask_list, image_shape, transforms=train_transform)\n",
    "dataloaders = torch.utils.data.DataLoader(data, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "76 76 76\n",
      "76 76 76 76\n",
      "Images and masks are created\n",
      "143 143\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#------------------------------------------------------------------\n",
    "# TRAINING\n",
    "#------------------------------------------------------------------\n",
    "from unet_4block_conv import *\n",
    "from unet_3block_conv import *\n",
    "from outils_prepro import * \n",
    "from data_loader_seg import *\n",
    "from model_prob_unet_init import *\n",
    "from create_images_3_classes import *\n",
    "from training import *\n",
    "from loss import *\n",
    "#---------------------\n",
    "# Data\n",
    "error = 25\n",
    "Area = 80\n",
    "image_shape = (128, 128) \n",
    "batch_size = 2\n",
    "#----------------------\n",
    "path_ori = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\originals_180919\\originals'\n",
    "path = r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg\\ora_180919\\Layers'\n",
    "image_Paths, mask_Paths = create_dir_paths(path)\n",
    "image_list , mask_list, _ =  create_box_images(image_Paths, mask_Paths, error, Area)\n",
    "print('Images and masks are created')\n",
    "print(len(image_list), len(mask_list))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# image = image_list[0]\n",
    "# mask =  mask_list[0]\n",
    "# image  = image.astype(np.float32)\n",
    "# mask  = mask.astype(np.float32)\n",
    "# # train_transform = A.Compose([A.Resize(image_shape[0],image_shape[1], always_apply=True),\n",
    "# #                             A.Rotate(limit=15,p=0.1),\n",
    "# #                             A.HorizontalFlip(p=0.5),\n",
    "# #                             ToTensorV2()])\n",
    "# train_transform = A.Compose([\n",
    "#     A.Resize(image_shape[0],image_shape[1], always_apply=True),\n",
    "#     A.RandomCrop(height=200, width = 200, p=0.2),\n",
    "#     A.PadIfNeeded(min_height=400, min_width=400, border_mode=cv2.BORDER_CONSTANT, \n",
    "#                   always_apply=True),\n",
    "#     A.VerticalFlip(p=0.2),              \n",
    "#     A.Blur(p=0.2),\n",
    "#     A.RandomRotate90(p=0.2),\n",
    "#     A.ShiftScaleRotate(p=0.2, border_mode=cv2.BORDER_CONSTANT),\n",
    "#     A.RandomBrightnessContrast(p=0.2),\n",
    "#     A.RandomSunFlare(p=0.2, src_radius=200),\n",
    "#     A.RandomShadow(p=0.2),\n",
    "#     A.RandomFog(p=0.2),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2(transpose_mask=True)\n",
    "# ]\n",
    "# )\n",
    "# transformed = train_transform(image=image, mask=mask)\n",
    "# image = transformed[\"image\"]\n",
    "# mask = transformed[\"mask\"]\n",
    "# image = torch.tensor(image, dtype=torch.float32)\n",
    "# mask = torch.tensor(mask, dtype=torch.long) \n",
    "# print(image.shape, mask.shape)\n",
    "\n",
    "for step, batch in enumerate(dataloaders): \n",
    "    X, y = batch\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 0  Loss: 0.7614256739616394  \n",
      "Current step: 30  Loss: 0.7852761745452881  \n",
      "Current step: 60  Loss: 0.7704504132270813  \n",
      "Saving Model0.torch\n",
      "Current step: 0  Loss: 0.7352789640426636  \n",
      "Current step: 30  Loss: 0.7344982624053955  \n",
      "Current step: 60  Loss: 0.7117462158203125  \n",
      "Current step: 0  Loss: 0.6347754001617432  \n",
      "Current step: 30  Loss: 0.6959714889526367  \n",
      "Current step: 60  Loss: 0.686574399471283  \n",
      "Current step: 0  Loss: 0.608639121055603  \n",
      "Current step: 30  Loss: 0.6397173404693604  \n",
      "Current step: 60  Loss: 0.6722604036331177  \n",
      "Current step: 0  Loss: 0.6173962354660034  \n",
      "Current step: 30  Loss: 0.6436432600021362  \n",
      "Current step: 60  Loss: 0.6388040781021118  \n",
      "Current step: 0  Loss: 0.6170381307601929  \n",
      "Current step: 30  Loss: 0.6370139122009277  \n",
      "Current step: 60  Loss: 0.6181458234786987  \n",
      "Saving Model5.torch\n",
      "Current step: 0  Loss: 0.5889506340026855  \n",
      "Current step: 30  Loss: 0.6207151412963867  \n",
      "Current step: 60  Loss: 0.6368532180786133  \n",
      "Current step: 0  Loss: 0.6326678991317749  \n",
      "Current step: 30  Loss: 0.6103347539901733  \n",
      "Current step: 60  Loss: 0.6240311861038208  \n",
      "Current step: 0  Loss: 0.5962569713592529  \n",
      "Current step: 30  Loss: 0.6075628995895386  \n",
      "Current step: 60  Loss: 0.619223952293396  \n",
      "Current step: 0  Loss: 0.5738645792007446  \n",
      "Current step: 30  Loss: 0.5952385663986206  \n",
      "Current step: 60  Loss: 0.610770583152771  \n",
      "Current step: 0  Loss: 0.5775127410888672  \n",
      "Current step: 30  Loss: 0.5984967947006226  \n",
      "Current step: 60  Loss: 0.6156904697418213  \n",
      "Saving Model10.torch\n",
      "Current step: 0  Loss: 0.609011173248291  \n",
      "Current step: 30  Loss: 0.5493030548095703  \n",
      "Current step: 60  Loss: 0.6145164966583252  \n",
      "Current step: 0  Loss: 0.5918616056442261  \n",
      "Current step: 30  Loss: 0.5822386741638184  \n",
      "Current step: 60  Loss: 0.6070374250411987  \n",
      "Current step: 0  Loss: 0.556525707244873  \n",
      "Current step: 30  Loss: 0.6361507177352905  \n",
      "Current step: 60  Loss: 0.6372673511505127  \n",
      "Current step: 0  Loss: 0.5589010715484619  \n",
      "Current step: 30  Loss: 0.5871504545211792  \n",
      "Current step: 60  Loss: 0.6054238080978394  \n",
      "Current step: 0  Loss: 0.548728346824646  \n",
      "Current step: 30  Loss: 0.5876110792160034  \n",
      "Current step: 60  Loss: 0.6072674989700317  \n",
      "Saving Model15.torch\n",
      "Current step: 0  Loss: 0.5465900897979736  \n",
      "Current step: 30  Loss: 0.5841861963272095  \n",
      "Current step: 60  Loss: 0.5923323631286621  \n",
      "Current step: 0  Loss: 0.5498844385147095  \n",
      "Current step: 30  Loss: 0.5776317119598389  \n",
      "Current step: 60  Loss: 0.6088632345199585  \n",
      "Current step: 0  Loss: 0.5145348310470581  \n",
      "Current step: 30  Loss: 0.6058493852615356  \n",
      "Current step: 60  Loss: 0.6018085479736328  \n",
      "Current step: 0  Loss: 0.5688762664794922  \n",
      "Current step: 30  Loss: 0.6227638721466064  \n",
      "Current step: 60  Loss: 0.5874183177947998  \n",
      "Current step: 0  Loss: 0.5488637685775757  \n",
      "Current step: 30  Loss: 0.5699717998504639  \n",
      "Current step: 60  Loss: 0.6066362857818604  \n",
      "Saving Model20.torch\n",
      "Current step: 0  Loss: 0.6132267713546753  \n",
      "Current step: 30  Loss: 0.5758359432220459  \n",
      "Current step: 60  Loss: 0.6152735948562622  \n",
      "Current step: 0  Loss: 0.5253421068191528  \n",
      "Current step: 30  Loss: 0.5958849191665649  \n",
      "Current step: 60  Loss: 0.5704388618469238  \n",
      "Current step: 0  Loss: 0.5324289798736572  \n",
      "Current step: 30  Loss: 0.6127382516860962  \n",
      "Current step: 60  Loss: 0.5871645212173462  \n",
      "Current step: 0  Loss: 0.5148850679397583  \n",
      "Current step: 30  Loss: 0.5618598461151123  \n",
      "Current step: 60  Loss: 0.6471707820892334  \n",
      "Current step: 0  Loss: 0.5565786361694336  \n",
      "Current step: 30  Loss: 0.5623723268508911  \n",
      "Current step: 60  Loss: 0.6060100793838501  \n",
      "Saving Model25.torch\n",
      "Current step: 0  Loss: 0.5310779809951782  \n",
      "Current step: 30  Loss: 0.5610799789428711  \n",
      "Current step: 60  Loss: 0.569618821144104  \n",
      "Current step: 0  Loss: 0.5100505352020264  \n",
      "Current step: 30  Loss: 0.6051839590072632  \n",
      "Current step: 60  Loss: 0.6020464897155762  \n",
      "Current step: 0  Loss: 0.5614614486694336  \n",
      "Current step: 30  Loss: 0.5603150129318237  \n",
      "Current step: 60  Loss: 0.6018821001052856  \n",
      "Current step: 0  Loss: 0.5069689750671387  \n",
      "Current step: 30  Loss: 0.6352630853652954  \n",
      "Current step: 60  Loss: 0.610776424407959  \n",
      "Current step: 0  Loss: 0.5081620216369629  \n",
      "Current step: 30  Loss: 0.5643230676651001  \n",
      "Current step: 60  Loss: 0.6026369333267212  \n",
      "Saving Model30.torch\n",
      "Current step: 0  Loss: 0.5525244474411011  \n",
      "Current step: 30  Loss: 0.5821230411529541  \n",
      "Current step: 60  Loss: 0.5974198579788208  \n",
      "Current step: 0  Loss: 0.5606917142868042  \n",
      "Current step: 30  Loss: 0.542561411857605  \n",
      "Current step: 60  Loss: 0.6139698028564453  \n",
      "Current step: 0  Loss: 0.5563353300094604  \n",
      "Current step: 30  Loss: 0.5609958171844482  \n",
      "Current step: 60  Loss: 0.5884301662445068  \n",
      "Current step: 0  Loss: 0.5208765268325806  \n",
      "Current step: 30  Loss: 0.5534180402755737  \n",
      "Current step: 60  Loss: 0.5955179929733276  \n",
      "Current step: 0  Loss: 0.518417239189148  \n",
      "Current step: 30  Loss: 0.530983567237854  \n",
      "Current step: 60  Loss: 0.6191781759262085  \n",
      "Saving Model35.torch\n",
      "Current step: 0  Loss: 0.5414383411407471  \n",
      "Current step: 30  Loss: 0.5521372556686401  \n",
      "Current step: 60  Loss: 0.6369165182113647  \n",
      "Current step: 0  Loss: 0.5193018913269043  \n",
      "Current step: 30  Loss: 0.5560745000839233  \n",
      "Current step: 60  Loss: 0.6131911277770996  \n",
      "Current step: 0  Loss: 0.5163218975067139  \n",
      "Current step: 30  Loss: 0.5935537815093994  \n",
      "Current step: 60  Loss: 0.5946298837661743  \n",
      "Current step: 0  Loss: 0.5329732894897461  \n",
      "Current step: 30  Loss: 0.5519298315048218  \n",
      "Current step: 60  Loss: 0.6562168598175049  \n",
      "Current step: 0  Loss: 0.49880772829055786  \n",
      "Current step: 30  Loss: 0.552312970161438  \n",
      "Current step: 60  Loss: 0.5965563058853149  \n",
      "Saving Model40.torch\n",
      "Current step: 0  Loss: 0.5150743722915649  \n",
      "Current step: 30  Loss: 0.6276490688323975  \n",
      "Current step: 60  Loss: 0.5909600257873535  \n",
      "Current step: 0  Loss: 0.4998432993888855  \n",
      "Current step: 30  Loss: 0.5608888864517212  \n",
      "Current step: 60  Loss: 0.594780445098877  \n",
      "Current step: 0  Loss: 0.5552530288696289  \n",
      "Current step: 30  Loss: 0.5616308450698853  \n",
      "Current step: 60  Loss: 0.5914167165756226  \n",
      "Current step: 0  Loss: 0.4863739609718323  \n",
      "Current step: 30  Loss: 0.5726498365402222  \n",
      "Current step: 60  Loss: 0.624138593673706  \n",
      "Current step: 0  Loss: 0.5141472816467285  \n",
      "Current step: 30  Loss: 0.5449244976043701  \n",
      "Current step: 60  Loss: 0.5924276113510132  \n",
      "Saving Model45.torch\n",
      "Current step: 0  Loss: 0.5312865972518921  \n",
      "Current step: 30  Loss: 0.5569020509719849  \n",
      "Current step: 60  Loss: 0.6131066083908081  \n",
      "Current step: 0  Loss: 0.5176540613174438  \n",
      "Current step: 30  Loss: 0.5958613157272339  \n",
      "Current step: 60  Loss: 0.5767616033554077  \n",
      "Current step: 0  Loss: 0.4912528991699219  \n",
      "Current step: 30  Loss: 0.4691619277000427  \n",
      "Current step: 60  Loss: 0.6166424751281738  \n",
      "Current step: 0  Loss: 0.5719327926635742  \n",
      "Current step: 30  Loss: 0.5480904579162598  \n",
      "Current step: 60  Loss: 0.6377831697463989  \n",
      "Current step: 0  Loss: 0.5087145566940308  \n",
      "Current step: 30  Loss: 0.502578854560852  \n",
      "Current step: 60  Loss: 0.5722814798355103  \n",
      "Saving Model50.torch\n",
      "Current step: 0  Loss: 0.48962682485580444  \n",
      "Current step: 30  Loss: 0.6027665138244629  \n",
      "Current step: 60  Loss: 0.5911798477172852  \n",
      "Current step: 0  Loss: 0.4934852123260498  \n",
      "Current step: 30  Loss: 0.5246118307113647  \n",
      "Current step: 60  Loss: 0.6238605976104736  \n",
      "Current step: 0  Loss: 0.49911415576934814  \n",
      "Current step: 30  Loss: 0.5482509136199951  \n",
      "Current step: 60  Loss: 0.6303315162658691  \n",
      "Current step: 0  Loss: 0.49330973625183105  \n",
      "Current step: 30  Loss: 0.5927779674530029  \n",
      "Current step: 60  Loss: 0.5762847661972046  \n",
      "Current step: 0  Loss: 0.5149304866790771  \n",
      "Current step: 30  Loss: 0.5502529144287109  \n",
      "Current step: 60  Loss: 0.5913190841674805  \n",
      "Saving Model55.torch\n",
      "Current step: 0  Loss: 0.5107917785644531  \n",
      "Current step: 30  Loss: 0.5479875802993774  \n",
      "Current step: 60  Loss: 0.606063961982727  \n",
      "Current step: 0  Loss: 0.5181750059127808  \n",
      "Current step: 30  Loss: 0.5596215724945068  \n",
      "Current step: 60  Loss: 0.6021522283554077  \n",
      "Current step: 0  Loss: 0.5472272634506226  \n",
      "Current step: 30  Loss: 0.49431312084198  \n",
      "Current step: 60  Loss: 0.5912312269210815  \n",
      "Current step: 0  Loss: 0.5233074426651001  \n",
      "Current step: 30  Loss: 0.5431965589523315  \n",
      "Current step: 60  Loss: 0.5900477170944214  \n",
      "Current step: 0  Loss: 0.5314491987228394  \n",
      "Current step: 30  Loss: 0.5454168319702148  \n",
      "Current step: 60  Loss: 0.5567482709884644  \n",
      "Saving Model60.torch\n",
      "Current step: 0  Loss: 0.5080769062042236  \n",
      "Current step: 30  Loss: 0.5529577732086182  \n",
      "Current step: 60  Loss: 0.5994961261749268  \n",
      "Current step: 0  Loss: 0.5452694892883301  \n",
      "Current step: 30  Loss: 0.5886702537536621  \n",
      "Current step: 60  Loss: 0.5875574350357056  \n",
      "Current step: 0  Loss: 0.51271653175354  \n",
      "Current step: 30  Loss: 0.5446804761886597  \n",
      "Current step: 60  Loss: 0.6042664051055908  \n",
      "Current step: 0  Loss: 0.5116318464279175  \n",
      "Current step: 30  Loss: 0.4829917550086975  \n",
      "Current step: 60  Loss: 0.5717506408691406  \n",
      "Current step: 0  Loss: 0.5055868625640869  \n",
      "Current step: 30  Loss: 0.5431188344955444  \n",
      "Current step: 60  Loss: 0.5896161794662476  \n",
      "Saving Model65.torch\n",
      "Current step: 0  Loss: 0.5108380317687988  \n",
      "Current step: 30  Loss: 0.5452831983566284  \n",
      "Current step: 60  Loss: 0.5910582542419434  \n",
      "Current step: 0  Loss: 0.5028588771820068  \n",
      "Current step: 30  Loss: 0.5441528558731079  \n",
      "Current step: 60  Loss: 0.6066704988479614  \n",
      "Current step: 0  Loss: 0.5120741128921509  \n",
      "Current step: 30  Loss: 0.5448325872421265  \n",
      "Current step: 60  Loss: 0.6124335527420044  \n",
      "Current step: 0  Loss: 0.5035518407821655  \n",
      "Current step: 30  Loss: 0.506443977355957  \n",
      "Current step: 60  Loss: 0.6174982786178589  \n",
      "Current step: 0  Loss: 0.5093111991882324  \n",
      "Current step: 30  Loss: 0.6256359815597534  \n",
      "Current step: 60  Loss: 0.5893245935440063  \n",
      "Saving Model70.torch\n",
      "Current step: 0  Loss: 0.4865376353263855  \n",
      "Current step: 30  Loss: 0.5389617681503296  \n",
      "Current step: 60  Loss: 0.558079719543457  \n",
      "Current step: 0  Loss: 0.5020647048950195  \n",
      "Current step: 30  Loss: 0.5902057886123657  \n",
      "Current step: 60  Loss: 0.6135867834091187  \n",
      "Current step: 0  Loss: 0.48238420486450195  \n",
      "Current step: 30  Loss: 0.5052019357681274  \n",
      "Current step: 60  Loss: 0.6195350885391235  \n",
      "Current step: 0  Loss: 0.5421532392501831  \n",
      "Current step: 30  Loss: 0.5424600839614868  \n",
      "Current step: 60  Loss: 0.5929893255233765  \n",
      "Current step: 0  Loss: 0.5116180181503296  \n",
      "Current step: 30  Loss: 0.5635354518890381  \n",
      "Current step: 60  Loss: 0.5882871150970459  \n",
      "Saving Model75.torch\n",
      "Current step: 0  Loss: 0.48380929231643677  \n",
      "Current step: 30  Loss: 0.5624074935913086  \n",
      "Current step: 60  Loss: 0.571643590927124  \n",
      "Current step: 0  Loss: 0.46456366777420044  \n",
      "Current step: 30  Loss: 0.5542888641357422  \n",
      "Current step: 60  Loss: 0.5947794914245605  \n",
      "Current step: 0  Loss: 0.43468016386032104  \n",
      "Current step: 30  Loss: 0.5430736541748047  \n",
      "Current step: 60  Loss: 0.5885196924209595  \n",
      "Current step: 0  Loss: 0.4576072692871094  \n",
      "Current step: 30  Loss: 0.5526531934738159  \n",
      "Current step: 60  Loss: 0.598021388053894  \n",
      "Current step: 0  Loss: 0.4439705014228821  \n",
      "Current step: 30  Loss: 0.4618605971336365  \n",
      "Current step: 60  Loss: 0.5950814485549927  \n",
      "Saving Model80.torch\n",
      "Current step: 0  Loss: 0.47789663076400757  \n",
      "Current step: 30  Loss: 0.5401221513748169  \n",
      "Current step: 60  Loss: 0.5888957977294922  \n",
      "Current step: 0  Loss: 0.5375112295150757  \n",
      "Current step: 30  Loss: 0.5393418073654175  \n",
      "Current step: 60  Loss: 0.5696704387664795  \n",
      "Current step: 0  Loss: 0.4380902647972107  \n",
      "Current step: 30  Loss: 0.6139868497848511  \n",
      "Current step: 60  Loss: 0.6098228693008423  \n",
      "Current step: 0  Loss: 0.488655149936676  \n",
      "Current step: 30  Loss: 0.5126601457595825  \n",
      "Current step: 60  Loss: 0.6028411388397217  \n",
      "Current step: 0  Loss: 0.42500436305999756  \n",
      "Current step: 30  Loss: 0.5752764940261841  \n",
      "Current step: 60  Loss: 0.6086651086807251  \n",
      "Saving Model85.torch\n",
      "Current step: 0  Loss: 0.41311562061309814  \n",
      "Current step: 30  Loss: 0.5501630306243896  \n",
      "Current step: 60  Loss: 0.5916135311126709  \n",
      "Current step: 0  Loss: 0.42903411388397217  \n",
      "Current step: 30  Loss: 0.5335323810577393  \n",
      "Current step: 60  Loss: 0.5997101068496704  \n",
      "Current step: 0  Loss: 0.4410337209701538  \n",
      "Current step: 30  Loss: 0.5445603132247925  \n",
      "Current step: 60  Loss: 0.5888471603393555  \n",
      "Current step: 0  Loss: 0.4498494267463684  \n",
      "Current step: 30  Loss: 0.5291734933853149  \n",
      "Current step: 60  Loss: 0.6044853925704956  \n",
      "Current step: 0  Loss: 0.42746150493621826  \n",
      "Current step: 30  Loss: 0.5313481092453003  \n",
      "Current step: 60  Loss: 0.5923867225646973  \n",
      "Saving Model90.torch\n",
      "Current step: 0  Loss: 0.386829674243927  \n",
      "Current step: 30  Loss: 0.5311646461486816  \n",
      "Current step: 60  Loss: 0.5887260437011719  \n",
      "Current step: 0  Loss: 0.4201676845550537  \n",
      "Current step: 30  Loss: 0.5410891771316528  \n",
      "Current step: 60  Loss: 0.5905236005783081  \n",
      "Current step: 0  Loss: 0.3911123275756836  \n",
      "Current step: 30  Loss: 0.536212682723999  \n",
      "Current step: 60  Loss: 0.6054635047912598  \n",
      "Current step: 0  Loss: 0.5638731718063354  \n",
      "Current step: 30  Loss: 0.5064171552658081  \n",
      "Current step: 60  Loss: 0.588238000869751  \n",
      "Current step: 0  Loss: 0.4045451283454895  \n",
      "Current step: 30  Loss: 0.5054738521575928  \n",
      "Current step: 60  Loss: 0.5934408903121948  \n",
      "Saving Model95.torch\n",
      "Current step: 0  Loss: 0.36812108755111694  \n",
      "Current step: 30  Loss: 0.5353444814682007  \n",
      "Current step: 60  Loss: 0.5739622116088867  \n",
      "Current step: 0  Loss: 0.44702810049057007  \n",
      "Current step: 30  Loss: 0.5396887063980103  \n",
      "Current step: 60  Loss: 0.5752391815185547  \n",
      "Current step: 0  Loss: 0.4145885705947876  \n",
      "Current step: 30  Loss: 0.5628745555877686  \n",
      "Current step: 60  Loss: 0.5939667224884033  \n",
      "Current step: 0  Loss: 0.4141886830329895  \n",
      "Current step: 30  Loss: 0.5297085046768188  \n",
      "Current step: 60  Loss: 0.5955808162689209  \n",
      "Current step: 0  Loss: 0.3652798533439636  \n",
      "Current step: 30  Loss: 0.47121304273605347  \n",
      "Current step: 60  Loss: 0.5511702299118042  \n",
      "Saving Model100.torch\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a10005d700>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABQn0lEQVR4nO29eZxddX3//3zfffYlM5M9TBLCkgSEEAIoihsKqIBL+wVF/NoFqeTbarUWv/xqsdpvrVppqQpFixW1UqVaU0XR4o4sCUsCAQIhCckkk2Qms293/fz+OMs9995z75yZzGQmM+/n4zGPmXvuOfeez8zc8zrvXYwxKIqiKIqX0EyfgKIoijL7UHFQFEVRSlBxUBRFUUpQcVAURVFKUHFQFEVRSojM9AlMBS0tLaa9vX2mT0NRFOWk4vHHH+82xrT6PTcnxKG9vZ1t27bN9GkoiqKcVIjIy+WeU7eSoiiKUoKKg6IoilKCioOiKIpSgoqDoiiKUkIgcRCRy0Rkl4jsFpGbfZ4XEbndfn6HiGwY71gRuVVEDorIU/bXFUWvuUJEhkTko8ezQEVRFGXijCsOIhIGvgRcDqwFrhWRtUW7XQ6ssb9uAO4IeOxtxphz7K/7i17zNuDHE1+SoiiKcrwEsRw2AbuNMXuMMSngXuCqon2uAu4xFo8AjSKyOOCxJYjI1cAeYGfwpSiKoihTRRBxWAoc8DzusLcF2We8Yzfbbqi7RaQJQERqgL8EPlnppETkBhHZJiLburq6AiyjlEN9o3zhp7vY2z08qeMVRVHmKkHEQXy2FQ+BKLdPpWPvAFYD5wCdwD/Y2z+J5W4aqnRSxpi7jDEbjTEbW1t9C/zG5dhQitt/vpsXjwxO6nhFUZS5SpAK6Q5guefxMuBQwH1i5Y41xhxxNorIV4Af2g8vAN4lIp8FGoGciIwZY74Y4FwnRG3CWv5wKjPVL60oinJSE0QctgJrRGQlcBC4Bnh30T5bsFxE92Jd3PuNMZ0i0lXuWBFZbIzptI9/O/AMgDHm1c6LisitwNB0CANATTwMwNCYioOiKIqXccXBGJMRkc3AA0AYuNsYs1NEbrSfvxO4H7gC2A2MAO+vdKz90p8VkXOw3Ez7gA9M4boCURePAjCUzJ7ot1YURZnVBGq8Z6eZ3l+07U7Pzwa4Keix9vb3BnjfW4Oc32RJREOEBIaS6el8G0VRlJOOeV0hLSLUxiMMq+WgKIpSwLwWB4DaeIRBjTkoiqIUoOKQiDCcVHFQFEXxMu/FoSYeYUjFQVEUpYB5Lw61Kg6KoiglqDioOCiKopSg4hDXmIOiKEox814cNOagKIpSyrwXB8dysOr4FEVRFFBxoDYRIWdgNK2FcIqiKA7zXhxq4lYHEW2+pyiKkmfei0OdIw4ad1AURXGZ9+JQo+KgKIpSwrwXh1oVB0VRlBJUHDTmoCiKUoKKg44KVRRFKSGQOIjIZSKyS0R2i8jNPs+LiNxuP79DRDaMd6yI3CoiB0XkKfvrCnv7pSLyuIg8bX9//VQstBw6KlRRFKWUcSfBiUgY+BJwKdABbBWRLcaYZz27XQ6ssb8uAO4ALghw7G3GmM8XvWU38DZjzCERWY81YnTppFc4DjoqVFEUpZQglsMmYLcxZo8xJgXcC1xVtM9VwD3G4hGgUUQWBzy2AGPMk8aYQ/bDnUBCROITWNOE0FGhiqIopQQRh6XAAc/jDkrv5MvtM96xm2031N0i0uTz3u8EnjTGJIufEJEbRGSbiGzr6uoKsAx/dFSooihKKUHEQXy2FTciKrdPpWPvAFYD5wCdwD8UvKDIOuDvgQ/4nZQx5i5jzEZjzMbW1tayJx8EHRWqKIpSyLgxB6y7/eWex8uAQwH3iZU71hhzxNkoIl8Bfuh5vAz4PnC9MealAOd4XOioUEVRlEKCWA5bgTUislJEYsA1wJaifbYA19tZSxcC/caYzkrH2jEJh7cDz9jbG4EfAR83xjw0+aUFR9t2K4qiFDKu5WCMyYjIZqysoTBwtzFmp4jcaD9/J3A/cAWwGxgB3l/pWPulPysi52C5mfaRdx9tBk4F/kpE/sre9iZjzNHjXGtZ1K2kKIpSSBC3EsaY+7EEwLvtTs/PBrgp6LH29veW2f/TwKeDnNdUURuPcLh/7ES+paIoyqxm3ldIg86RVhRFKUbFAY05KIqiFKPiANQldFSooiiKFxUHLMtBR4UqiqLkUXFA23YriqIUo+KADvxRFEUpRsUBFQdFUZRiVBzQOdKKoijFqDigMQdFUZRiVBzQUaGKoijFqDigo0IVRVGKUXFAR4UqiqIUo+KAjgpVFEUpRsUBHRWqKIpSjIqDjc50UBRFyaPiYKOjQhVFUfIEEgcRuUxEdonIbhG52ed5EZHb7ed3iMiG8Y4VkVtF5KCIPGV/XeF57uP2/rtE5M3Hu8ggaNtuRVGUPONOghORMPAl4FKgA9gqIluMMc96drscWGN/XQDcAVwQ4NjbjDGfL3q/tVizptcBS4D/EZHTjDHTGhBQt5KiKEqeIJbDJmC3MWaPMSYF3AtcVbTPVcA9xuIRoFFEFgc8tpirgHuNMUljzF6sudSbJrCmSWEFpFUcFEVRIJg4LAUOeB532NuC7DPesZttN9TdItI0gfdDRG4QkW0isq2rqyvAMiqjo0IVRVHyBBEH8dlWPDKt3D6Vjr0DWA2cA3QC/zCB98MYc5cxZqMxZmNra6vPIRNDYw6Koih5xo05YN25L/c8XgYcCrhPrNyxxpgjzkYR+Qrwwwm835TjHRUq4qdPiqIo84cglsNWYI2IrBSRGFaweEvRPluA6+2spQuBfmNMZ6Vj7ZiEw9uBZzyvdY2IxEVkJVaQ+7FJri8wOipUURQlz7iWgzEmIyKbgQeAMHC3MWaniNxoP38ncD9wBVbweAR4f6Vj7Zf+rIicg+Uy2gd8wD5mp4h8B3gWyAA3TXemEhS27a6OBTGoFEVR5i6BroLGmPuxBMC77U7Pzwa4Keix9vb3Vni/vwX+Nsi5TRXeaXBtJ/KNFUVRZiFaIW2jo0IVRVHyqDjY6KhQRVGUPCoONnUJHRWqKIrioOJg01BlDfzpHUnN8JkoiqLMPCoONm31cQCODiRn+EwURVFmHhUHm3gkTFN1lCODYzN9KoqiKDOOioOHtroER9RyUBRFUXHw0lYf5+igioOiKIqKg4eF9QmODqhbSVEURcXBw0LbcsjlSprAKoqizCtUHDwsrE+QzRmODWs6q6Io8xsVBw9tdQkAjqhrSVGUeY6Kg4eFTq2DprMqijLPUXHwsLDesRw0Y0lRlPmNioOHllrLclC3kjIfGEtn2X10aKZPQ5mlqDh4iEVCLKiJaa2DMi/47uMdvOX23zCm0w8VHwKJg4hcJiK7RGS3iNzs87yIyO328ztEZMMEjv2oiBgRabEfR0Xk6yLytIg8JyIfP54FTpQ2rXVQ5gm9wymSmZyKg+LLuOIgImHgS8DlwFrgWhFZW7Tb5VizntcANwB3BDlWRJYDlwL7Pa/1e0DcGHMWcB7wARFpn8ziJsPC+rjGHJR5QTqbAyCVyc3wmSizkSCWwyZgtzFmjzEmBdwLXFW0z1XAPcbiEaBRRBYHOPY24GNYc6QdDFAjIhGgCkgBA5NY26RYWJfQmIMyL3BEIanioPgQRByWAgc8jzvsbUH2KXusiFwJHDTGbC96rfuAYaATy6L4vDGmJ8B5TgkL6+N0DyXJZPUDo8xtHFFI6f+64kMQcRCfbcX9Jcrt47tdRKqBW4BP+Dy/CcgCS4CVwEdEZFXJSYncICLbRGRbV1dXpfOfEG31CXIGrZJW5jyOWymt4qD4EEQcOoDlnsfLgEMB9ym3fTXWhX+7iOyztz8hIouAdwM/McakjTFHgYeAjcUnZYy5yxiz0RizsbW1NcAygpGvdVDXkjK3cdxKGnNQ/AgiDluBNSKyUkRiwDXAlqJ9tgDX21lLFwL9xpjOcscaY542xrQZY9qNMe1YIrLBGHMYy5X0evu1aoALgeenYrFBcKqkNSitzHU0IK1UIjLeDsaYjIhsBh4AwsDdxpidInKj/fydwP3AFcBuYAR4f6Vjx3nLLwFfA57Bckt9zRizYzKLmwxOfyVtoaHMdVIqDkoFxhUHAGPM/VgC4N12p+dnA9wU9Fiffdo9Pw9hpbPOCC21MUTUclDmPqmMFTpMasxB8UErpIuIhEO01Ma1EE6Z8ziWQ1otB8UHFQcfrEI4FQdlbpPKWJXRmsqq+KHi4INVCKduJWVuk85abiWNOSh+qDj40Faf0IC0MufRVFalEioOPlhV0iktDlLmNG4qq/6fKz6oOPjgFMJ1aetuZQ6jloNSCRUHH9rqnHGhKg7K3CWlloNSARUHH7SFhjIfUMtBqYSKgw/LmqoA2NM1PMNnoijTh7bPUCqh4uBDY3WM5c1VPHOwf6ZPRVEmRTqbYySVqbiPWg5KJVQcynD2ska2d/TN9GkoyqT455/v5h1f/l3FfVLaslupgIpDGc5e2kBH7yg9Aec6fPx7O/ifZ49M81kpSjAO9Y3S0Tta9nljTL4ITsVB8UHFoQxnLWsA4OkArqVUJse3HzvAg8+rOCizg2Qmx1g6W/Z5ryDomFDFDxWHMqxfaonDjgN94+7rZDX1jaSn85QUJTBj6SyZnCk77taxGkBjDoo/Kg5lqE9EWdVaw44AlsNhFQdlluFYA2NlLvxeQVBxUPxQcajA2UsbeLpjfHHo7LfEoXdE504rs4Ok7VIq51ryBqE1IK34oeJQgbOWNXJ4YGzc2Q6H+63AX/+oWg7K7MC1HMqIQ4HloOKg+BBIHETkMhHZJSK7ReRmn+dFRG63n98hIhsmcOxHRcSISItn29ki8rCI7BSRp0UkMdkFHg+vsIPSO8axHhzLQd1KymxhPHFIqltJGYdxxUFEwlhznS8H1gLXisjaot0uB9bYXzcAdwQ5VkSWA5cC+z3bIsA3gRuNMeuA1wIzctVdu6SekDBu3MEJSI+msxUzRBTlRJF3K5ULSKs4KJUJYjlsAnYbY/YYY1LAvcBVRftcBdxjLB4BGkVkcYBjbwM+BhjPtjcBO4wx2wGMMceMMTNyxa2ORVjTVsfT4xTDOZYDqGtJmR0EdSuFQ6KprIovQcRhKXDA87jD3hZkn7LHisiVwEFHBDycBhgReUBEnhCRj/mdlIjcICLbRGRbV1dXgGVMjrOXNbCjox9jTNl9DvePUR0LA+paUmYHyUwwy6EmFtaAtOJLEHEQn23FV8py+/huF5Fq4BbgEz7PR4CLgffY398uIm8oeRFj7jLGbDTGbGxtba10/sfF2csaODac4lC/f1A6k81xdDDJ6YvqAOjTjCVlFpBMB7Mc6hJRDUgrvgQRhw5guefxMuBQwH3KbV8NrAS2i8g+e/sTIrLIPuZXxphuY8wIcD+wgRnirGWNQPliuO6hFNmc4YxF9QD0quWgzALydQ5lxMGxHOJhjTkovgQRh63AGhFZKSIx4BpgS9E+W4Dr7aylC4F+Y0xnuWONMU8bY9qMMe3GmHYsQdhgjDkMPACcLSLVdnD6EuDZqVjsZDhzcR2xSIgn9vf6Pt9pp7GuXWxZDv2jajkoM0suZ9yLfzm3kiMINfGIioPiS2S8HYwxGRHZjHXRDgN3G2N2isiN9vN3Yt3dXwHsBkaA91c6dpz36xWRL2AJiwHuN8b8aLILPF7ikTDnLGvksX3+4uBkKp2x2LIcNOagzDTeAPNoObeSLR61Kg5KGcYVBwBjzP1YAuDddqfnZwPcFPRYn33aix5/EyuddVZw/som7vzVHoaTGWrihb8yJ1NpVUsN0bDMSreSMYaxdI4qO2iuzG2SHldScpwK6dp4pKDPkqI4aIV0ADatXEA2Z3hyf1/Jc4f7x4iFQzTXxGisjs1Kt9KvXuhiw6d+xrEhnYk9H/BaDuMFpGvjEVLZXMVsPGV+ouIQgA0rGgkJPLavp+S5zv4xFjUkEBEaq6Kz0q20r3uY0XSWPd069nQ+kEx7xaFMzMG2FhxLWDOWlGJUHAJQl4iydkk9j+09VvLcYVscABqro7Oy+d5Q0hoX2dE7MsNnopwIvG6lIJaD97GiOKg4BOT89mae3N9X8iHqHBhlsSsOsVlpOQza4nCwwmQwZe5Q4FYqk8rqFsGpOChlUHEIyAUrm0lmcgWT4YwxHOlP5i2HquisbJ8xNGaLQ5+Kw3zAay2Ml8pam7DEQYPSSjEqDgHZ2N4MwFZP3KFnOEUqm2NR/cniVlJxmA8ESWVNZ3OEBKqiVgabWg5KMSoOAWmpjbOqtYbH9ubFwUlj9bqVxtKVZ/fOBGo5zC+CpLKmMjlikRCxiHUJSGVn1/+sMvOoOEyAC1Y2s21fD7mcZYIftsVhUUMVYFkOMPs6szoxh0N9o5qyOA9wspVqYuGybqVkJkc0HCIWFvexl2xO/0/mOyoOE+D89mYGxjLsOjIIQOdAkeVQFQNm37hQx3IYS+c4Njy7zk2ZepwLfUNVtOKY0LjXcvCIw29f7ObsWx+gW+ti5jUqDhPgfDvu8NOdRwBrPGg4JLTUxgFosi2H2ZaxNJTMUGdnpWjcYe7jCEJ9VbR84z3XcrBiDt6A9N7uIYZTWZ7rHJj+k1VmLSoOE2B5czVvXreQL/7iRbYf6ONwf5KFdXHCIcs0b5ikOBwZGOPQNMYDhpMZTrNbims669yn0HIoP88hVsZyGElZgvLS0aFpPlNlNqPiMEH+/p1n01aXYPO3n2D30UE3jRWsgDRMfKbDx+7bwYf/46mpPM0CBpMZd97EwT4thJvrOAHphqooo6nyjfeiYf+AtCMOWlE/v1FxmCCN1TFuv/YcDvWNsb2jv0AcXLfSBAPSu48OTZu7J5nJksrkWNpYRV0iopbDPMAJSDdWRwsyl7ykMoZYOEQsXGo5OOmvL3Wp5TCfUXGYBOed0sxH33Q6AAvr8+JQFQ0TC4cm5FZKZXJ09o/SNZiclkyi4aT1Qa+JhVnaWKXprPOAZCZHOCTUxsu7lVKuW6k0W2kkZSUw7OlSy2E+E6hlt1LKB16zikw2xxvXLnS3iQgN1dEJuZUO9Y2SM9aHtX807bqmpgonU6k2EWVZU5UGpOcByUyWeCREIhqq0Fspa1sOpQFpx63U2T/GUDLj9l9S5hdqOUySUEj4P29Yw5n2kB+HpuqJdWbd35OPARwdnPrUwcGkdS618YhlOag4zHnG0jlbHMJkcoaMT8fVdNaUDUh74xR71XqYtwQSBxG5TER2ichuEbnZ53kRkdvt53eIyIYJHPtRETEi0lK0fYWIDInIRyezsJmisSpG3wRmOnjFoWsaxMGxHOoSEZY2VTGYzMy6Ij1larEshzCJqPXxHvNpjWGlsopHHAoD0jX2YKg93Rp3mK+MKw4iEga+BFwOrAWuFZG1RbtdDqyxv24A7ghyrIgsBy4F9vu89W3Ajye4nhmnYYKWw4ECy2Fsys/H6atkWQ7VgKazznWSmRzxqGU5gH/b7pJU1myh5XD6ojpCoums85kglsMmYLcxZo8xJgXcC1xVtM9VwD3G4hGgUUQWBzj2NuBjWLOiXUTkamAPUHHe9GxkMm6lhfVWEd3RgWmwHBxxsC0H0B5LU8VsbUWS9LiVAN90VqcILmq3zyioc0hnaKyOsby5mpc0nXXeEkQclgIHPI877G1B9il7rIhcCRw0xmz3vpCI1AB/CXyy0kmJyA0isk1EtnV1dQVYxomhsXribqW1i+tJREPT4lYadNxKdswB4KAO/TlunuscYNP/e5DHX+6d6VMpIZnJkoiGXXHwS2d1s5WcVNaigHRVLMzq1lq1HOYxQcRBfLYV3zKV28d3u4hUA7cAn/B5/pPAbcaYiv+Vxpi7jDEbjTEbW1tbK+16QnGqUoN0ZjXGsP/YCCuaq2mrS0xLQHrYYzm01MaIR0JqORwn6WyOv7hvO12DSV60+2zNJtyAtO0y8ktnTWVyxMIhRIRYOFQSkK6OhlnVUsPe7mG30aQy9Rhj+Opv9kxrh4TJEkQcOoDlnsfLgEMB9ym3fTWwEtguIvvs7U+IyCLgAuCz9vYPAf9XRDYHW87M0+RWSY/vWuofTTOYzLC8uZq2uvikLIcXjwzy9d/tK/v8UDLj9u0XEZZqOutx8y+/eolnDlp9hwbGZl9wPx+QLh9zcCwHgFikUByGkxmqY2FWt9WSzOSO62bicP+YiksFeoZTfPpHz/Hf24svqTNPEHHYCqwRkZUiEgOuAbYU7bMFuN7OWroQ6DfGdJY71hjztDGmzRjTboxpxxKRDcaYw8aYV3u2/yPw/4wxX5yKxZ4IGt0q6fFdS06m0ormalrr4pMKSH/zkZf56y07y1oqg2MZauIRRCwjTgvhjo9dhwf5pwdf5C1nLSYkebfdbCKZKYw5+FkOadtyAFscPO0zRtNZqmIRVrXUAJNvo9E7nOI1n/0FP37m8KSOnw84NSXDZdqczCTjioMxJgNsBh4AngO+Y4zZKSI3isiN9m73YwWQdwNfAT5Y6dgpX8UsorHKEofe4fHvKF1xWGBZDpNxKzkf3HLtlb0dWQGWNWmtw2TJ2O6k+kSUv7lqHbXxCAOzMC04n63kuJX8LYeobTlEw+JaDulsjnTWuJYDTD5jqXsoSSqb44DGuMoybFejjyRn301GoNJHY8z9WALg3Xan52cD3BT0WJ992stsvzXI+c0mnArn/glYDsubLMthcCzDWDrr3vEFwWlx0DWYZFlTdcnzQ2MZd04wWJbDseEUo3bQcTbQPZQkGg7RYAvrbGV7Rz87Ovr57LvOZkFtnPqq6Cy1HLIkvG6looC0McYqgvNYDk6FtHMnWx0Ls6AmRn0iMukeS86QqdnWwn424bS3cURiNqEV0lNM4wTadh/oGaGlNkZNPEJbndWjaSJxh7F0lkP9oxWPK25/4AjIbOrOeuM3Huevf/DMTJ/GuHTYd8DnLm8EoC4RnZ0xh7RlOVSVSWV1ahrcmIMnIO3sWxWzYlSr22on3WPJSYYIcqM0X3H6WDkiMZtQcZhiHHHoDSAO+3tGWN5sXaxbnVqHCYjDvmPDOKn2XWXcSoPJDLWJ/B35KQus99vbPXvEoaN3lEP9U18AONUc6rMn/9kpwfWJCAPjWA6pTO6E10OMpa2AdLxMhbRjJeQth7DbeM+5WFXbVuWqltpJWw7DajmMiyMKI2o5zH2qomFikVDggPQKRxzsaXJdEwhKe/velLUcxtIFMYeVdpBx7yxqi9A3mpqVvvtiDvWN0lAVdS2xukS04nmnszle+ZkHue/xjhN1ikBpQDpZFHNwrASnAM4KSDviYFsOUWuNq9tqODqYZHASFtKQfeHTdi3lGU1bojA0C2MOKg5TjIiworma/3y8g8df7im7Xzqb41DfmCsObZOwHJxgdE0sHNit1Fgdo7kmxt5ZUvk6ls4yls6dFOJwsG/ULSQEqK+KVIw59I2k6R5K8eIJLCQzxuTFIeKfyuqIQ8x+PhYWt7eSM8vBsRzaF1g3E94eYEEZsgVFLYfy5C0HdSvNC+68bgO18QjX3vUo3912wHefzr4xsjnjupUW1MQJycRiDnu6hllYH2d5c3XZ44aT2YKANFjWw2zp1d9rtzc/Ge4uD/WNssQrDoloxTtqZ029wyfO5+5YAPFomGhYCElpKms6W2o5+AWkARbUWAkWQbLvinHSM0+Gv+1M4biT1HKYJ5zaVsd/3fQqzl/ZxF/ct4Mv/vzFkn28NQ4A4ZCwoDY+of5Ke7uHWNVSS2td3DfmkMsZ3378K+3K13IYY/jdS93uRWQ6ce4qh1NZ39bSswnLcsgPd6pPRBhMZsoWeTmB2N4Jjo09HpzYQTxiVT8nouESyyGZqRSQdmIO1v9Msy0OPZNYw5Abc5i+9f/Nfz/L3/34uWl7/aniyMAYX/3NnpKbCddy0ID0/KGxOsa/vX8TV5+zhM//9AUe2FlYCFQsDoBVJV0msOzHnu5hVrbW0FrrX13tpMcVi8OqVsuPXO5u5emD/bz7K4+y+d+fmHaB8F44xwvuziQDY2kGxzJu80KwYg7GlE9DdC2HE+hWcUaExu14QyIadl1FDs7fNO5TIV1sOTTVTG4uOuQD0sOpbEEF9lTyu5e6eXRPefftTNM/muazP3meSz73Cz79o+f42bNHCp53s5U0ID2/iIZDfOadZ3P2sgY+8p3t7Pb4nvf3jBALhwrGjE6kSrp3OEXfSJpVLTWW5eAzZtTbkdWLU/m6r4z18PIxS7ge2HmED//HU9N6R+/1R8/muIPT+2ZJUcwByouas7YT6VZyrATnwl8VDZe4lfIB6Xy2UnFA2hEHp6izZxJrGPL8XqbLtdQ7kppUsPxE0Duc4o1f+BVf/uVLvHqN1f+tOEbluN6Gk5lZ1+VXxWGaSUTD3HHdecQiIT7wjW0c6hvlmYP9PHWgl2VNVYRD+d6EbXXB3UpOMHpVqyUOyUzOLTpycEeElriVagteo5hOu3Zi8+tO5Yc7OvmL+3aQnab+OAXiMEs/5OAvDnV2inC5i1PecpgZtxJAPBoqKYJLF9U5eCukvXUOABG7OHEyAue1TKej1sEYQ+9welYWIgLsOjJI12CSf7rmHL747nOB0tiCUxmdM4VzvGcDOhz2BLC0sYovvvtc3vuvj/HKz/zc3f6WsxcX7NdaF+fYcIpszhSIhh977NzzVS217oejazBJvaemYbCM5XDKgmpEyo+APNQ3Rk0szEfedBqxSIgv/OwFLl+/iDetWxRwxcHxXjhnc+DyoF3jsKwoIA0wMFrZcugbTQf6m04FTnvuuJ2JlIiEK6Sy2gLik8rqxBzAijv0TMI15nWVTEfG0kgqSyqbm5XBXMiveXVrLfGIleJeLGTeLKXhZKagO4JVJBufsU4GajmcIF65uoV7/mATt1xxJndet4Ef/enF3Pb75xTs01aXIJszgUz4vd3DRELCsqYqT41EodUx5Jnl4CURDbOkoapsrUNnv5WVIyK876J2YHKpjEHwCkK5i+xs4GDvKNGw0GL/rsEavQrjWw7GnDiXmWs52AVwiWio1K1UoUJ6JJ0hFgkVCFlT9WQth6xbFDodwu98TkZmaTKDYy05v4O6eIShZOHvoVAc8j8bY3jrP/+Wux/aW/E93vrPv+Gj391ecZ/JopbDCeRVp7bwqlNbyj7fVpe/yLfWxTHGuN1Ui9nbPcyKBdVEwiFa68qIQxnLASx3VLmMpUN9Y/kq4KoI8UhoWmZNgOWXjYSETM7MasvhUN8oixuqCHkumvW2P76cO8y7np6RlBvcnU6cgLRT4+CXreTWOYRLA9Kjqawbb3Borom51eETYWgszdLGKvpG0tNiOXhfcyiZcfuazRac83POqzYRKYjDQKF15f15NJ2lfzTNkYHKv/djQynfoTlTgVoOswjnIu8Epf/qB8/wnq8+4rvvnq5hVtmxg/xx/pZDccwB7FqH7mHfIFhn/yhLGqxAuYjQVh/n6Dj/pJOldyTtZgDN9pjDEk8aK3gth8rZSjC96ZxenPhC3nIIl8QcSiwHj1tpOGkN+vHSVB2bVNxkOJl1iwb7pkH4vedU/Df41qMv87kHnp/y95wIfaNpomGhxhbb2njEJ+aQdXtgeVtoOFZ0sZgUMziWcWNfU42KwyzC23xvR0cf33xkPw+/dKyk70ouZ9h7bJhVrVbWUUNVlGhYSiwHJ+ZQFy/951nZUsPgWIZjRe6CZCZL91CqIPC6sC7BkWmYbw2W6b24IUE0LLPacrBqHAq73jriUM5l1DeScvfpmUQR2WRwU1k92UrFjffcgHTYCUiHyOYM2ZxhNJ0p8XE318ToGU5NOJtmOJlhcUMCEeifBnGsJA4P7DzClhkeoNM3kqahKuZa/7Xx0or64VTG7Y4w5HErOTdKxUkmXrJ2HVOdj2dgKlBxmEV4LYBP//A5RKwshuc6C0dRHuofJZXJuSmpIuJb6+DkmdfESwNaTo+l4krpw3YDvMUN+bvkhfUJjkxiEFEQekfSNFXHqB+nT9FMks7mODIwVlAAB1bQN+4TZHToH027f6MTlbFUHJCO+8UcnIC0x3IAa50jqWxBMBqsWodkJldSL1EJYwxDKeuutqEqOj2Ww7BXHApfv380Tf8Mt+3oG0m58QawbiZKLIdU1o0Zemc6OJ+FSpaD81oqDvOAqliYuniE7247wGP7etj8ulMB2Hmov2A/54LuXOAB3yrpoWSGRDREJFz6Z3ZcUsVB6YM+KZtt9XG6pslysD5AMRqqorPWcjgyMEbOFP5OHCq17e4fTdPuiMMJqnUoTmVNRMOuYDikiruy2t+TGUscii2HpuqJ1zqMpLIYY/nZG6ui0xJz8GZQFQv0gD2CdyZHlPaNpN06EfB3Kw0nvZaDRxzs/6lKmViOIM6oOIjIZSKyS0R2i8jNPs+LiNxuP79DRDZM4NiPiogRkRb78aUi8riIPG1/f/3xLPBko7U+zr5jI5y+sI4/e8MaFtTEeOZgoTg4geSVrUXiUOxWGstQ6+NSAljaVEUsHCqpdei0A48F4lCXYDCZcS2RqcIYY32AqqPUVUWPu0J6OJlh/7Gpz6pygrHe6miH+ir/tt3GWAH2xQ3W7/lEVUk74uCkRCYi5YvgYp5UVme7X0DamYs+kf5Keas1QkN1bFosB28cp/gi2j+axpjKbpnppm80XWA5FAekszmrSaJrOXjcf27MocL55y2HGYo5iEgY+BJwObAWuFZE1hbtdjmwxv66AbgjyLEishy4FNjvea1u4G3GmLOA9wHfmNTKTlKcf5Rb3nImkXCIdUsb3GH2Ds8eGqA+EXH3BX9xqOSPDIeEUxZUl9Q6OAVwhW6liXeMDcJQMkMmZ2iqjk6J5fDlX+7m6i8/NEVnl8cZjFTOcvBzK42ksqSzhsbqKI2TTAWdDE5NQ2Eqa+UiOOd7KptjJJXxzVaCifVXcjPl4mEaq6LTFHPIt6P3upUcYYaZrbrvH0nRUJXPoKqNRwvEyoklOu5kb7ZSMMth5t1Km4Ddxpg9xpgUcC9wVdE+VwH3GItHgEYRWRzg2NuAjwGu7WeMedIY40SSdgIJEYkzT7h8/SLee+EpvOY0q9x+/ZJ6Xjgy6LoGjDH85sUuXrm6pSDNtbU2Ts9wsqCSeWgs7Zup5ODXgO9g3xjNNbGCYhynxcd4aXUTxZvqV5+IMHicH+SXjg7TM5yacgvHsRyWNPhYDgn/OdLOxamxKkpzzeSyfSaDn1spkzMFdQDF8xycYjjHcnBmOTg0uZ1Zg6/BydmviUWmzWXYO5xyuxp7rbfhVNb9HMykq7JvNO265MC6iKcyOfez7FgKTTUxwiEp+L8NEnPIu5VmLltpKeDtO91hbwuyT9ljReRK4KAxplIFxzuBJ40xJbesInKDiGwTkW1dXV0BlnFy8L9ftZJPXb3efbx+aQOZnOGFw1ZsYPfRIQ71j3HJ6a0Fx7XWxckZODac/1X5dWT1srK1hpePjRQISmf/aIHVAPn6i6m2HFxxqIpSX3X8Izcdq6d7As0Lg3Cwb5TmmphvpWq5tt3ORamhyrYcTpQ42FaC4zJK+EyDS2VyhAQ3FlUQkE5nSxIYmm230kRiDt4am8bqaQpIj6RYZGe6DZbp4zRTlkMyk2UklS10K9mfRXdutON6i0WojoULiuAcsRtNly/wmw2Wg1+NRXGUp9w+vttFpBq4BfhE2TcVWQf8PfABv+eNMXcZYzYaYza2trb67TInWL+kAYBn7KD0r16whNCxLBz8CuEGxzK+BXAOq1pqSGVzHOwddbd19o2VuE/abMthqmsdnAtmU00+IH08zcecUaOVxCGbM25GVlAO9hYO+fFSLubgCF9DtWM5nLiYg9OuG/CdI53O5lxrAfJCkioTkK6vihKSiWVc5d1KVkC6fzQ95cHh3uEUTdUx6hLRgspjb5bSTFkO7s1BtdetZH0WHWvA2+SwJhYpqnPIn3e5+dIDs0AcOoDlnsfLgOIE4nL7lNu+GlgJbBeRffb2J0RkEYCILAO+D1xvjHkp6GLmIsubq6hLRNyg9K9e6OLUttqSi5WfOAwlMyWtM7ycsagegO0dfe62Q54COIf6RIRENDT1biX7A9BUHaU+ESWdNSXB06CkMjlXFLoGy1/EvvdEB5d87hcTSnP0K4BzqAtkOcROaLaS41KCfOtub9whmcm51gLkLYfRtNVau7rIrRQOCY3VsQlZDsUBaWPKFwtOFisNOlpSP1BgOcxQYaXXKnZwbtQGbSHz/o5q4sWWgycTK+m/BtetVCbp5HgJIg5bgTUislJEYsA1wJaifbYA19tZSxcC/caYznLHGmOeNsa0GWPajTHtWCKywRhzWEQagR8BHzfGTH108SRDRFi/pIFnDg0wmsry6N4eLjmt1FJqrc0X0DkMJStbDuuW1FMbj/DwnmOA9c82OJZxW2d4z2Fh/dQXwjnZJg1VluUAk7/TOzIwhmN0VLIcnu0cIJnJuSm742GMKZkA56U+EWEsnSuZV+D01WmoitJsZ+uciLTKZCZbEC9y50hnCi2HuI84OIJZHJAGu7/ScVgOQKC56kEZS2cZTWdpqolRlygvDjNlOeTjaZ6YQyXLIR4pDEh7+oyVsxwGxzJEQuK6DqeacV/VGJMBNgMPAM8B3zHG7BSRG0XkRnu3+4E9wG7gK8AHKx07zltuBk4F/kpEnrK/2ia+tLnD+qX1PNc5wEO7u0llcr7i0FJnma9OrYMxhqGxDDUVLIdIOMQFK5t5+CVLHDr7S9NYHdomMGsiKE5qZGN11DMbYXIf5kOei/2xofIXISfVNWhR38BohuFUtqxbqVzbbjcgXR2jsTpKNmdOSGvpZDrnZioBJOwLv9ciS2X83UqOJecXW3GqpIPivSt2LpBTWevguiSrLXHwBm4HZoU42E33vNlK9o2aI5yOONTEI5ZbqchyiNh9vIqb9TkMjqWpS0TK9l87XgI5q4wx92MJgHfbnZ6fDXBT0GN99mn3/Pxp4NNBzmu+sH5pA6lMjrsf2ksiGmLTyuaSfapjEWrjEddySGZyZHKmYkAa4KLVC3jw+aN09o/mZxY0lLpQ2uoTPHdooGT78dA3mqIuHiFqzwyAyX+YOz1xhEqWw8t2d9mg8ZMOO421UswBLP/vAk9qcd9ImnDI6qvjpIL2jqRoqJ4eF4DDWCbrVkdD3nLwupXSWX+3knNB87ccYu4QqCAMJTOIQHU0nBeHKbxQOzcWTdVR6hJRDni6Bjv/Q7FwaMY6/faNlloObszBnZDnjGQNUxMPFzQ3HBhNs7gxwYGe0bI3FUPT2FcJtEL6pGCdHZT+3UvHuGDlggK3gRdvrUPQ0vqLVi8A4OGXjrkX2GK3Ejj9laY+ldW5WOZnI0zScujPC1s5ccjljNt6PKiLzAnWl3MrOf5eP8uhsSqKiLhFZJOZwzxRkulCl1FeHDyWQ3FA2t7f+d37iYM102FibqWaWIRQSFzhn8rmg32eZIY6n5hDSGBRQ2LmAtI+bqVatybDthyS+Xnd1cUB6bGMmzpdrtbBaro3fY21VRxOAla21LgfWD+XkoO3v1KljqxezlxUT2N1lIdfOsahvlFCAgvrSstK2urjDKeyUzpYpXck5V44x7Mcdh8d5In9vWVfq7NvjPpEhBULqsuKw5HBMTc2ENRF1mGLw3LPrG8vbtvuojvUvtG0u6bjmcM8UYoD0m62ksdySGVyrisJ/NxKpf8zTTVWUD1oNtlwMuOmxDqFYFOZVtpT5FbyinP/aJp6O4V4xtxKoynCISn4/BW7lYZLYg75WqaB0bRrrZardVBxUAiHhLWLrcyi4voGL97+St6AYCVCIeHClQt4eM8xDvWNsbA+4duLya2SnkLrwWmdAd6LrP+H+VM/fI4//4+nyr6WM6BoQW2c7jIxB69bJKjl0NE7SnUsXFDM5KXcwJ+B0bxVlO9NNP0XqtKAtBNz8IhD1rhN9yBvOfRWCEg3V8fI5EzgdhTDyawb78pbDlMZcyh0Kw15ZjD328LcMAW1M8dzfo7l6FAVDRMSb0A6QzgkxCMhamJhN04zms6SyRkW2xly5W7IBsbS6lZS4HVntPGKZQ1ul08/vG4lx3StlK3kcNHqBXT0jrJ1X09JAZzDwjqnSnrqMpacpnvgaX9d5i5pT/cQ+3tGSprIORzqG2NxQ4LW2nhZy8EJRq9sqQkscgf7RlhqT8Xzo9zAH6td88xbDn4xh1QmS9zPcrDPr8rHbemuIaDADXrSqGP2xW9qYw7OlLUYtYkIOZMP8DriUD+DzRz7PS5TBxEpaL43nLT6WIkI1fEII6ksuZxxrdBFQdxK49z8HQ8qDicJN73uVH6w+eKKmQnLmqoYHMtw+4Mvuh+KIDnQTtxhf8+Ib7wBPIVwU5ix5OSpg9XCoSYW9v0wpzJWoV7OULaxXmf/KIsbq2ipjTE4linpJwTwcs8w4ZBw7vLGCVkOy3wa7jmUG/jjxBzASmGMhGRC2T6TZSxdGJCO+1RIp7OGaCT/f+SmslaMOdjWT0CBs9xK+QtXY3VsyrOVauMRYpFQyd/AFYcZbAPfN5oqqHFw8PbiGklZcRmwelABjKSz7o1GU3WUmli4glsprW4lJRjXblrB1ecs4Qs/e4H/+/2ngWCWw5q2WlpqrTvDclk5ba5baWosh2zOMDBW2NK4vsr/w3ygdwSnRKC4iyxY1b+9I2mWNCTcGc/FQ4zAcistbaxiSWMVXUOFfajKYYmDf7wBoDYWQaTU4ukbSbmWg4hVRHYiqqSTmaJUVqfOoVLMoUQcfGIObmfWyYmDVQE/lQHpNE22YBWnEw/YMYeGqigDo5njqro/rvPzGVta65kjPZLKUm2LgvM7H0lm3M9AfSJqdXL1sRyMcQb9qFtJCUBNPMI/XnMut/2vV7iB1/oA4iAiXLjKsh7KuZXq4hGqouEpy1gasFsqe+f+lmvQts8jCMXDicDbSbbKFYdunz5Q+3tGOGVBNQvr42RzpqAPle85jqXpH01XtBxCdtDRK2pZ2zfvbZ3QdII6s5ZkK0UCpLK6bqXKdQ4QvL9ScV+vxuqpnenQM5xPZnAth2Sh5dBQFSWVzU266v546PNxKwEFF/uRVNZjOdh9l1J5y6G+KkpNPOIb5xlJZcmZ6WudASoOc5K3n7uMH//Zq7nzuvMKcu8r8crVLYB1gfXDqpKOc2SKmu+5rTNqPJZDmcE5TufY6liYPV1DJc/nU3ATLLAtIL+4w8vHRljRXO3pFVV5LU4aayXLwTlvr1tpcMwSvgaPVTTZOcwTJVlU5xANCyGpXAQXCYcIST5u4lvn4KnVCIKvOEyhi6cgXuVJETXGskitmIO1fSbiDpZbsYzlMObEHPIjWZ3f+XAy48Yc6hMR6uIR3y7D+aZ7ajkoE2R5czWXrV8UeP/L1y/iHecu5cJVpQV2Dm1TWOvQ61NBagUQSz8I+44NU5+IsH5pQ0mLcchXRy9t9FgOReLQP2JZAZblEKwFeYcrDuUtB7Du3gaKUimttXnEoebEdGZNZnIF7RRExJojXaG3ElgxH2MsMYn6ZKtNNG7i51aaUsthJEVzdalbaTRtzdFwLAc48f2V0tkcQ8lMQY2DQ20ibwlYloMlCs7vajiZKbAcigcEOUz3FDhQcVBsmmpifOF/nVPg5immrb50oNBkcdsLVHtjDv6zEfZ1j9DeUsPq1hrfmINjOSxqSLgNCIvTWV/usY5b0VzjpuWOF5Tu6LWC3+OJQ31VYfM9tyNrieVwgmIOkcI7/0Q0XOpWKhIARyz84g1giUxTwLkUyYx1ga71tP5uqIrZrsSp8f/3DadLMt2GxjIFDQ+Pt+p+0ufmUwDnUOe1HFIZqm1RcMRhJJV1PwN1iYjvaFGY/o6soOKgTACr+d7YlHzA+9w8dY/lUCa7ZG/3MO0LaljVUkvPcKokJbSzf5SW2hjxSJhENExtPFJiOTg1DqcsqKalNo5IMMuhKppvf1EOa+BPaeM3r895okVkkyGdzZHNmYKYAzjiUFghXWw5xF1x8K++B6vWIYjl4A76KXIrpbK5AgtmsqSzOQaTGfd/p9aTreQVB6fqfiIdeKcCb9PFYrwX+5Gkx3Kwvw8lMwyMWbPf45GwNT1OLQdltrOwPs7IFFVJ9/rcXTVUWWMUvVlEyUyWQ/2jtLfUsNKu8Si2Hqwah/zdfUttrMRycNpmrGiuJhoOsaBm/EaCHb0jLGsqX+PgUJ+IFrRV9nUrVUfJ5MyUVpgX406BK+rSGY+GGPN2ZS2KOUA+KO0XjHZoqokGmiPtV53vdmadggu1Y7046bVOxtjgWNoVgpl0K/nd+DjUJqx6hmzOWJaDbalVu5aDla3kCFttPOz7P6MxB2VW0Vbn1DpUdscc6BkpaWEN1kXTmWrVN5IiJPmeSpAvKPO6aA70jGAMrGypZlWrLQ4+c6+9WVYttfGSbKWXjw3TUht372bb6uLjB6T7Ktc4ONQVWQ59o/5uJSDQxXWyuPOji91KkXBhKquP5RALYDk0VQfrr+RXnV+pM+u3H9vP7qOliQbl8I6XBTtjLGYNXZrtbiXvwJ/RVH7qXm0sPyVuYCztfhac7KZii3O6p8CBioMyAdpcX33pHfdoKst3tx3g7V9+iFd/9hf8yTcfL7AA9nYPc8nnfsF7vvooqUzOrSAOhfJ35e6dnudCu7fbuuNvX1DD8uZqIiFhb3fhhaR4et2C2pivW+mUBfmsIyvzany30tIA4uDEHJwPsJun7icO0xiUdiyH4v7+iWjIdSsZY0hnTUnMwbEkigf9eHFcY+PhdBstDEjbFdZFtQ67jw7y8e89zV2/Dj7Ty3Fted19zkXUKw7OhfOEi4NrOZZaDs459YykyOSMazlUFWUrOSnotXGr3Xuy6GZruudHg4qDMgEW2Vk+33p0v3vxTWdzfO2hvVz0mQf5i/t2MDCa5l3nLePB54/yuQd2AdaH8w+/vpV0Jseje3v45H/vpNeTiuhQ77bQyH+YnRqHlS01RMMhVjRXF1gOg2NpBpOZUsuhSBz294xwSrNXHCoPLxocS9M3kh43jRWsD3zO5Bup9Y2kSERDBT2OnFTQ6ezM6rqVKgSkU7blVs5yqORWarbTcccbWuRYDsUxByj1/393WwcAT+7vq/iaXvySGZzme/0eYY6EQ3YNyolt2+0OsfK1HKxtzg2WY6nFIiFi4ZBb5+C1HKC0An8omSEk+VjFdDB9Noky51jZUsMfv3oldz+0j18+f5T3XHgK//PcEfZ0DXPxqS38n9ef6s6aiEVC3Pmrl1jTVssPth9i/7ERvvVHF/DzXUf5l1/toTYeYc3C2oLX93MD7D027I7aBFjVWlOQzurXZrylNk7viOXCioRDjKWzHB4YY4XHcmirt1p7O/vkcoZnDvVz9rJGAHdSXBC3Ur0nlbI2HvHNcW9y3SrTKQ6OW6nwwl8VzbclSWetC3v5bKVKMYcYOWOJd6WstiEfl4fzt/VmbGWyOb735EFCAi8eHXKL18aj18en77SlGBhNI5KvfahPRGbErRTynIMX52LviEONJzvMGhVqraF9geVCdafHJTNuJh7Y8+Hj0zfoBwJaDiJymYjsEpHdInKzz/MiIrfbz+8QkQ0TOPajImJEpMWz7eP2/rtE5M2TXZwytYgIt7xlLT/98Gt41akt3PXrPWDgX9+3kW/84SYuWLUAEUFEuPVt67hgZTMf+e52fv1CF5+6ej0XrFrAx958Bq87vZUhT7aJg19n1n3dw7R7mg2ubLHEwbl79RtQ1GJ/iBz3Q0evFbcodisZk095/d6TB7nyiw/x2N4e65ieYAVwkDftnTtUb9M9h3yF8fRdqBzXUXFAusBysK2LaLjwohIkIO32VxrHtTTsYzm01sVprYvznW0H3L/dr1/somswyXsvPAWA7Qf6Kr6ug/P+heKQdyvVJ/Luyplovtc3mipxmTo4MQcn3lXtSfetjlmjQgfGMm4BX40nRuFlujuyQgBxEJEw8CXgcmAtcK2IrC3a7XJgjf11A3BHkGNFZDlwKbDfs20t1qzpdcBlwJft11FmCatba7nr+o088vE38MCHX8MbzlxYcgcTi4S447rzWLu4nj957Wqu3bQCsNqP/9O157J2cT3rltQXHFPvYzm8fGyElZ6L+qrWWpKZnDvcx89yaK0tHJnqpLGuaM6LTL7LrHX8T3ceBuA/th4Agtc4QGnb7v7R0tYJ9YkoVdEw335sP09VuAg6AfvJUC4gHffEHBxxiBXtE8RycBISnOLAcrgBac9dcTQc4uOXn8FTB/r47uPW7/i72zpYUBPjQ288DZHgriXHbecVslp74E+x9TETbbvL9VWC/P+Kn+VQa1dDF2YrOa1BCtcw3bMcIJjlsAnYbYzZY4xJAfcCVxXtcxVwj7F4BGgUkcUBjr0N+Bhgil7rXmNM0hizF2su9abJLE6ZXhY1JHyraR2aa2Lc/2ev5i8vO6Nge30iyo/+9GI+8qbTC7YXpx6OpfNprA5uOqsdd+j0GVC0oLawEO4lu+VGoeWQF4exdJbfvNhNOCTc/3QnQ8kMHb2jJKIhFoxT4wDeLKvC3j5eQiHhy9dtYHAszTu+/BCf/O+dBZO/wJpU9847fsf1dz/m21V2PPIxh1LLwXE5pbOVLYdyRXAAZy9rIBwStu7rqXge+TqHQqF5+7lLOb+9ib//yS72dg/zP88d4epzl9JUE+O0trqKw5y89AynaS66+FpupXTJ775cM8fpxO/mwMG1HOxsOq8YV8fDdA9ZgWrnf8oRgOFk4f/DdHdkhWDisBQ44HncYW8Lsk/ZY0XkSuCgMWb7JN4PEblBRLaJyLaurq4Ay1BmE36+0ppYmHBIXMthv5vGmheHfDrrELuPDvHvj+1nTVtdwYCi4uZ7W7Yf4oxFde52yA8vOjKY5JE9xxhNZ7nptasZTWe5f0en2401iE/X+ZA6tRTedt1eXnd6Gz/780t49wUr+NpD+/jED3YWPP/QS91s7+jn1y908aF7nwrUNdZLPlup8KJcFQ0znMxiTD7rpWxAuswIWrAuwOuX1PPInmMVz2MomSYRDZUMjRIRPnnlevpGUrz7K4+Qzhp+b+MyADac0shTB/rGDXZDYV8lh/pEPpW12HKYiZiD398ffGIOHtdbTSzCYdsSLrYchnwthxl2KwF+n47iv2C5fXy3i0g1cAvwiUm+H8aYu4wxG40xG1tby09HU04eRKSg2tgJPDvBObBGodbGI/z6xW7e/ZVHAOuO3EuLp/neMwf7eebggOvWclhQGyck1mS7B587SlU0zAdfdyqrWmv47uMH6OgbCeRSAsv1tLq1hk/+904++5Pn6fW06y6mPhHl01efxftf1c73nzzIgZ78fIqv/+5lFtTEuPnyM/jJzsPc8v2nJ1RRXS4gvbKlhtF0lv09I67lMJmANMCFqxbw1IE+RlPlLZuhZLbsBMK1S+q5/qJ2OvvHOGtpA2csslyL5y5von807dsepZiekVRBw0awLqKpTI7uoVSpW2kGYg7lAvaOG8mxHLyusZp4vuuxE3OoTfjHHGaLW6kDWO55vAw4FHCfcttXAyuB7SKyz97+hIgsCvh+yhzFG0Dc5yMOIsKq1hp+/vxRMjnDv//xBaxuLcx6qo1HiEdCHBtO8R9bDxCLhLj6nELjMxwSWmrjHBkY4+fPH+XiNS0komHedd4ytu7r5YUjQ4HFIR4J84PNF/Ou85bx5V++xFg651sA5eWG16wiJPAvdn7/gZ4RHnz+CNduWsGNl6xm8+tO5d6tB/ibHz4b2IJwA9JF8YSN7U0AbNvX64k5+FdIBxGHdNbwZAUXUHHTvWI+fOlprF1czx+/ZpW77dwVjQAVXxesmMnznYMlf3PnQtnZP1pQX1KfiDKcyrqieCLoGy6fdRUOCTWxsH+2UixCxv5bl8YcisVhdriVtgJrRGSliMSwgsVbivbZAlxvZy1dCPQbYzrLHWuMedoY02aMaTfGtGMJwgZjzGH7ta4RkbiIrMQKcj82FYtVZj8NVVF+u7ubj923nQd2HqapOlriv123pJ76RIRv/OEmTltYV/IaItaFv6N3hP966iBXrF/k6wNeWJ/gty92c7BvlDee2QbAOzcsIyTWRShIppJDbTzCZ9/1Cu68bgOrWms4d0VTxf0XN1TxrvOW8Z1tHRwdGOObj75MSIR3X2BZOB9502m8/1XtfO2hfdxwz7ZAbTdcy6EoW2lNWx118QiP7+/NWw5l6xwqX3A2tjcREiq6loaTmYKLXjENVVHu/7NXc+UrlrjbVrfWUpeI8MQ4Qekn9/cyms7yqlNbCrY7LhanI2v+vezamRNkPTh9nyrdHDgtNKAoW8nzsyNw8UiISEgKLIcTMegHAoiDMSYDbAYeAJ4DvmOM2SkiN4rIjfZu9wN7sILHXwE+WOnYcd5vJ/Ad4FngJ8BNxpjj79alnBTceMlq1i2p53+eO8oT+/tYt6ShZJ9PvHUdv/yL1/k+59BSF+dnzx5hcCzD/zp/he8+C+vjHLJ9vK87vc3eluA1p1luynJT8Spx2frF/Pwjry25ePlx4yWryWRzfPEXu/nO1gNceuZCt9JbRPjrt63jU1et45cvdPHOL/+uwAXlxXE9JdP+AelwSDj3lCYe91gOxYkEzuPxiqrqElHOWtrAI3vKB6UHk5lAEwi9hELCOcsbx7UcHtrdTUhwh1Plz6u0pgLyhWjlZpNPNY4IlctWgsK2ItVRr1spv90pCBURahOFMx2SmRzprJl2yyHQqxtj7scSAO+2Oz0/G+CmoMf67NNe9Phvgb8Ncm7K3OKKsxZzxVmLMcZweGDM9+6oKhaumI8P0FITI501tC+oLjujwhn684plDe7PAO/etIJf7urytUqmklMW1PC2VyzhnodfBuD6V55Sss97L2pnZUstH/zW47znq4/y35svdi942Zzhw//xFD3DKb7+B5vKVkgDnLeiiX988AW67RqBcl1Zx/u9AlywagH/9tA+xtLZkuA3WJbDQs/vMygbVjTxzz9/sWRQkJeHXjrGWcsaS9w2tWXEwe3MeoIsB7d1RkXLwWMVeETaa215XWO1RdPgBk5A6wzQ9hnKLEVEWNxQVfYiMR5OZtLvn7+8bMaRU+vwhjMXFmx/07pFPHTz6zl90fSKA8AHX3sqYM3xvqjobtjh4jUt/NsfbKKzf5Q//85TbkbP5x7YxZbth/jt7m7u3bq/bEAaLHeQMfCo7Q6a6DwHLxeuaiaVzbmppwd6Rtj870+4CQTjxRzKce6KRnIGdpSpAxkcS/PUgT4uPrX09+Rt4FgckIYTJw5OgV6lSm+n6rn4d+R97LUKvNPjwNN0b5KfjaCoOChzkmVNVUTDwrs2LKu4D8Abi8QBJudSmgynL6rjU1et41NXr6+YNrthRRP/31vW8uDzR/nyL3fz/Sc7uPNXL/HuC1ZwwcpmPvfALo4MjBELh3wrc1+xvJGQwMOOOEwyIA2wsb3Zjjv0MJTM8Edf38YPd3Ry6xbLY2xlK028bvXc5Vac5v5nOn2ztB7d00M2Z3xddmXdSj5V99PJjo5+gIo3Fs4NT/Hv2nHpObMcvPt7Y04noiMraG8lZY7yBxev5PKzFhW4i4p56ysW095SzdqiSu0TzXsvag+03/UXncIT+3v5h5+9QDQU4oKVzXzyynXs6Rrmitt/w3e3dZStU6iNRzhzcT07Dw0ApTGHII33HOoTUdYvbeDhl7rZebCf3V1DvPXsxfxwRye/3HWU4QpuoUo0VEd554ZlfPOR/QyNZfi7d5xdcD4PvdRNIhpig0+w3/t+xUVwcOIsh237eljWVFV2FjvkXWDF4uDMdKhPlLrMvC1LTkRHVlDLQZmj1MQjnNpW2S0Uj4Q575TyM7NnGyLC373jLE5fWMeihgR3XHce0XCI0xfV8b6L2snkTEmmkpfzTslfVP1mSEMwywHggpXNbN3Xy4PPH+Wv37aWL/z+ObQvqObTP3qO0XR2Um4lgM//3tl89E2n8YPth3jXnb9zGyCCFYw+v73ZN85RNwvcSsYYtu7r5fz2yv9TecshUrTdWld9cTxlhiwHFQdFOYmojkX4r5texQMfek3BPIMPXbrGHZVaDq84FLfPeM1pLVxz/nK3f9J4vHK15dq57sIVXH9RO7FIiJsvP9Md2jPZWJGIsPn1a/jX921kf88Iv3/nwxzoGeHowBgvHBkqmwUWi4TcWItXHBLRMLFIaEL9lX78dCfv+eojE+7J9PKxEbqHkm5dSTmci3pxexFHLOqLLvp1ieKYw/SPCAUVB0U56UhES7O16hNR7rjuPP7y8jPKHFUoDvFw4fGnttXxmXeeTdgnXuHHJae18vU/2MStb1vnbnvzuoVcYLdsn6zl4PD6Mxby7T++kKFkhmvuesRtiHhxhRThukTUatdddNEsN5vcjwM9I/zFfTt4aPcx/vFnL07onJ2eU5O1HJxspWLLoSZWznJQt5KiKAE4v725oLCsmKWNVe7ApmK30kQJhYRLTmstSMUUEf7qrWtJREMFTQ4ny/qlDXzrjy5gKJnhH372Ao3VUdYuLh8fqktEqItHSgLyDVX5mQ67Dg/y46f9A97ZnOEj37FavV22bhFff3gfz3UOBD7frft6aKyOcmpR9XYxTsyhuKbEsST8Yg7O3GnIi8NkrbOgqDgoyjxBRFzroditNFWsX9rAjr9+s+t2morX+9YfXUBDVZQ3nLHQNxPLoS4R8a2EtzqzZvjOtgNc+cXf8iffeoI/+vo2uormjH/1N3t4bF8Pt165js+88ywaqqL81X89E6gZIFjtSTae0lTxHMFjOZRJZXX6KhXv71gPg2MZt0nldKLZSooyj/j985cTDklJx9Sp5HitkmLWL23goZtfT2Sci2G52oKGqigP7e7mt7u7eeXqBVxyWiv/8LMXuOwff82fvmENIbGmy33x57u5bN0i3rlhKSLCzZedwcf+cwf/+UQHv7dxue9rO3QPJdnTPczvn195P/DEHEosB/9spXzb7gwN9rzy6XYpgYqDoswrLjmtlUtOO/m6GAdxofzlZWe4VeJemu1q+T957Wo+culpRMIhXndGGx+69yn+eku+m8+q1hr+3zvOcutN3nXeMu7dup/P/Ph53rTWvz+Xw7Z9VkHg+eMEo621WK9T3MeqOhrmNae1uqN2i/f3Wg7THYwGFQdFUeYI65f699r680tP49pNKwoCxactrGPL5lfR0TtKTTxCXcLq5OstRAyFhE9dvZ4rv/gQn/nJ8/zdO84q+97b9vUQj4TKnoMXR+iKLYdQSLjnD0rnmjmxCCfWMJic/o6soDEHRVHmOMuaqn0ziCLhEO0tNbTWxUlEw74V6uuWNPD+V7bz7cf28/jL5ZsNbn25l1csb6yYSuyQT2UNdoF39i+0HKbfraTioCiKUoEPX3oaixsS3PL9Z9yW5y8fG+Yrv97DD546yLZ9Pew82B/IpQRW25Zb37aWK85aHGh/1600pm4lRVGUWUNNPMKtV67jA994nM8/sIu+kTT3PdFRMoRpvPoGBxHhf79qZeD3d6fBJdP84KmD7O0e5u3nlkxOnnJUHBRFUcbhzesW8cYzF/Ivv95DLBLivReewh9evJLRdJZ93cMMjGV49ZrpCfQ7MYqHXzrGj585zKb2Zm68ZPW0vJcXFQdFUZQA/N07zmLD4428/dylBY31pnvuhxO4/q+nDrG0sYo7rtsw5enCfgR6BxG5TER2ichuEbnZ53kRkdvt53eIyIbxjhWRT9n7PiUiPxWRJfb2qIh8XUSeFpHnROTjU7FQRVGU46G1Ls4HX3tqxY6r00EkHKIqGqY6Fuar79vIAntWyXQzrjiISBj4EnA5sBa4VkTWFu12Odas5zXADcAdAY79nDHmbGPMOcAPgU/Y238PiBtjzgLOAz4gIu2TXaCiKMrJzp9fehp3vXcjZ1ZoHzLVBHErbQJ2G2P2AIjIvcBVWDOeHa4C7rHHhT4iIo0ishhoL3esMcbbtKQGcKI7BqgRkQhQBaSA4A1OFEVR5hh//JpVJ/w9g7iVlgIHPI877G1B9ql4rIj8rYgcAN5D3nK4DxgGOoH9wOeNMeUTjBVFUZQpJ4g4+DU0Ke5EVW6fiscaY24xxiwHvgVstjdvArLAEmAl8BERKZFNEblBRLaJyLaurq7xV6EoiqIEJog4dADeblLLgEMB9wlyLMC/A++0f3438BNjTNoYcxR4CNhYfIAx5i5jzEZjzMbW1pOvV4yiKMpsJog4bAXWiMhKEYkB1wBbivbZAlxvZy1dCPQbYzorHSsiazzHXwk8b/+8H3i9/Vo1wIWe5xRFUZQTwLgBaWNMRkQ2Aw8AYeBuY8xOEbnRfv5O4H7gCmA3MAK8v9Kx9kt/RkROB3LAy8CN9vYvAV8DnsFyS33NGLNjKharKIqiBEP8JiKdbGzcuNFs27Ztpk9DURTlpEJEHjfGlLjtQRvvKYqiKD6oOCiKoiglzAm3koh0YcUtJksL0D1Fp3MyMN/WC7rm+YKueWKcYozxTfecE+JwvIjItnJ+t7nIfFsv6JrnC7rmqUPdSoqiKEoJKg6KoihKCSoOFnfN9AmcYObbekHXPF/QNU8RGnNQFEVRSlDLQVEURSlBxUFRFEUpYV6Lw3jjT+cCIrJcRH5hj1zdKSJ/Zm9vFpGficiL9vemmT7XqUREwiLypIj80H48p9cLYA/Zuk9Enrf/3hfN5XWLyIft/+lnROTbIpKYa+sVkbtF5KiIPOPZVnaNIvJx+3q2S0TefDzvPW/FIeD407lABviIMeZMrA63N9nrvBl40BizBnjQfjyX+DPgOc/jub5egH/Cand/BvAKrPXPyXWLyFLgT4GNxpj1WI09r2HurfffgMuKtvmu0f5cXwOss4/5sn2dmxTzVhzwjD81xqQAZ4TpnMIY02mMecL+eRDrgrEUa61ft3f7OnD1jJzgNCAiy4C3AF/1bJ6z6wUQkXrgNcC/AhhjUsaYPub2uiNAlT1SuBprVsycWq8x5tdA8STMcmu8CrjXGJM0xuzF6pK9abLvPZ/FIcj40zmFiLQD5wKPAgvtmRvY39tm8NSmmn8EPobVDt5hLq8XYBXQBXzNdqd91Z6HMifXbYw5CHwea/5LJ9YMmZ8yR9dbRLk1Tuk1bT6LQ5Dxp3MGEakF/hP4kDFmYKbPZ7oQkbcCR40xj8/0uZxgIsAG4A5jzLlYc9hPdpdKWWw/+1VYo4SXADUict3MntWMM6XXtPksDkFHmJ70iEgUSxi+ZYz5nr35iIgstp9fDBydqfObYl4FXCki+7Bcha8XkW8yd9fr0AF0GGMetR/fhyUWc3XdbwT2GmO6jDFp4HvAK5m76/VSbo1Tek2bz+IQZPzpSY+ICJYf+jljzBc8T20B3mf//D7gByf63KYDY8zHjTHLjDHtWH/TnxtjrmOOrtfBGHMYOGBPVwR4A/Asc3fd+4ELRaTa/h9/A1Y8ba6u10u5NW4BrhGRuIisBNYAj036XYwx8/YLa7TpC8BLwC0zfT7TtMaLsUzLHcBT9tcVwAKsTIcX7e/NM32u07D21wI/tH+eD+s9B9hm/63/C2iay+sGPok1X/4Z4BtAfK6tF/g2VkwljWUZ/GGlNQK32NezXcDlx/Pe2j5DURRFKWE+u5UURVGUMqg4KIqiKCWoOCiKoiglqDgoiqIoJag4KIqiKCWoOCiKoiglqDgoiqIoJfz/WFPvkV9hapMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------\n",
    "# Model \n",
    "input_channels = 3\n",
    "num_classes = 3\n",
    "learning_rate = 1e-3  \n",
    "epochs = 101\n",
    "loss_fn  =  DiceLoss() \n",
    "folder_name = 'model_unet_1_data_aug'\n",
    "\n",
    "\n",
    "filters = 8\n",
    "beta = 101\n",
    "z_dim = 10\n",
    " # different learning rate and the same error as Porba Unet\n",
    "#----------------------\n",
    "if filters == 8:\n",
    "    featureDim = 16384\n",
    "if filters ==4:\n",
    "    featureDim = 8192\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelPath = os.path.join(r'C:\\Users\\kmorales\\Desktop\\2DO PhD\\Strasbourg\\Hugo_seg', folder_name)\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)\n",
    "\n",
    "#net = Probabilistic_UNET(input_channels, num_classes, filters, z_dim, image_shape, featureDim)\n",
    "net = UNET(input_channels, num_classes) \n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0)\n",
    "#summary(net)\n",
    "#sum(p.numel() for p in net.parameters() if p.requires_grad) \n",
    "tloss, tloss_list = training_Unet(dataloaders, epochs, device, loss_fn, net, optimizer, modelPath)\n",
    "plt.plot(tloss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad1b2571548246114fecf39d36c90373949ef600ffd6c7010bad9bfc5901cee0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
